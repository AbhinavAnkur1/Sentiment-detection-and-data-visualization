# Sentiment detection and data visualization

<img src="harrypotter.jpeg">

### Sentiment Analysis Using NLP




```python
import numpy as np
import pandas as pd
from scipy import stats
from scipy import optimize
import matplotlib.pyplot as plt
import seaborn as sns
import re
```

### 1. We Import the book for basic analysis 


```python
import os
for root, dirs, files in os.walk("/Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text"):
    for file in files:
        if file.endswith(".txt"):
             print(os.path.join(root, file))
```

    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book2.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book3.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book1.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book4.txt


### Distribute the dataset into paragraph


```python
import os
hp_paragraphs = []
for root, dirs, files in os.walk("/Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text"):
    for file in files:
        if file.endswith(".txt"):
            print(os.path.join(root, file))
            with open(os.path.join(root, file), "r", encoding="utf8", errors='ignore') as input:
                paragraphs = input.read().split("\n\n")   #\n\n denotes there is a blank line in between paragraphs.
            #print(paragraphs[0])
            hp_paragraphs.extend(paragraphs)
            
print(len(hp_paragraphs))

```

    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book2.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book3.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book1.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book4.txt
    10710



```

### Seperate the paragraph into sentences


```python
import os
hp_sentences = []
for root, dirs, files in os.walk("/Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text"):
    for file in files:
        if file.endswith(".txt"):
            print(os.path.join(root, file))
            with open(os.path.join(root, file), "r", encoding="utf8", errors='ignore') as input:
                sentences = input.read().split(". ")   #. denotes end of sentence
            hp_sentences.extend(sentences)
            
print(len(hp_sentences))

```

    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book2.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book3.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book1.txt
    /Users/animeshgiri/Desktop/Animesh/INFO 6105/finalproject/sample_text/Book4.txt
    21509



```python
print(hp_sentences)


### Converting the data into DataFrame using pandas


```python
import pandas as pd
hp_df = pd.DataFrame(hp_sentences, columns = ['Sentence'])
hp_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sentence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>\nHARRY POTTER AND THE CHAMBER OF SECRETS\nby J</td>
    </tr>
    <tr>
      <th>1</th>
      <td>K</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Rowling\n\n(this is BOOK 2 in the Harry Potter...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Mr</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Vernon Dursley had been woken in\nthe early ho...</td>
    </tr>
  </tbody>
</table>
</div>



### Importing Nltk for Sentiment Analysis


```python
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()
#sid.polarity_scores(hp_paragraphs[100])
```

    [nltk_data] Downloading package vader_lexicon to
    [nltk_data]     /Users/animeshgiri/nltk_data...
    [nltk_data]   Package vader_lexicon is already up-to-date!



```python
for i in range(0,10710,1):
    z=sid.polarity_scores(hp_paragraphs[i])
    lists=sorted(z.items())
    x, y = zip(*lists)
    plt.plot(x, y)
plt.xlabel("Sentiment")
plt.ylabel("Percentage")

plt.show()
```


    
![png](output_17_0.png)
    



```python
import re
def eda(sentences):
    processed_sentences = []

    for s in sentences:
        # Remove all the special characters
        processed_sentence = re.sub(r'\W', ' ', str(s))

        # remove all single characters
        processed_sentence= re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_sentence)

        # Remove single characters from the start
        processed_sentence = re.sub(r'\^[a-zA-Z]\s+', ' ', processed_sentence) 

        # Substituting multiple spaces with single space
        processed_sentence = re.sub(r'\s+', ' ', processed_sentence, flags=re.I)

        # Removing prefixed 'b'
        processed_sentence = re.sub(r'^b\s+', '', processed_sentence)

        # Converting to Lowercase
        processed_sentence = processed_sentence.lower()

        processed_sentences.append(processed_sentence)
        
    return processed_sentences
```


```python
processed_sentences = eda(hp_sentences)
```


```python
import nltk
nltk.download('stopwords')
```

    [nltk_data] Downloading package stopwords to
    [nltk_data]     /Users/animeshgiri/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!





    True




```python
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer (max_features=2500, min_df=1, max_df=0.8, stop_words=stopwords.words('english'))
processed_sentences_vectors = vectorizer.fit_transform(processed_sentences).toarray()
```


```python
processed_sentences_vectors.shape
```




    (21509, 2500)




```python
labels = []
for i in range (21509):
    if(i<5000):
        x=0
    else:
        x=1
    labels.append(x)
```


```python
print(labels)
```

    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]



```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(processed_sentences_vectors, labels, train_size = 0.8, test_size=0.2, random_state=100)
```


```python
from sklearn.ensemble import RandomForestClassifier

text_classifier = RandomForestClassifier(n_estimators=200, random_state=100)
text_classifier.fit(X_train, y_train)
```




    RandomForestClassifier(n_estimators=200, random_state=100)




```python
predictions = text_classifier.predict(processed_sentences_vectors[:,:2500])
predictions
```




    array([0, 1, 1, ..., 1, 1, 1])




```python
y_labels = []
for i in range (21509):
    if(i<5000):
        x=0
    else:
        x=1
    y_labels.append(x)
```


```python
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(y_labels, predictions))
print(classification_report(y_labels, predictions))
print(accuracy_score(y_labels, predictions))
```

    [[ 3879  1121]
     [  120 16389]]
                  precision    recall  f1-score   support
    
               0       0.97      0.78      0.86      5000
               1       0.94      0.99      0.96     16509
    
        accuracy                           0.94     21509
       macro avg       0.95      0.88      0.91     21509
    weighted avg       0.94      0.94      0.94     21509
    
    0.9423032219071087


### Distribution of words in each sentence 


```python
hp_df = pd.DataFrame(processed_sentences, columns = ['Sentence'])
hp_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sentence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>harry potter and the chamber of secrets by j</td>
    </tr>
    <tr>
      <th>1</th>
      <td>k</td>
    </tr>
    <tr>
      <th>2</th>
      <td>rowling this is book 2 in the harry potter ser...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>mr</td>
    </tr>
    <tr>
      <th>4</th>
      <td>vernon dursley had been woken in the early hou...</td>
    </tr>
  </tbody>
</table>
</div>




```python
hp_df['NumWords'] = hp_df['Sentence'].apply(lambda x: len(x.split()))
hp_df[['NumWords']].hist(figsize=(12, 6), bins=10, xlabelsize=8, ylabelsize=8);
plt.title("Distribution of number of words in each sentence")
```




    Text(0.5, 1.0, 'Distribution of number of words in each sentence')




    
![png](output_33_1.png)
    



```python
stop = stopwords.words('english')
```

### Removing Specific stopwords


```python
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
stop_words = list(STOP_WORDS)
stop_words.append('said')
print(len(stop_words))
print(stop_words)
```

    327
    ['every', 'five', 'hereupon', 'her', 'none', 'some', 'seems', 'over', 'we', 'three', 'them', 'get', 'bottom', 'neither', 'four', 'whereas', 'amongst', 'hundred', 'hence', "'re", 'along', 'with', 'seeming', 'yours', 'across', 'by', 'nor', 'former', 'namely', 'who', "'ve", "'s", 'where', 'used', 'n’t', 'both', 'nowhere', 'put', 'should', '’s', 'yourselves', 'themselves', 'could', 'ca', 'elsewhere', 'becoming', 'yet', 'now', 'because', 'anyway', 'however', 'what', '‘d', '’d', 'enough', 'even', 'almost', 'again', 'than', 'ten', 'had', 'whereafter', 'cannot', 'us', 'towards', 'since', 'within', 'behind', 'be', 'well', 'say', 'they', 'whom', 'thence', 'above', 'not', 'to', 'hereby', 'one', 'when', 'thus', 'n‘t', 'beyond', 'among', 'sometimes', 'am', '’ve', 'are', 'through', 'somewhere', 'me', 'sometime', 'was', 'while', '‘ve', 'few', 'part', 'most', 'why', 'please', 'therefore', 'just', 'throughout', 'moreover', 'forty', 'hereafter', 'around', 'made', 'until', 'this', 'due', 'were', 'on', 'afterwards', 'been', 'move', 'side', 'another', 'without', 'mostly', 'any', 'quite', 'does', 'then', 'upon', 'less', 'many', 'between', 'first', 'whose', 'thru', 'ever', 'latterly', 'thereafter', 'sixty', 'beforehand', '‘ll', 'ours', 'empty', 'for', 'often', 'up', 'really', 'itself', 'together', '’re', '‘s', 'there', 'it', 'off', 'she', 'your', 'last', 'several', 'noone', 'became', 'herein', 'under', 'fifty', 'everything', 'into', 'give', 'unless', "'m", 'next', 'always', 'already', 'whole', 'wherein', 'whenever', 'perhaps', 'never', 'of', 'seemed', 'seem', 'go', 'being', 'you', 'although', 'only', 'that', 'done', 'out', 'did', 'therein', 'mine', 'more', 'also', 'our', 'after', 'show', 'each', 'herself', 'thereby', 'take', 'their', 'rather', 'via', 'very', 'least', 'anyone', 'much', 'everywhere', 'two', 'back', 'have', 'has', "'ll", 'or', 'otherwise', 'whoever', 'an', 'such', 'nine', 'various', 'its', 'those', 'nevertheless', 'do', 'doing', 'though', 'eleven', 'is', 'the', 'besides', 'becomes', 'third', 'once', 'him', 'become', 'he', 'toward', 'else', 'fifteen', 'might', 'beside', 'must', 'serious', 'myself', 'anyhow', 'can', 'something', 'himself', '‘m', 'down', 'nobody', 'own', 'wherever', 'using', 'as', '’m', 'my', 'amount', 'twelve', 'front', 'twenty', '‘re', 'about', 'full', 'all', "n't", 'during', 'others', 'either', 'anything', 'before', 'his', 'onto', 'which', 'name', 'whether', 'see', 'meanwhile', 'from', 'ourselves', 'other', 'whence', "'d", 'whereupon', 'whatever', 'everyone', 'except', 'anywhere', 'per', 'against', 'no', 'somehow', 'a', 'would', 'below', 'someone', 'alone', 'these', 'six', 'regarding', 'thereupon', 'i', 'too', 'whither', 'at', 'still', 'how', 'may', 'and', 'in', 'whereby', 'yourself', 'so', 'further', 'but', 'indeed', 'latter', 'nothing', '’ll', 'call', 'keep', 'hers', 'eight', 'will', 'same', 'top', 'if', 'formerly', 're', 'make', 'here', 'said']



```python
all_sentences = hp_df['Sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
all_sentences
```




    0                           harry potter chamber secrets j
    1                                                        k
    2        rowling book 2 harry potter series original sc...
    3                                                       mr
    4        vernon dursley woken early hours morning loud ...
                                   ...                        
    21504    hugged harry tightly saw whispered ear think d...
    21505                                          touch harry
    21506    harry ron clapping bye harry hermione kissed c...
    21507    point worrying told got dursleys car hagrid co...
    21508                                                 meet
    Name: Sentence, Length: 21509, dtype: object




```python
all_words = ''.join([word for word in all_sentences]).replace('\n', ' ')
all_words
python
all_words = ''.join([word for word in all_sentences]).replace("\'", "")
all_words


### Wordcloud for the text


from wordcloud import WordCloud

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=500, max_font_size=110, background_color="white", max_words=3000, contour_width=3, contour_color='steelblue')

# Generate a word cloud
wordcloud.generate(all_words)

# Visualize the word cloud
wordcloud.to_image()
```




    
![png](output_41_0.png)
    




```python
import re
NON_ALPHANUM = re.compile(r'[\W]')
NON_ASCII = re.compile(r'[^a-z0-1\s]')
def normalize_texts(texts):
    normalized_texts = ''
    lower = texts.lower()
    no_punctuation = NON_ALPHANUM.sub(r' ', lower)
    no_non_ascii = NON_ASCII.sub(r'', no_punctuation)
    return no_non_ascii
  
hp_df['Sentence2'] = all_sentences.apply(normalize_texts)
hp_df.head()
hp_df['Sentence2'] = hp_df['Sentence2'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))
```

### Occurence of main Different Words in the book


```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

def get_top_n_words(corpus, n=10):
    vec = CountVectorizer(stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in   vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

words = []
word_values = []
for i,j in get_top_n_words(hp_df['Sentence2'], 40):
    words.append(i)
    word_values.append(j)
fig, ax = plt.subplots(figsize=(16,8))
ax.bar(range(len(words)), word_values);
ax.set_xticks(range(len(words)));
ax.set_xticklabels(words, rotation='vertical');
ax.set_title("HarryPotter Soceer's Stone ");
ax.set_xlabel('Word');
ax.set_ylabel('Number of occurences');
plt.show()
```


    
![png](output_44_0.png)
    



```python
import spacy

nlp = spacy.load('en_core_web_sm')

# Process `text` with Spacy NLP Parser
text = '. '.join(all_sentences[:10000])
processed_text = nlp(text)
```


```python
sentences = [s for s in processed_text.sents]
print(len(sentences))

print(sentences[100:115])
```

    12817
    [best spell set fire harry dudley stumbled backward look panic fat face, dad told magic ll chuck house haven got haven got friends jiggery pokery harry fierce voice., hocus pocus squiggly wiggly muuuuuum howled dudley tripping feet dashed house., muuuum know harry paid dearly moment fun., dudley hedge way hurt aunt petunia knew hadn magic duck aimed heavy blow head soapy frying pan., gave work promise wouldn eat finished dudley lolled watching eating ice cream harry cleaned windows washed, car mowed lawn trimmed flowerbeds pruned watered roses repainted garden bench., sun blazed overhead burning neck., harry knew shouldn risen dudley bait dudley thing harry thinking., maybe didn friends hogwarts wish famous harry potter thought savagely spread manure flower beds aching sweat running face half past seven evening exhausted, heard aunt petunia calling walk newspaper harry moved gladly shade gleaming kitchen., fridge stood tonight pudding huge mound whipped cream sugared violets., loin roast pork sizzling oven eat quickly masons soon snapped aunt petunia pointing slices bread lump cheese kitchen table., wearing salmon pink cocktail dress harry washed hands bolted pitiful supper., moment finished aunt petunia whisked away plate upstairs hurry passed door]



```python
from collections import Counter, defaultdict

def find_character_occurences(doc):
    """
    Return a list of actors from `doc` with corresponding occurences.
    
    :param doc: Spacy NLP parsed document
    :return: list of tuples in form
        [('xxx', 622), ('yyy', 312), ('zzz', 286), ...]
    """
    
    characters = Counter()
    for ent in doc.ents:
        if ent.label_ == 'PERSON':
            characters[ent.lemma_] += 1
            
    return characters.most_common()

print(find_character_occurences(processed_text)[:20])
```

    [('harry', 2696), ('ron', 1072), ('weasley', 204), ('harry ron', 168), ('don', 159), ('yeh', 119), ('harry potter', 79), ('didn', 76), ('harry didn', 61), ('dursley', 49), ('pomfrey', 46), ('dudley', 44), ('malfoy', 42), ('george', 41), ('harry harry', 36), ('dobby', 36), ('couldn', 35), ('ron harry', 34), ('hedwig', 31), ('filch', 28)]



```python
def get_character_offsets(doc):
    """
    For every character in a `doc` collect all the occurences offsets and store them into a list. 
    The function returns a dictionary that has actor lemma as a key and list of occurences as a value for every character.
    
    :param doc: Spacy NLP parsed document
    :return: dict object in form
        [('xxx', 622), ('yyy', 312), ('zzz', 286), ...]
    """
    
    character_offsets = defaultdict(list)
    for ent in doc.ents:
        if ent.label_ == 'PERSON':
            character_offsets[ent.lemma_].append(ent.start)
            
    return dict(character_offsets)

character_occurences = get_character_offsets(processed_text)
```


```python
print(list(character_occurences.keys()))
```

    ['harry potter', 'j. k.', 'vernon dursley', 'harry', 'petunia harry', 'don', 'vernon heartily', 'dudley', 'don son', 'dudley large', 'dursley', 'didn mean', 'uncle', 'wasn', 'harry owl hedwig', 'skinny', 'scar hint', 'harry harry', 'harry twelfth birthday', 'vernon harry', 'petunia lounge', 'mrs', 'mason', 'dudley uncle', 'mason dudley', 'ma', 'quidditch harry', 'ron weasley', 'didn miss', 'ron', 'harry point unlocking hedwig', 'cupboard stairs wand broomstick', 'livid', 'eye harry', 'sang dudley', 'day dudley', 'harry dudley', 'backward', 'magic will', 'hocus pocus', 'muuuum', 'soapy', 'mow lawn', 'didn', 'jack', 'honor', 'dobby', 'dobby house', 'rude isn', 'elf hung', 'dobby earnestly', 'sit harry', 'noisy tear sit', 'dobby shake', 'warn leapt', 'dobby don harry', 'dobby bed hedwig', 'dobby speak ill', 'wizard', 'dobby punish', 'dobby shut', 'dobby serve', 'dobby hear', 'dobby know', 'voldemort harry dobby', 'ron painful dobby', 'don belong', 'danger harry', 'plot harry potter', 'plot', 'harry potter peril', 'dobby funny', 'hang hasn', 'nod', 'dobby head', 'dobby eye', 'harry hint', 'hasn', 'don dobby', 'albus', 'dobby voice', 'harry s. ve', 'don write', 'dobby slyly', 'foot harry potter', 'mustn angry dobby', 'dobby good', 'step nimbly harry', 'ron untidy', 'dobby blink', 'harry harry potter mustn angry', 'danger', 'harry dobby', 'stairs mouth', 'harry sprang', 'cupboard corner', 'will kill harry', 'harry flay', 'joke harry', 'birthday', 'potter', 'magic harry', 'weren', 'lock', 'maniac', 'lay bed', 'h p e rr', 'ron harry', 'george ron eld', 'george', 'haven', 'dad didn', 'didn will long', 'ron jerking', 'hedwig', 'cupboard stairs', 'george passenger', 'harry room', 'harry george', 'george watch', 'fred george', 'harry hear', 'ron harry george', 'inch inch', 'hedwig harry', 'moon harry couldn', 'harry ron', 'dobby warning', 'harry fiasco', 'idea joke', 'dudley dursley', 'malfoys', 'ghoul attic', 'harry stupid dobby', 'fred', 'berserk', 'yard harry', 'ron house', 'weasley', 'cowered rage', 'bill charlie', 'fred chest', 'arthur', 'ginny ron undertone harry', 'autograph harry', 'wizard harry', 'gilderoy lockhart', 'lawn', 'peony bush', 'gerroff gerroff', 'knobby bald', 'gerroff', 'lasso', 'bet', 'harry finger', 'dad', 'mundungus fletcher', 'george muggle', 'arthur weasley', 'harry house', 'bullfrog', 'ginny ron', 'ronald room harry', 'harry chudley', 'martin miggs mad muggle', 'ron magic', 'george bedroom', 'harry sit dinner', 'weasley ginny', 'harry ginny', 'ginny', 'trick man', 'lockhart', 'harry ron elder', 'don meet diago alley', 'george planning', 'quidditch long didn', 'exam result', 'wl', 'bill', 'bill old weasley', 'charlie', 'charlie romania', 'bill egypt', 'roar', 'joke', 'chimney don worry', 'arthur mrs', 'soot don', 'don panic', 'dia gon alley', 'alley sooner', 'bell', 'pale', 'malfoy', 'rang bell', 'scar broomstick', 'borgin', 'nez', 'malfoy lip', 'arthur weasley harry', 'insert candle', 'wizard blood', 'hurry borgin', 'minute', 'jumpy harry', 'knockturn alley', 'tray', 'm harry yeh', 'leapt', 'dragon dung', 'skulkin', 'harry don', 'yeh', 'lookin fer', 'harry harry harry', 'bushy brown', 'gringotts harry', 'harry yeh', 'harry glasses', 'trou ble', 'don bite', 'granger', 'gringotts goblin vault', 'lee jordan', 'fred george lee jordan', 'magic', 'harry hermione undertone', 'don push', 'jaunty angle', 'ron 59', 'leapt foot', 'harry lockhart', 'cauldron harry', 'voice harry', 'harry potter malfoy', 'red ginny', 'ginny cauldron', 'wizard don', 'mrs granger', 'yell dad', 'george mrs', 'dawn', 'george percy', 'don ginny', 'george forgotten', 'wise arthur', 'ron harry harry', 'foot hedwig', 'trolley harry', 'faulty', 'foggy harry', 'george jealous', 'dark', 'blossoming blackness', 'ron hedwig jump', 'hood', 'mile ron knuckle white', 'ron foot', 'noooooo ron', 'steam billow', 'backward harry', 'loud thuds', 'hufflepuff ravenclaw', 'cruel sarcastic', 'ill ron', 'harry didn', 'faithful sidekick weasley', 'moment', 'hetty bayliss', 'angus', 'harry feel', 'slimy', 'ron beaten', 'wizardry', 'mouth', 'better harry', 'couldn barrier harry', 'stone stair', 'thomas neville longbottom', 'amazing neville', 'harry couldn', 'neville round', 'errol', 'ron neville', 'ron neville timid', 'don gran', 'neville stuffed', 'harry know', 'don suppose', 'people hall', 'couldn', 'ron don', 'mrs weasley', 'guilt harry', 'don mind', 'didn lockhart', 'harry harry harry lockhart', 'scar harry', 'harry start', 'harry glass', 'tufty', 'pale green', 'ron didn', 'isn justin happily', 'tell glad', 'wasn t.', 'beetle button', 'beetle', 'volley', 'firecracker', 'quidditch minutes', 'harry m m colin creevey', 'colin creevey', 'colin', 'dad milkman couldn', 'm harry', 'crabbe', 'meet harry', 'creevey lockhart', 'bell rang', 'castle harry', 'deaf harry', 'harry frank', 'better', 'ginny will', 'snap harry', 'merlin', 'dean thomas', 'miss hermione granger', 'hermione granger', 'beamed lockhart', 'point gryffindor', 'dean seamus', 'neville cowering', 'breath lockhart', 'seamus finnigan', 'nip', 'ron muttered', 'avoid colin creevey', 'harry schedule', 'harry glad', 'harry shake awake', 'whassamatter harry', 'oliver harry', 'crack dawn', 'meet field', 'stairs harry', 'colin brandishing', 'colin m hurry', 'weren harry weren colin', 'quidditch colin breathlessly', 'fred george weasley gryffindor', 'harry quafe', 'colin awe', 'keeper', 'colin didn', 'fred george weasley', 'katie 107 bell', 'angelina johnson yawning', 'weasley head', 'oliver george', 'hung grass', 'george funny clicking noise', 'don believe', 'flint', 'aren', 'seeker weasley', 'george jump alicia', 'colin harry', 'bush', 'ron bush', 'bin wonderin', 'mighta bin', 'lockhart harry', 'ron slug', 'ron bend basin', 'boarhound fang', 'givin', 'fer', 'aren keen', 'startin ter', 'jerking head ron', 'tryin', 'neville longbottom', 'don blame yeh fer tryin ter', 'bu', 'bin givin', 'lockhart yeh didn', 'chin don', 'bin growin', 'harry reason believe', 'jus', 'harry beard', 'rang potter weasley', 'filch', 'magic weasley', 'argus filch caretaker', 'gloom', 'ron feel', 'gladys gudgeon bless', 'veronica smethley', 'litde drowsy', 'great scott', 'ron wasn', 'neville dean', 'moonlight', 'ginny weasley', 'garden shed', 'oliver wood', 'missile harry', 'don fulfill', 'inch', 'nick harry', 'nick fold', 'nick wave', 'want join', 'nick shake', 'juggle head polo', 'patrick delaney', 'nick stuffed', 'harry nick quickly', 'filch isn', 'frog brain', 'norris', 'pupil filch', 'manacle hung wall filch desk', 'rat intestine', 'fall filch', 'harry filch', 'distract filch', 'z. nettles', 'd. j. prod didsbury', 'brick red', 'don breathe', 'nick', 'wouldn', 'patrick', 'keenly harry', 'george weasley', 'harry shivered', 'dungeon', 'foot', 'ron nervously', 'harry wasn', 'myrtle harry', 'ron dungeon', 'smell', 'nicholas de mimsy', 'walk harry', 'bellow oy myrtle', 'don tell', 'myrtle', 'don lie myrtle', 'sobs', 'bitterly dungeon wall', 'nick shoulder', 'patrick nick', 'patrick head', 'nick nick frighten', 'hear', 'soo', 'smell blood', 'hermione', 'sudden gasp', 'leapt backward', 'norris caretaker', 'bustle noise', 'cat h p x', 'argus filch', 'granger lockhart', 'norris harry', 'squib rubbish', 'weren hungry ron', 'bell ron', 'earth squib harry', 'squib', 'attacker', 'skower', 'norris ron', 'petrify filch', 'joke ron', 'harry ron response', 'wizards don', 'ron fu', 'sprang roll', 'ron justin finch fletchley', 'granger myth', 'helga hufflepuff', 'tale', 'heir slytherin', 'loony ron', 'harry hear slytherin', 'better gryffindor', 'throng colin creevy', 'finch fletchley', 'poke harry', 'hurry', 'teddy', 'fiithy spider', 'toy broomstick', 'mirror', 'myrtle harry ron', 'chin girl bathroom', 'ron percy weasley', 'don care', 'finger cat', 'ginny percy', 'ron weren', 'ron thick', 'h t', 'villager lockhart', 'howl harry', 'seeker', 'don hesitate', 'harry sin', 'knotgrass', 'harry seeker', 'boos', 'leaden sky', 'bludger slytherin', 'george bludger', 'adrian pucey bludger', 'harry head harry', 'fred swung bludger bludger', 'harry bludger', 'harry air', 'bat', 'madam hooch', 'whistle rang harry', 'fred george bludger', 'harry oliver george angrily', 'wood', 'harry slytherin', 'fred george hear harry', 'whistle harry', 'zigzagged', 'adrian pucey', 'harry ear', 'harry hung', 'broom knee', 'harry m fix', 'don harry', 'granger escort', 'madam pomfrey', 'pomfrey grimly', 'pajama', 'cuff harry', 'doesn swung', 'pomfrey', 'filthy', 'harry wake', 'dobby didn harry potter', 'harry heave', 'dobby sponge', 'dobby lip', 'dobby nodding', 'dobby hid', 'dobby didn', 'dobby dream harry', 'dobby pluck pillowcase', 'mark house', 'dobby freed', 'forever dobby', 'bludger harry', 'know dobby', 'freeze horrorstruck', 'harry potter meddle sir', 'dangerous dobby harry', 'dobby dobby mustn', 'moan dobby', 'harry potter dobby', 'mcgonagall', 'minerva', 'stairs bunch grape', 'ray moonlight lie', 'colin rigid', 'harry beds', 'colin dobby weren', 'percy weasley', 'harry haven', 'keyhole harry', 'fine harry', 'harry light', 'wish dobby', 'chameleon ghoul', 'rage', 'filch neville', 'know m squib', 'duel ron', 'harry hermione', 'don want', 'brown', 'harry ron time', 'miss granger', 'jaw', 'disarm don', 'harry swing', 'disarm lockhart', 'harry knee', 'harry foot', 'neville justin', 'harry leapt', 'longbottom cause', 'matchbox neville round', 'idea lockhart', 'screams', 'don potter', 'justin grin', 'harry justin', 'hall', 'cousin dudley', 'harry mouth', 'il96 harry lie awake hour', 'justin', 'norris colin creevey', 'couldn justin', 'harry ernie', 'didn chase', 'ernie', 'balaclava speak', 'aren yeh', 'harry stamp stairs', 'justin finch fletchley', 'nick longer', 'justin harry', 'wee potter', 'justin danger', 'harry macmillan', 'backward tongue', 'nick stairs', 'gargoyle lemon', 'griffin', 'harry wait', 'claw', 'leapt slytherin harry', 'hung limply', 'grubby', 'didn harry', 'noise', 'phoenix harry', 'bird poke', 'balaclava', 'wasn harry', 'talkin ter', 'myrtle bathroom', 'isn', 'nick fate', 'harry hurry', 'ginny didn', 'ron knowingly', 'fred george ginny chosen', 'harry christmas', 'percy', 'badge', 'ron hide', 'hear gloop gloop', 'glass', 'book', 'murky brown', 'harry lie facedown', 'ron gaze', 'draco malfoy', 'peter weasley', 'ron nose', 'lucius', 'colin potter picture', 'autograph lick', 'heir harry ron', 'ron jaw', 'ron clench', 'azkaban harry', 'hearty guffaw', 'pomfrey couldn', 'ron hundredth', 'filch harry', 'slam', 'glance filch', 'wet bathroom', 'm. riddle', 't. m. riddle', 'harry amazement', 'trace', 'tower harry', 't. m. riddle diary', 'don chuck', 'chuck harry', 'thirty wl', 't. m.', 'knowingly', 'culprit', 'wash', 'ginny weasley harry', 'dwarf', 'arry potter', 'harry diary', 'classroom', 'valentine ginny', 'wand harry', 'ron ron', 'tom', 'mean harry', 'thegirl', 'don word', 'wizard didn', 'marvolo grandfather', 'tom harry', 'riddle hurry', 'dungeon harry', 'lit', 'yeh outta', 'don good', 'knockturn alley didn harry', 'rotter song ernie macmillan', 'harry ron gloomily', 'didn guidance', 'charlie outdoor type', 'stairs dormitory', 'cloak lie', 'neville', 'ron dean', 'dean swore', 'harry woke', 'stair', 'library harry', 'boos shouts', 'cup gryffindor', 'weren take', 'bed', 'harry ron shake', 'lee didn', 'ravenclaw', 'cloak harry', 'neville dean sea mus', 'ron stub', 'ron swore', 'bin expectin', 'catch', 'fang', 'yeh ter', 'rang corridor', 'don apply', 'granger bell rang', 'ron leapt', 'harry dean hung', 'harry m sorry', 'hannah', 'shrivelfig harry ron draco', 'ernie hannah', 'unhappier', 'ron twirl', 'centaur', 'magic wouldn', 'weasley lockhart', 'stiffen resolve', 'george ginny', 'voice', 'joy sight', 'tooth harry', 'glow harry', 'mile', 'shield', 'fang didn', 'harry stuff', 'loud click', 'ron fang', 'fang wasn', 'aragog', 'ron kill', 'aragog click', 'mosag', 'fang harry', 'eye weren', 'noisy rocky', 'fang flung window', 'pat', 'aragog wouldn', 'foot hide', 'cloak', 'harry swung', 'moon', 'ron ron wake', 'harry ron girl', 'bathroom harry', 'ron bitterly', 'kind', 'dobby teetering edge', 'harry harry ginny', 'ron ginny', 'ron grinning', 'tell', 'percy didn', 'harry m starving harry', 'attacker ron', 'lay', 's. pipes', 'answer', 'understand parseltongue', 'nick nick', 'hermione ravenclaw', 'mirror rods jaw', 'mcgon agall', 'ron aghast', 'dormitory harry', 'hide', 'skeleton', 'flitwick burst', 'mcgonagall harry', 'staffroom', 'monster lockhart', 'gilderoy', 'flitwick don recall', 'percy wasn', 'wasn stupid', 'harry sun', 'couldn ginny', 'robe jade', 'bandon banshee harelip', 'harry harry lockhart', 'harry myrtle', 'hornby', 'parseltongue harry', 'ron shake head', 'weren hear', 'ron gasp', 'knob ron harry', 'harry jabbed wand', 'pipe don', 'harry muttered', 'wand lit', 'ron lockhart', 'lockhart lockhart', 'harry flung', 'ron m', 'wait lockhart', 'solid wall', 'ginny wake harry', 'ginny head', 'tom tom', 'stare harry', 'talk harry', 'ginny simply', 'glad', 'squib cat harry', 'armando dippet', 'ginny write', 'isn life', 'tom marvolo', 'witch harry', 'harry brain', 'albus dumbledore', 'eerie', 'crimson', 'swan', 'twice', 'duel', 'couldn kill', 'mother', 'harry m', 'cast', 'horrorstruck harry', 'flight harry', 'kill himi', 'eerie song', 'scaly', 'sword', 'harry potter riddle voice', 'cry harry', 'fawke', 'die harry potter', 'm hurry harry', 'harry lap diary', 'mile floo', 'ginny fang', 'lockhart ron', 'danger lockhart', 'harry isn', 'ginny grab', 'hide pipe', 'ginny ginny', 'ginny hasn', 'diary harry', 'dumbledore', 'weasley hospital', 'wiser wizard', 'pomfrey awake', 'alert kitchens', 'gilderoy harry', 'sword gilderoy', 'knee harry', 'heir', 'proof harry', 'harry dumbledore simply minute speak', 'dobby elf odd', 'meaningfully harry', 'harry swift', 'dobby dobby', 'clench unclench', 'dobby squeal', 'throw dobby', 'dobby free', 'crash stairs time', 'swing cloak', 'harry potter free dobby elf', 'harry moonlight', 'harry grinning', 'dobby harry dobby', 'harry sock', 'harry potter great far dobby know sobbed farewell harry potter', 'jam doughnut', 'george head ravenclaw', 'win tease', 'thing', 'uncle vernon', 'dudley harry', 'harry spellbooks', 'ron weasley ron bellow', 'hedwig night', 'didn flinch', 'harry parents', 'lily james potter', 'harry affectionate', 'ron tall', 'ginny harry', 'bill take', 'tombs wouldn', 'ron s. percy head', 'sun harry', 'ron beneath harry', 'untrustworthy', 'george beetles soup', 'bye ron harry', 'percy head', 'wasn t. heart', 'broomstick', 'kit', 'jaws harry froze', 'ftom king', 'm. mcgonagall', 'headmistress harry', 'mistake harry', 'harry happy', 'harry untidy', 'nosy', 'harry mother', 'harry shins', 'marge attend', 'harry duddy', 'kitchen harry', 'ruddy', 'hedwig asleep', 'ron hermione', 'errol hedwig', 'shrieking stairs harry', 'hall harry', 'harry hall', 'growl harry', 'harry time', 'vernon petunia', 'don smirk', 'aunt marge', 'namby pamby', 'beat', 'petunia', 'don rise', 'dripping marge', 'marge', 'couldn afford', 'brandy glass', 'nosh petunia', 'pardon', 'brandy vernon', 'pie gape', 'feel', 'harry s. boy', 'vernon leg harry', 'cupboard door', 'second', 'didn muggle', 'broomstick harry', 'neck harry', 'bang harry', 'decker', 'stan shunpike', 'harry choo fall', 'stan didn', 'harry annoy', 'stan', 'neville longbottom harry', 'ernie prang', 'harry nervously', 'ern wales', 'harry stan', 'abergavenny minute stan', 'knight', 'neville harry', 'magic cornelius', 'crisis don', 'vampire', 'dinnit ern', 'well', 'stan pimple', 'choo', 'harry wouldn', 'harry passenger', 'alley harry righto stan', 'lie low', 'diagon alley', 'harry ern', 'stan low', 'pavement harry', 'magic stan leapt', 'didja neville', 'stan ern', 'tom fudge', 'bye harry', 'ern tom', 'tom lantern', 'fudge harry', 'tom innkeeper', 'marjorie dursley', 'harry rim', 'didn occur harry', 'cauldron hang', 'law harry', 'don send', 'crumpet harry tom', 'harry fudge', 'black harry', 'wardrobe hedwig harry', 'potter don hesitate', 'hedwig sky', 'harry eat breakfast', 'harry backyard', 'harry didn homework', 'florean fortescue', 'witch burning', 'harry free sundaes half hour harry', 'apothecary replenish', 'malkin', 'hang harry', 'harry shop corner', 'blotts', 'cauldron', 'forget', 'dad evening', 'menagerie wasn', 'inch wall', 'newts harry ron', 'jewel', 'wastepaper bin', 'ron stuff', 'tiger harry', 'don ridiculous ron mr', 'azkaban guards', 'george start', 'harry nice', 'yo', 'george heave', 'fred dug sumptuous', 'aren ron hasn', 'ron win time', 'harry thud wood', 'guard', 'suppose', 'wasn happy', 'george crouching', 'tom wake', 'harry short stretch', 'street harry', 'harry trip knight', 'harry trolley', 'ginny catch', 'ginny huffily', 'r. j. lupin', 'nutter', 'strawberry mousse', 'sugar', 'potty weasel', 'ron massaging', 'knuckles m go', 'ron foot harry', 'patch', 'neville cloak harry', 'pain neville', 'beneath cloak', 'kneeling neville', 'nervously', 'ginny neville', 'harry harry didn', 'meow neville', 'boars harry', 'ron hurry', 'hall light', 'weasley ron', 'pomfrey nurse', 'foot long', 'harry puffed chest', 'woooooooooo harry', 'wasn cocky', 'don fred didn pass harry', 'dad azkaban time', 'sword scabbard', 'scurvy braggart rogue knight', 'hung wall', 'cadogan', 'lit dim crimson', 'firelight', 'trelawney', 'warn', 'neville sooner', 'gloom harry', 'hang', 'harry teacup', 'blob', 'harry cup', 'wasn didn', 'lavender brown', 'sibyll trelawney', 'trelawney classroom', 'ron cheer', 'clatter', 'harry stupid', 'cup ron', 'trelawney didn', 'carrot', 'clip hasn hasn bin', 'forefinger spine', 'oaf', 'malfoy harry', 'beau', 'aren harry', 'beam yeh', 'don insult', 'lesson yeh', 'fer hippogriff', 'yeh walk', 'bows yeh', 'hippogriff', 'parvati', 'yeh yeh', 'tha', 'tha harry', 'hippogriff yeh', 'yeh don', 'backward didn', 'chestnut harry', 'loud harry', 'pomfrey mend', 'don reckon', 'moanin', 'bin', 'reckon', 'shoulda', 'hippogriff fer later', 'tankard table', 'tankard', 'barrel', 'harry lettin', 'swagger dungeon', 'longbottom neville', 'longbottom', 'ron roughly don', 'hunt', 'neville sweat', 'ladle stone basin', 'icy jet', 'ron savagely', 'shabby', 'loony loopy', 'thomas amazement', 'dean', 'riddikulus riddikulus', 'easy m', 'fox', 'red neville', 'frighten didn', 'neville neville', 'mummy foot', 'harry head', 'dean dean', 'crab riddikulus', 'ron frozen', 'riddikulus', 'harry feet', 'harry didn harry', 'lesson', 'draco', 'battlefield', 'kappas creepy', 'didn t.', 'lesson lesson', 'quidditch season', 'quidditch cup', 'angelina johnson katie bell', 'oliver angelina', 'wetter night', 'quidditch cup harry', 'harry portrait hole', 'zonko s.', 'chart harry', 'warning', 'stupid', 'ron don hurt', 'paw ron', 'cat chase', 'better hurry', 'ron angrier harry', 'dean thomas good', 'zonko joke', 'hall filch caretaker', 'tower password', 'doze fortuna', 'portrait swing', 'wear harry harry', 'harry colin', 'awe harry', 'owlery hedwig', 'lesson harry', 'shouldn difficulty kappas', 'trick break', 'harry lap', 'honeyduke', 'nick gryffindor', 'evening harry', 'portrait', 'holdup', 'password', 'tiptoe go ginny', 'castle', 'fitch', 'harry ron hermlone', 'filch dungeons', 'snape harry', 'whet', 'wilder hannah', 'cap', 'harry moment', 'katie', 'giggle', 'oliver hufflepuff', 'bitter sidelong', 'wolf', 'snout', 'ron catch', 'roorn', 'oliver alicia', 'captain', 'harry rise', 'sky', 'harry numb', 'wetter life', 'yell', 'eerie silence', 'didn clue', 'didn george fred', 'george beat', 'quidditch match', 'don beat', 'ron hermione lookin', 'willow harry', 'whomping willow', 'flitwick bring', 'broomstick chapter', 'harry hospital', 'didn argue complain', 'harry hollow', 'don worry', 'don essay', 'broomstick chance', 'don ray', 'prey', 'm', 'psst harry', 'george inside', 'george jokes', 'harry young carefree', 'george wand', 'don bother', 'cellar honeyduke', 'george briskly', 'don forget', 'filch didn', 'fred george years horrible', 'noise harry', 'cellar wooden', 'bald head', 'counter honeyduke', 'haven harry', 'aren harry m harry', 'ron goggle', 'george reckon filch', 'cellar know harry', 'jelly', 'didn cloak', 'zonko shrieking', 'curvy', 'red harry', 'tankard harry', 'mcgonagall flitwick', 'dripping butterbeer', 'mobiliarbus', 'gillywater', 'rosmerta voice', 'rosmerta', 'rosmerta dear don', 'flitwick foot', 'rosmerta fudge', 'james potter harry', 'tankard loud', 'flitwick', 'james married lily', 'godfather harry', 'james lily', 'james potter', 'musta bin ter', 'harry lily james house', 'bin lily james secret keeper', 'lily james', 'care abou', 'harry ter', 'motorbike givin', 'bin potters', 'peter pettigrew', 'harp', 'lily james sirius', 'heap', 'cruel', 'feet harry', 'don affect', 'album', 'harry harry didn answer', 'madman harry dangerous', 'don danger', 'red swollen', 'ywve hear', 'bellowed flung harry neck', 'hagrid', 'sob wave', 'buckbeak isn', 'buckbeak hippogriff', 'harry fer', 'knee ve bin', 'abou buckbeak', 'lettuce', 'bein azkaban', 'kep', 'norbert', 'dragon hagrid', 'livin', 'bein', 'weren keen', 'heap parcel', 'maroon', 'broom harry', 'dad harry', 'nimbus', 'alley mean', 'ill', 'ron hop', 'shrill tint', 'pocket sneakoscope', 'ron pocket', 'menagerie', 'reason annoy', 'flitwick filch caretaker', 'jinx free', 'jinx', 'ron furious', 'noisy wood', 'afford wood', 'ron firebolt', 'harry mcgonagall', 'firebolt', 'wonder', 'ron fine', 'light', 'wand', 'harry boggart', 'cupboard desk', 'conjure', 'harry charm', 'ride', 'expecto patronum', 'patronum harry', 'shouldn', 'expecto patronum harry', 'alight', 'lose', 'harry expecto patronum harry', 'lily harry', 'minute harry', 'james', 'shouln', 'patronum harry bellow', 'harry wand', 'tower ravenclaw', 'slytherin', 'harry evening', 'silence harry', 'jaws mouth', 'harry jaw', 'passwords neville', 'harry ravenclaw', 'harry minutes firebolt', 'red look', 'hermione harry', 'weird spiky shape long', 'ron head', 'ron george', 'cho chang', 'joke firebolt', 'blur harry', 'alicia spinnet', 'katie bell knee', 'awoke start', 'harry didn ron', 'people ravenclaw', 'rejoin', 'hooch', 'whistle', 'broomstick firebolt', 'jordan', 'brake jordan', 'katie bell', 'alicia harry', 'cho fall', 'chang comet', 'knock broom harry', 'hooch whistle', 'scarlet blurs', 'alicia angelina katie', 'angelina johnson george', 'block ray moonlight lay', 'dean thomas light', 'weren dream', 'portrait hole', 'tower', 'woke', 'neville ron', 'neville didn', 'bin comin', 'chris mas', 'bin feelin', 'yeh weren', 'blamin yeh', 'gawd', 'yeh practicin', 'quidditch ev ry', 'tha harry ron', 'yeh ron', 'ron crane', 'zonko harry', 'map pocket', 'don understand', 'gasp', 'habit', 'tower belong harry neville', 'palm harry', 'zonko', 'tricks', 'nick .', 'zombie harry', 'hem harry cloak', 'cloak giveaway', 'hide shadowy', 'harry s.', 'pomfrey harry', 'guilt', 'hallucin', 'magic downward', 'harry potter law', 'swollen shut harry', 'spat', 'bag zonko trick', 'don throw', 'map didn', 'prongs', 'moony', 'padfoot', 'idiot', 'wormtail', 'jaw go', 'severus', 'bunch', 'ron neck', 'jus exac ly', 'don work', 'jus go', 'ron weakly', 'crabbe goyle', 'harry better', 'flitwick classroom', 'flitwick reprovingly harry', 'classroom harry', 'flitwick angry', 'dim', 'parvati lavender', 'didn didn', 'granger leave', 'wasn holiday', 'exam', 'charlie weasley', 'ear harry', 'couldn walk', 'fred george weasley deal', 'george jokes harry', 'air wake', 'jug beneath', 'cat', 'castle weren', 'cho', 'kickoff', 'bell johnson', 'weasley weasley wood', 'lee', 'lee point', 'hooch flint', 'quaffle', 'warrington warrington', 'george weasley warrington', 'johnson', 'fre weasley', 'hooch zooming', 'hooch blow', 'lee silence', 'keeper lee jordan', 'lee commentary', 'katie bell katie', 'katie cart', 'hooch whistle rang', 'possession johnson', 'quaffle flint', 'bole derrick', 'keeper quaffle', 'bludger warrington knock quaffle', 'harry catch', 'foot harry', 'foul harry', 'great heights', 'scores lee', 'angelina johnson', 'harry neck', 'alicia katie', 'plaster crimson', 'beam harry borne', 'air harry', 'exam harry', 'bin coope long', 'morning harry', 'witch', 'fortescue choco', 'nut sundae', 'mark', 'boggart minute', 'belt harry', 'didn join harry ron', 'crystal ball', 'hurry parvati', 'ronald weasley', 'harry potter tower room hotter', 'curtain', 'hippogriff appear', 'eye', 'trelawney head', 'moment harry', 'trelawney harry', 'don come', 'dissendium harry', 'harry cloak', 'yeh don wan yeh', 'comin harry ron hermione', 'macnair yeh', 'buckbeak yard', 'buckbeak', 'idiot ron ron', 'hear wild', 'sprang harry', 'inch long', 'jaws', 'torso', 'ron leg', 'knuckle', 'george gotten', 'harry patch dim', 'moan', 'ron moaned', 'harry trap dog', 'animagus ron', 'hung', 'waxy', 'rang harry', 'harry chest', 'knuckle harry', 'backward wall', 'inches harry', 'harry grunt', 'don roar harry', 'don deny', 'ftom', 'lunar chart', 'hermione m hermione', 'crookshanks leapt', 'trick haven', 'somebody harry', 'weren couldn', 'dot', 'wizard animagus', 'peter pettigrew chapter', 'peter', 'don understand harry', 'peter lupin nodding', 'peter alive', 'peter pettigrew azkaban', 'peter pettigrew animagus', 'foolhardy', 'sober', 'wolf wait moon', 'harry james potter', 'james sirius', 'peter smallest slip beneath willow', 'sirius james', 'peter wormtail', 'james prongs', 'animal harry', 'harry ron hermione', 'james jealous', 'james talent', 'glance', 'haven hear', 'harry azkaban', 'roar rage', 'don talk don', 'catch joke severus', 'glint', 'knee', 'finger', 'peter couldn', 'don peter', 'peter true harry', 'secret keeper', 'peter moment', 'night', 'rat', 'flash blue white', 'colorless hair', 'azkaban peter', 'voldemort', 'peter don', 'peter difficulty', 'peter will', 'james pettigrew', 'lily james secret keeper', 'dart', 'harry dormitory year', 'harry pettigrew', 'weren commit', 'weren peter', 'rejoin pettigrew', 'black', 'transform', 'peter picture', 'peter alive harry', 'journeyed north', 'peter lupin', 'aren ron', 'pettigrew', 'james harry', 'harry james wouldn', 'james understand harry', 'lily james voldemort', 'don lie', 'peter don understand', 'bye peter', 'wall harry', 'don kill harry', 'm. don reckon', 'muttered mobilicorpus', 'hung inches', 'paw knot', 'word', 'limb', 'claw paw', 'ron pettigrew', 'jaw jaw claws', 'bang burst', 'manacle ron', 'yelp', 'expecto patronum expecto hermione', 'expecto expecto', 'harry couldn speak', 'patronus', 'forward grass', 'dementor', 'sweat harry', 'thunder', 'merlin second', 'nasty', 'weasley granger', 'black bewitched', 'behavior', 'harry awake', 'listen harry', 'animagus pettigrew', 'fairy tale', 'didn arrive time', 'stupid trick', 'harry didn clue go', 'yell couldn', 'beneath foot', 'hourglass', 'harry neck darkness', 'jug hermione', 'load', 'macnair', 'ron quick', 'walden macnai', 'don wan ter', 'beaky yeh', 'ron dive', 'bowl harry', 'cloud', 'harry mustn', 'hear burst song', 'head', 'harry don understand', 'moon harry', 'quick', 'fang boarhound', 'don go', 'harry flung bush', 'mark soft', 'lake bush', 'macnair harry', 'buckbeak harry', 'yotm better', 'airborne harry', 'leapt chair', 'harry robe', 'battlement harry', 'truly', 'harry gazed', 'kiss', 'pomfrey hear', 'bellowed', 'hear severus', 'stag', 'las night', 'free bin celebratin', 'mighta', 'eat anythin las night', 'yeh hear', 'packin', 'leavin isn', 'happenin harry', 'magic don', 'don lupin', 'don feel', 'stuff', 'harry carriage', 'offer', 'harry potter don want', 'pettigrew harry', 'dad conjure patronus', 'don recall', 'malfoy quidditch', 'hang rs', 'harry read reread', 'trolley', 'godfather godfather', 'godfather', 'exit hedwig', 'dursley wake', 'dursley hummed', 'dursley didn', 'tabby cat', 'weren young', 'baker s. eyed', 'harvey', 'stare don', 'cat didn', 'jim mcguffin', 'jim ted weatherman don', 'howard isn harry', 'dursley lie awake', 'couldn petunia', 'albus dumbledore albus', 'flock', 'reason lose', 'lemon', 'mcgonagall gasp', 'couldn kill harry potter voldemort power', 'cara better', 'beard hide', 'motorcycle', 'tuft jet', 'wouldn t. scar come handy', 'whiskery kiss', 'g', 'birthday harry', 'dudley birthday', 'harry spiders cupboard stairs', 'cupboard harry', 'wear', 'thing harry', 'scar', 'petunia dudley', 'angel harry dudley', 'harry plates', 'atta boy dudley', 'dudley unwrap', 'figg', 'harry direction dudley mouth', 'photograph cat', 'paws tufty', 'yvonne vacation majorca', 'harry weren', 'zoo aunt petunia', 'don cry mummy', 'doorbell rang', 'vernon didn', 'dudley brown', 'shrunk wash', 'dudley gang', 'maniacs', 'harry best', 'hobby', 'eat zoo restaurant dudley tantrum knickerbocker', 'dudley piers', 'weren t.', 'vernon dudley', 'leapt howls', 'dudley gibber', 'harry piers', 'weren harry', 'harry dream', 'bald', 'baggy', 'harry longest punishment', 'malcolm gordon', 'stupid dudley', 'harry hunting harry', 'figg s. mrs', 'tub', 'harry mail', 'harry dudley poke', 'didn belong', 'h.', 'vernon kitchen', 'dad dudley', 'harry point', 'red green', 'didn stop', 'harry dudley room', 'dudley wasn', 'keyhole dudley', 'don answer', 'petunia didn', 'harry cupboard', 'cupboard silence', 'marge dudley', 'harry trip upstairs', 'harry dudley harry', 'rang clock', 'postman', 'vernon lap harry', 'milkman hand', 'harry leapt air', 'harry waist', 'nightfall dudley', 'h. potter', 'harry grab', 'vernon stand', 'hasn dudley', 'dud ley', 'harry eleventh birthday', 'gentleman kindly', 'harry minutes', 'lay watch birthday', 'wake dudley annoy', 'shivered harry', 'mane hair wild', 'couldn cup', 'sofa dudley', 'dad yeh', 'dursley yeh', 'knot', 'trodden harry', 'birthday yeh', 'teapot', 'don touch', 'fattenin', 'dursley don', 'yeh m keeper', 'abou hogwart fer', 'jus second', 'fill', 'abou harry', 'explode dursley', 'dad weren', 'dursley ve', 'gasp horror', 'harry wizard silence', 'harry wizard', 'sofa', 'yeh reckon', 'abou time yeh', 'wizardry headmaster', 'harry go white', 'harry potter knowin story', 'yeh 3', 'yeh ter hogwart knowin', 'great myst', 'sayin', 'gulpin', 'nah', 'day harry', 'myst', 'mark yer', 'leapt sofa', 'sword m warning', 'big myst', 'dunno human', 'bidin time', 'outta kinda', 'harry something go night', 'lily james potter son', 'yeh didn', 'yeh yer letter', 'harry kip', 'alley harry', 'payin fer deliverin', 'humbugs teabag', 'mm', 'fer gringotts', 'gringotts', 'fer yeh', 'storm harry', 'rob gringotts harry', 'yeh die hunger', '0', 'bungler', 'harry london', 'tom m', 'potter m proud', 'harry shake', 'doris crockford', 'harry doris crockford', 'quirrell tremblin', 'tremblin nervous', 'money harry', 'dragon liver', 'barn brown', 'harry swarthy', 'beard harry', 'harry counter', 'free goblin', 'cart', 'm go', 'gringotts goblin', 'malkin squat', 'stupid minute', 'mmm harry', 'isn gamekeeper harry', 'harry forgettin', 'knowin quidditch don', 'muggle', 'knowin', 'drag harry', 'vindictus viridian', 'yeh couldn', 'barrel slimy', 'claws hang', 'knut scoop', 'red don', 'outta', 'ollivander', 'close harry', 'mahogany', 'misty', 'measure harry', 'wave harry', 'inch nice', 'wand head', 'harry wand box', 'harry pale stare', 'curious', 'sun hung', 'paddington', 'fer bite', 'don know magic', 'don worry harry', 'harry cupboard force', 'lie bed', 'lift grunt', 'harry didn school don', 'harry swing round', 'ginny quiet', 'backpack', 'm fre m george boy', 'mother cara', 'm george', 'twin', 'walk', 'don stop don', 'harry harry harry potter', 'fre george', 'handkerchief ron', 'mom geroff', 'nosie', 'goggle zoo', 'don dare', 'cry don ginny will send load', 'george joking', 'mark nose', 'bye harry ron', 'harry potter ron', 'george joke ron', 'bill head', 'bill old', 'charlie old', 'wear dudley old', 'shouldn t. mean', 'bettie bott', 'nice', 'ptolemy harry', 'frog', 'nicolas flamel', 'morgana', 'don ron', 'morgana hengist', 'paracelsus merlin', 'bertie bott', 'ron lap', 'ron weasley ron muttered', 'neville toad', 'george give bet', 'mom dad', 'don know', 'm. don suppose', 'bill africa gringotts ron', 'gringotts don', 'thickset', 'afford', 'ron don foot', 'ron ron leapt', 'dad doesn', 'conductor', 'jus round', 'neville cloak', 'fred hurt', 'stream wall', 'don flap', 'cap hall', 'troll harry', 'abbott hannah', 'susan hufflepuff', 'susan', 'terry ravenclaw', 'terry', 'finch fletchley justin hufflepuff harry', 'granger hermione', 'weren people', 'sally anne potter', 'hall potter', 'ruff', 'thomas dean', 'lisa ravenclaw ron turn', 'harry zabini blaise slytherin', 'pumpkin pasty', 'dish', 'gravy ketchup', 'nick prefer', 'nicholas', 'swung neck', 'dad muggle', 'mom didn', 'ron gran', 'filch caretaker', 'magic bedtime', 'neville head', 'hear zoom', 'hung portrait', 'caput draconis', 'percy portrait swing', 'ron treacle tart', 'turban didn', 'nick happy', 'binns', 'turban', 'zombie weren', 'fought zombie', 'smell hung turban weasley', 'harry far', 'harry plate', 'untidy scrawl', 'hedwig lucky', 'don expect', 'asphodel', 'bezoar stone', 'melt', 'dungeon hour', 'ron freckle', 'fang rest', 'git fer', 'charlie hagrid', 'gringotts spokesgoblin', 'rob gringotts ron', 'harry gringotts', 'harry chapter', 'duel harry', 'harry darkly', 'ron couldn', 'neville broomstick', 'hang broomstick', 'gran', 'george weasley complain', 'hurry harry', 'hooch upf', 'harry broom', 'whistle neville', 'jumpy', 'gasp slip', 'neville lie facedown grass heap', 'longbottom gran', 'leapt broomstick', 'broom', 'javelin', 'patil', 'follow harry', 'flitwick borrow', 'chalk bin', 'broomstick potter', 'didn scratch', 'quidditch potter', 'quidditch', 'joke dinnertime', 'century harry', 'wizard duel harry', 'couldn miss', 'bathrobe', 'ron portrait', 'harry fine neville', 'madam pomfrey mend minute', 'don leave', 'foot don', 'crabbe weren', 'ron waist', 'caughty don', 'shan don', 'harry bathrobe minute', 'weren room', 'harry portrait swing forward', 'don mind m go', 'grubby littie', 'send broomstick', 'flitwick beaming harry', 'bolt dinner', 'ron unwrap', 'mcgonagall mean', 'ball ball', 'keeper harry', 'harry swing bat', 'don worry quaffle', 'game quidditch', 'harry catch harry didn miss single wood', 'harry broomstick', 'wizard baruffio', 'lay desktop', 'seamus', 'wingardium leviosa', 'gar', 'flitwick clapping', 'granger ron', 'stairs don', 'great leap harry', 'troll', 'hear yell', 'troll berserk', 'troll couldn', 'ron stand', 'lumpy', 'lip white', 'lucky weren', 'finish', 'granger foolish', 'hang head', 'smell troll', 'noisy', 'harry quidditch', 'quidditch age', 'noisy evening', 'dean west ham fan row', 'dean good', 'oliver speech', 'george locker', 'marcus flint', 'chaser girl', 'reserve johnson', 'quaffle slytherin', 'gryffindor keeper', 'harry hasn', 'cannonball harry', 'fred weasley', 'chaser bell', 'terence higgs', 'neck neck', 'higgs', 'wham roar rage', 'hooch speak', 'dean thomas yell send ref red card', 'ron red', 'dean ron', 'harry outta', 'jordan m warn', 'spinner', 'flint quaffle', 'jerk harry swing', 'binoculars harry', 'scoop', 'broom neville', 'rule lee jordan', 'cup', 'm tellin yeh', 'meddlin', 'don concern yeh', 'nicolas flamel aha harry', 'chapter mirror', 'stairs weasley ron', 'insultin', 'ron strode', 'weren drawback', 'bathrobe harry', 'thick brown', 'pence', 'nmat', 'cloak d.', 'george wear', 'harry sweater', 'didn bang', 'flitwick read', 'harry weasleys', 'george gryffindor', 'hairs harry neck', 'cloak didn', 'mirror trick', 'harry knobbly', 'bacon aren', 'harry couldn eat', 'harry route', 'beam', 'ron mirror', 'ron paisley pajama ron', 'bill m holding', 'harry mirror', 'walk knock', 'harry don harry', 'harry harry feel', 'dwell', 'don admirable', 'chapter', 'couldn t.', 'drearn hermione', 'flamel li', 'muddy', 'george complain fault', 'don talk', 'don play', 'moment neville', 'bunny hop', 'sprang apart', 'report neville shake', 'malfoy neville', 'give neville', 'didn malfoy', 'don neville', 'flamel', 'grindelwald', 'don read', 'stone transform', 'gringotts stone', 'don slytherin', 'time', 'sorcerer stone harry didn', 'harry don forget', 'mortis hermione', 'don nag', 'weasley didn', 'poor weasley', 'harry m warning', 'weasley potter', 'neville crabbe', 'sorcerer stone harry', 'george steal', 'til', 'ron chapter', 'moleskin', 'don shoutin', 'harry guarding', 'don rabbitin', 'harry laws ron', 'dragon', 'burns charlie', 'stone fer', 'outta gringotts', 'abou fluffy', 'don pose', 'abou yeh', 'bin do readin', 'dragon head', 'die', 'norbert norbert mommy', 'charlie care wild', 'charlie answer', 'boot jus playin', 'bye norbert', 'tennis wall', 'brandy fer', 'teddy bear case', 'sobbed harry', 'mommy', 'staircase harry', 'jig malfoy', 'foot stair', 'neville harry neville', 'astronomy tower', 'doesn genius', 'longbottom hear', 'harry catch neville', 'don tell potter', 'harry didn sleep night', 'quidditch matches', 'lo st', 'ravenclaws hufflepuff', 'exams weren', 'harry halfway', 'ron astronomy', 'quirrell', 'map jupiter', 'hall neville', 'filch malfoy', 'filch hurry', 'isn filch', 'abou time', 'bin waitin fer', 'fiercely yeh', 'copyin', 'dangerous go', 'go', 'yeh yer', 'fang hagrid', 'yeh coward']



```python
doc = processed_text

keywords = Counter()
for chunk in doc.noun_chunks:
    if nlp.vocab[chunk.lemma_].prob < - 8: # probablity value -8 is arbitrarily selected threshold
        keywords[chunk.lemma_] += 1

keywords.most_common(20)
```




    [('harry', 851),
     ('ron', 416),
     ('ve', 289),
     ('don', 145),
     ('people', 107),
     ('didn', 100),
     ('hagrid', 100),
     ('thing', 96),
     ('eye', 95),
     ('lupin', 89),
     ('harry ron', 69),
     ('dumbledore', 68),
     ('snape', 67),
     ('door', 64),
     ('head', 63),
     ('yeh', 63),
     ('professor mcgonagall', 63),
     ('hand', 62),
     ('-PRON-', 61),
     ('wand', 60)]




```python
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib.pyplot import hist
from cycler import cycler

NUM_BINS = 10

def normalize(occurencies, normalization_constant):
    return [o / float(len(processed_text)) for o in occurencies]

def plot_character_timeseries(character_offsets, character_labels, normalization_constant=None):
    """
    Plot characters' personal names specified in `character_labels` list as time series.
    
    :param character_offsets: dict object in form {'xxx': [123, 543, 4534], 'yyy': [205, 2111]}
    :param character_labels: list of strings that should match some of the keys in `character_offsets`
    :param normalization_constant: int
    """ 
    x = [character_offsets[character_label] for character_label in character_labels] 
        
    with plt.style.context('fivethirtyeight'):
        plt.figure()
        n, bins, patches = plt.hist(x, NUM_BINS, label=character_labels)
        plt.clf()
        
        ax = plt.subplot(111)
        for i, a in enumerate(n):
            ax.plot([float(x) / (NUM_BINS - 1) for x in range(len(a))], a, label=character_labels[i])
            
        plt.rcParams['axes.prop_cycle'] = cycler(color=['r','k','c','b','y','m','g','#54a1FF'])
        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))


plot_character_timeseries(character_occurences, ['harry', 'ron'])

```


    
![png](output_51_0.png)
    



```python
def get_words_paragraphs(paragraphs, words):
    
    word_para = defaultdict(list)
    for p in paragraphs:
        for w in words:
            word_para[w].append(p.count(w))
            
    return dict(word_para)

harry_ron_occurences = get_words_paragraphs(hp_paragraphs[:1000], ['Harry', 'Ron'])
harry_occurences = [v for k,v in harry_ron_occurences.items() if k == 'Harry'][0]
ron_occurences = [v for k,v in harry_ron_occurences.items() if k == 'Ron'][0]

plt.plot(harry_occurences)
plt.plot(ron_occurences)

```




    [<matplotlib.lines.Line2D at 0x7faa3dc42668>]




    
![png](output_52_1.png)
    



```python
moral_words = []
with open('mfd2.0.dic', "r") as input:
    pair_lines = input.read().split("\n")
for p in pair_lines:
    moral_words.append(p.split('\t')[0])
print(moral_words[0:11])
```

    ['%', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']



```python
from tqdm import tqdm
def get_any_words_paragraphs(paragraphs, words):
    
    word_para = []
    for p in tqdm(paragraphs):
        total = 0
        for w in words:
            total += p.count(w)
        word_para.append(total)
            
    return word_para

moral_occurences = get_any_words_paragraphs(hp_paragraphs[:10000], moral_words)
plt.plot(moral_occurences)
```

    100%|███████████████████████████████████| 10000/10000 [00:08<00:00, 1231.20it/s]





    [<matplotlib.lines.Line2D at 0x7faa6bda60b8>]




    
![png](output_54_2.png)
    



```python
plt.plot([m / 4 for m in  moral_occurences[:10000]])
plt.plot(harry_occurences)
plt.plot(ron_occurences)
```




    [<matplotlib.lines.Line2D at 0x7faa3da69d30>]




    
![png](output_55_1.png)
    



```python
bool_harry_occurences = [bool(v) for v in harry_occurences]
bool_ron_occurences = [bool(v) for v in ron_occurences]
```


```python
df_moral = pd.DataFrame(list(zip(moral_occurences, harry_occurences, ron_occurences)), 
                        columns =['morals', 'harry', 'ron'])
df_moral
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>morals</th>
      <th>harry</th>
      <th>ron</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>58</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>45</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>104</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>19</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>995</th>
      <td>9</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>996</th>
      <td>96</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>997</th>
      <td>170</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>998</th>
      <td>151</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>999</th>
      <td>97</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>1000 rows × 3 columns</p>
</div>




```python
import pymc3 as pm
```


```python
fml = 'morals ~ harry + ron + harry:ron'

with pm.Model() as model:
    pm.glm.GLM.from_formula(formula=fml, data=df_moral, family=pm.glm.families.NegativeBinomial())
    
    trace = pm.sample(4000, cores=2)
```

    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pymc3/sampling.py:468: FutureWarning: In an upcoming release, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.
      FutureWarning,
    Auto-assigning NUTS sampler...
    Initializing NUTS using jitter+adapt_diag...
    Multiprocess sampling (2 chains in 2 jobs)
    NUTS: [alpha, mu, harry:ron, ron, harry, Intercept]
    WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
    WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.




<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>





<div>
  <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [10000/10000 00:13&lt;00:00 Sampling 2 chains, 0 divergences]
</div>



    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pymc3/math.py:246: RuntimeWarning: divide by zero encountered in log1p
      return np.where(x < 0.6931471805599453, np.log(-np.expm1(-x)), np.log1p(-np.exp(-x)))
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pymc3/math.py:246: RuntimeWarning: divide by zero encountered in log1p
      return np.where(x < 0.6931471805599453, np.log(-np.expm1(-x)), np.log1p(-np.exp(-x)))
    Sampling 2 chains for 1_000 tune and 4_000 draw iterations (2_000 + 8_000 draws total) took 29 seconds.



```python
import arviz as az
```


```python
def plot_traces(trcs, var_names=None):
    '''Plot traces with overlaid means and values'''

    nrows = len(trcs.varnames)
    if var_names is not None:
        nrows = len(var_names)

    ax = az.plot_trace(trcs, var_names=var_names, figsize=(12,nrows*1.4),
                      lines={k: v['mean'] for k, v in
                             az.summary(trcs,var_names=var_names).iterrows()})

    for i, mn in enumerate(az.summary(trcs, var_names=var_names)['mean']):
        ax[i,0].annotate('{:.2f}'.format(mn), xy=(mn,0), xycoords='data',
                         xytext=(5,10), textcoords='offset points', rotation=90,
                         va='bottom', fontsize='large', color='#AA0022')

def strip_derived_rvs(rvs):
    '''Remove PyMC3-generated RVs from a list'''

    ret_rvs = []
    for rv in rvs:
        if not (re.search('_log',rv.name) or re.search('_interval',rv.name)):
            ret_rvs.append(rv)
    return ret_rvs
```


```python
rvs = [rv.name for rv in strip_derived_rvs(model.unobserved_RVs)]
rvs
```




    ['Intercept', 'harry', 'ron', 'harry:ron', 'mu', 'alpha']




```python
plot_traces(trace[1000:], var_names=rvs);
```

    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
      FutureWarning,
    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
      FutureWarning,
    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/plots/backends/matplotlib/traceplot.py:216: UserWarning: A valid var_name should be provided, found {'a', 'm', 'h', 'I', 'r'} expected from {'alpha', 'harry:ron', 'mu', 'ron', 'harry', 'Intercept'}
      invalid_var_names, all_var_names
    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
      FutureWarning,



    
![png](output_63_1.png)
    



```python
# Transform coefficients to recover parameter values
np.exp(az.summary(trace[1000:], var_names=rvs))
```

    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
      FutureWarning,





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>1.000830e+02</td>
      <td>1.034585</td>
      <td>94.254635</td>
      <td>1.069114e+02</td>
      <td>1.001001e+00</td>
      <td>1.000000</td>
      <td>inf</td>
      <td>inf</td>
      <td>2.718282</td>
    </tr>
    <tr>
      <th>harry</th>
      <td>1.528062e+00</td>
      <td>1.039770</td>
      <td>1.417649</td>
      <td>1.638859e+00</td>
      <td>1.001001e+00</td>
      <td>1.000000</td>
      <td>inf</td>
      <td>inf</td>
      <td>2.718282</td>
    </tr>
    <tr>
      <th>ron</th>
      <td>1.321807e+00</td>
      <td>1.073581</td>
      <td>1.147976</td>
      <td>1.497804e+00</td>
      <td>1.001001e+00</td>
      <td>1.001001</td>
      <td>inf</td>
      <td>inf</td>
      <td>2.718282</td>
    </tr>
    <tr>
      <th>harry:ron</th>
      <td>9.831437e-01</td>
      <td>1.031486</td>
      <td>0.929601</td>
      <td>1.043938e+00</td>
      <td>1.000000e+00</td>
      <td>1.000000</td>
      <td>inf</td>
      <td>inf</td>
      <td>2.718282</td>
    </tr>
    <tr>
      <th>mu</th>
      <td>1.740978e+30</td>
      <td>inf</td>
      <td>1.005013</td>
      <td>1.589240e+44</td>
      <td>7.053257e+06</td>
      <td>69633.426720</td>
      <td>inf</td>
      <td>inf</td>
      <td>2.718282</td>
    </tr>
    <tr>
      <th>alpha</th>
      <td>4.162018e+00</td>
      <td>1.060775</td>
      <td>3.750916</td>
      <td>4.678605e+00</td>
      <td>1.001001e+00</td>
      <td>1.001001</td>
      <td>inf</td>
      <td>inf</td>
      <td>2.718282</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_moral2 = pd.DataFrame(list(zip(moral_occurences, bool_harry_occurences, bool_ron_occurences)), 
                        columns =['morals', 'harry_p', 'ron_p'])
df_moral2
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>morals</th>
      <th>harry_p</th>
      <th>ron_p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>58</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>45</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>104</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>19</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>995</th>
      <td>9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>996</th>
      <td>96</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>997</th>
      <td>170</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>998</th>
      <td>151</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>999</th>
      <td>97</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>1000 rows × 3 columns</p>
</div>




```python
g = sns.factorplot(x='morals', row='harry_p', col='ron_p', data=df_moral2, kind='count', aspect=2)

# Make x-axis ticklabels less crowded
ax = g.axes[1, 0]
labels = range(len(ax.get_xticklabels(which='both')))
ax.set_xticks(labels[::5])
ax.set_xticklabels(labels[::5]);
```

    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/seaborn/categorical.py:3717: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`.
      warnings.warn(msg)



    
![png](output_66_1.png)
    



```python
df_moral2.groupby(['harry_p', 'ron_p'])['morals'].agg(['mean', 'var'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>mean</th>
      <th>var</th>
    </tr>
    <tr>
      <th>harry_p</th>
      <th>ron_p</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2" valign="top">False</th>
      <th>False</th>
      <td>101.959707</td>
      <td>9242.035071</td>
    </tr>
    <tr>
      <th>True</th>
      <td>121.218391</td>
      <td>6328.451751</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">True</th>
      <th>False</th>
      <td>174.552727</td>
      <td>24070.153232</td>
    </tr>
    <tr>
      <th>True</th>
      <td>347.315217</td>
      <td>138049.053392</td>
    </tr>
  </tbody>
</table>
</div>




```python
import numpy as np
import pymc3 as pm
import matplotlib.pyplot as plt
%matplotlib inline

fml = 'morals ~ harry_p + ron_p + harry_p:ron_p'

with pm.Model() as model:
    pm.glm.GLM.from_formula(formula=fml, data=df_moral2, family=pm.glm.families.NegativeBinomial())
    trace = pm.sample(8000)
```

    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pymc3/sampling.py:468: FutureWarning: In an upcoming release, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.
      FutureWarning,
    Auto-assigning NUTS sampler...
    Initializing NUTS using jitter+adapt_diag...
    Multiprocess sampling (4 chains in 4 jobs)
    NUTS: [alpha, mu, harry_p[T.True]:ron_p[T.True], ron_p[T.True], harry_p[T.True], Intercept]
    WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
    WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
    WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
    WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.




<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>





<div>
  <progress value='36000' class='' max='36000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [36000/36000 00:31&lt;00:00 Sampling 4 chains, 0 divergences]
</div>



    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pymc3/math.py:246: RuntimeWarning: divide by zero encountered in log1p
      return np.where(x < 0.6931471805599453, np.log(-np.expm1(-x)), np.log1p(-np.exp(-x)))
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pymc3/math.py:246: RuntimeWarning: divide by zero encountered in log1p
      return np.where(x < 0.6931471805599453, np.log(-np.expm1(-x)), np.log1p(-np.exp(-x)))
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pymc3/math.py:246: RuntimeWarning: divide by zero encountered in log1p
      return np.where(x < 0.6931471805599453, np.log(-np.expm1(-x)), np.log1p(-np.exp(-x)))
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pymc3/math.py:246: RuntimeWarning: divide by zero encountered in log1p
      return np.where(x < 0.6931471805599453, np.log(-np.expm1(-x)), np.log1p(-np.exp(-x)))
    Sampling 4 chains for 1_000 tune and 8_000 draw iterations (4_000 + 32_000 draws total) took 60 seconds.



```python
rvs = [rv.name for rv in strip_derived_rvs(model.unobserved_RVs)]
print(rvs)
plot_traces(trace[1000:], var_names=rvs);
```

    Got error No model on context stack. trying to find log_likelihood in translation.


    ['Intercept', 'harry_p[T.True]', 'ron_p[T.True]', 'harry_p[T.True]:ron_p[T.True]', 'mu', 'alpha']


    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
      FutureWarning,
    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
      FutureWarning,
    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/plots/backends/matplotlib/traceplot.py:216: UserWarning: A valid var_name should be provided, found {'a', 'm', 'h', 'I', 'r'} expected from {'alpha', 'harry_p[T.True]:ron_p[T.True]', 'harry_p[T.True]', 'mu', 'ron_p[T.True]', 'Intercept'}
      invalid_var_names, all_var_names
    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
      FutureWarning,



    
![png](output_69_3.png)
    



```python
np.exp(az.summary(trace[500:], var_names=rvs)[['mean','hdi_3%','hdi_97%']])
```

    Got error No model on context stack. trying to find log_likelihood in translation.
    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/arviz/data/io_pymc3_3x.py:102: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.
      FutureWarning,





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>1.020028e+02</td>
      <td>95.201910</td>
      <td>1.096178e+02</td>
    </tr>
    <tr>
      <th>harry_p[T.True]</th>
      <td>1.712578e+00</td>
      <td>1.505312</td>
      <td>1.923218e+00</td>
    </tr>
    <tr>
      <th>ron_p[T.True]</th>
      <td>1.192438e+00</td>
      <td>0.985112</td>
      <td>1.447735e+00</td>
    </tr>
    <tr>
      <th>harry_p[T.True]:ron_p[T.True]</th>
      <td>1.673639e+00</td>
      <td>1.273794</td>
      <td>2.225541e+00</td>
    </tr>
    <tr>
      <th>mu</th>
      <td>4.368528e+42</td>
      <td>1.000000</td>
      <td>1.844274e+46</td>
    </tr>
    <tr>
      <th>alpha</th>
      <td>3.717309e+00</td>
      <td>3.366925</td>
      <td>4.116486e+00</td>
    </tr>
  </tbody>
</table>
</div>




```python
trace[1000:]['Intercept'].mean()
```




    4.6254256337483906




```python
trace[1000:]['harry_p[T.True]'].mean()
```




    0.5377079689607736




```python
trace[1000:]['ron_p[T.True]'].mean()
```




    0.17594129574923567




```python
trace[1000:]['harry_p[T.True]:ron_p[T.True]'].mean()
```




    0.5152067955429208



# Text Rank Algorithm


```python
from sentence_transformers import SentenceTransformer
```


```python
bmodel = SentenceTransformer('all-MiniLM-L6-v2')
```


```python
bert_embeddings = bmodel.encode(hp_sentences[:2000])
bert_embeddings.shape
```




    (2000, 384)




```python
for i, (sentence, embedding) in enumerate(zip(hp_sentences[:2000], bert_embeddings)):
    print("Sentence:", sentence)
    print("Embedding:", embedding)
    print("")
    if i == 3: break
```

    Sentence: 
    HARRY POTTER AND THE CHAMBER OF SECRETS
    by J
    Embedding: [-4.93234880e-02  5.36252670e-02 -2.07240693e-02  6.74753962e-03
     -5.44992760e-02  4.73527703e-03  1.01671424e-02 -6.65984973e-02
      1.36869540e-02 -4.78445068e-02 -6.11548498e-02  4.70420942e-02
     -2.91775055e-02 -4.63095214e-03 -3.29879299e-02 -2.05020346e-02
      1.05388174e-02  9.38901678e-02  3.06045972e-02 -6.88970909e-02
      1.85281336e-02 -1.70141144e-03  5.44482246e-02 -4.20237668e-02
      8.34871270e-03  5.95409377e-03  4.61779088e-02  2.93322206e-02
     -7.10249841e-02 -9.95241255e-02 -3.47956754e-02 -8.03491194e-03
     -5.60158603e-02 -5.18047214e-02 -6.29849061e-02 -5.24122342e-02
     -6.49099797e-03  3.61507423e-02  3.41611542e-02 -6.52395794e-03
     -3.52358744e-02  3.72366235e-02 -1.68127827e-02  5.30787669e-02
     -3.71860020e-04  2.21250448e-02 -2.48641409e-02 -3.10840029e-02
     -2.63341572e-02  3.41985188e-02 -7.61241391e-02  1.89457145e-02
     -1.09015003e-01  4.63399850e-02  1.71690751e-02  3.19967978e-02
     -6.49880851e-04  1.54167376e-02  3.67241912e-02 -1.23269148e-02
     -9.89033934e-03 -1.20276950e-01 -3.03743631e-02  3.75952609e-02
      5.10283150e-02 -6.06199279e-02  7.81545695e-03  5.51841445e-02
     -2.22186018e-02 -5.69206811e-02 -4.65874970e-02  2.77248230e-02
      4.15199213e-02 -7.37819225e-02  4.36723203e-04  4.40043892e-04
     -1.33341119e-01 -9.95035693e-02 -2.51596048e-02 -4.38331328e-02
      2.20504254e-02 -5.43844923e-02 -3.50428000e-02  1.31497867e-02
     -5.12057059e-02 -1.16519155e-02  3.81452180e-02 -4.02484043e-03
     -7.17204064e-02  1.63712557e-02  3.20433564e-02 -4.58824039e-02
     -5.99792413e-03  9.23863053e-02 -2.73275431e-02 -4.89077298e-03
     -9.42248292e-03 -3.43354195e-02 -1.74623411e-02  5.08305989e-02
      3.52509841e-02  6.63698604e-03  1.64616257e-02  1.58271585e-02
      8.50422978e-02 -1.25881493e-01  3.11418269e-02 -3.51057686e-02
      8.69918801e-03 -7.96437822e-03 -2.18189023e-02 -3.81296165e-02
     -4.78984043e-02 -2.83500720e-02  5.64764254e-02 -1.73167686e-03
      1.41037866e-01  2.11751293e-02 -9.08885500e-04  9.75014344e-02
      3.58840562e-02  4.05806787e-02 -2.98830178e-02 -7.53547065e-03
     -8.08404684e-02 -2.99857017e-02  2.98936255e-02 -1.57818125e-33
      1.55214975e-02 -3.95471267e-02 -3.99651714e-02  1.07594490e-01
      8.50578025e-02 -4.62388992e-02  1.53438039e-02 -2.26500928e-02
     -7.73470178e-02  3.11608724e-02 -2.01078281e-02 -1.85923036e-02
     -1.09515727e-01 -4.63720597e-02 -2.10498795e-02 -5.63626215e-02
     -7.54874572e-02  5.58569580e-02  2.54658107e-02  3.81783210e-03
     -6.45173294e-03  6.46966994e-02  7.14632869e-03  4.42878722e-04
      4.05787751e-02 -1.21467002e-02 -1.56719089e-02 -3.13462317e-03
     -2.12998185e-02  1.54002197e-02 -1.25580663e-02  4.30641090e-03
      1.29165258e-02 -5.64363077e-02 -4.80468683e-02 -2.26922631e-02
     -5.23877479e-02 -6.65535256e-02  5.98762594e-02  1.39613505e-02
     -1.01381704e-01  2.24420335e-02  4.07774597e-02 -6.41042367e-02
     -2.22006422e-02  8.87177512e-02 -6.92458004e-02  6.19330555e-02
      2.50796378e-02  7.84368888e-02 -2.49871165e-02 -3.53593379e-03
     -2.80551780e-02 -4.75260466e-02 -4.81477752e-03 -2.16242839e-02
      1.45979030e-02  2.30083521e-02  4.44217883e-02 -3.36746201e-02
      2.31209081e-02  1.15080900e-01  2.34600734e-02  1.28535964e-02
      6.35169968e-02  6.85715750e-02 -8.64742547e-02 -5.81030026e-02
     -3.34399082e-02 -5.20533125e-04 -1.43676728e-01  4.95382547e-02
      4.05378826e-02 -6.54346496e-02 -4.43145186e-02 -2.24251542e-02
      2.47267932e-02 -3.40071060e-02 -9.36581790e-02 -1.15224764e-01
     -1.15133440e-02  4.96975370e-02 -3.62081453e-02  8.44002292e-02
     -3.12098023e-02 -3.95783484e-02  1.93467569e-02 -9.63644981e-02
     -4.41314429e-02  2.93164840e-03  5.71715757e-02 -1.87890895e-04
      4.49993787e-03 -8.86776000e-02 -1.58858914e-02 -3.85708808e-34
      6.74879476e-02 -8.24410394e-02  1.94007084e-02  2.28136275e-02
      8.79663751e-02  6.16316311e-02 -9.49839950e-02  2.88482551e-02
      6.92783967e-02  2.41300259e-02 -1.06043719e-01  1.25880716e-02
      9.36693698e-02  1.88194476e-02  5.05376011e-02 -1.91643033e-02
      8.17321390e-02 -4.70755771e-02  3.18730809e-02  3.56986448e-02
     -3.07326913e-02  7.11434036e-02 -2.44918559e-02 -4.48880680e-02
      7.29494169e-02  4.05618222e-03  1.32827275e-02 -3.54510732e-02
     -1.79623496e-02  4.00699340e-02  1.64162554e-02 -6.23505516e-03
     -8.76533426e-03  3.66266561e-03 -5.85495792e-02 -6.26200885e-02
      8.79570171e-02 -4.70305718e-02 -2.28692591e-02 -8.98600519e-02
      4.28089350e-02  3.02138906e-02 -1.82633772e-02  4.35062908e-02
      7.07501359e-03 -1.91640388e-02  5.11599705e-02  8.94263089e-02
     -4.72292490e-02  5.17977215e-02 -4.88224067e-02  8.56585987e-03
     -1.85274761e-02 -5.43359481e-02  1.62081216e-02  3.41265127e-02
     -7.65823433e-03 -1.72300357e-03  5.36499806e-02 -2.86815013e-03
     -9.22887847e-02  3.13546248e-02 -6.71564564e-02  1.85391940e-02
     -4.49219309e-02  6.98712654e-03 -4.78803404e-02  2.55067311e-02
      8.48030671e-03 -1.64567549e-02 -3.64028625e-02  2.00054888e-02
     -3.26530039e-02 -1.60015048e-03  4.65647951e-02  6.17123842e-02
      2.40677502e-02  6.18222356e-02 -1.92056578e-02  2.88010444e-02
      7.34134465e-02  8.40843841e-02 -1.32204182e-02  6.40358031e-02
      1.50664613e-01  6.31889626e-02  4.69917692e-02  3.92991751e-02
     -1.77842956e-02 -1.88980512e-02  7.20015988e-02  5.04761785e-02
      4.04034294e-02 -3.38877365e-02  8.51185247e-02 -1.67324110e-08
     -1.60770342e-02  1.10174026e-02 -4.19639498e-02 -7.02605918e-02
      6.46108240e-02 -1.19336173e-02  5.03774770e-02  5.55868708e-02
     -9.93818510e-03  8.98621455e-02  9.71643254e-02  5.34228794e-02
      4.77715507e-02  1.36927478e-02 -4.03015167e-02  1.48526328e-02
      8.09422955e-02 -7.23654479e-02 -2.86115948e-02  1.07356012e-02
      4.44034003e-02 -7.00996211e-03  7.40346536e-02 -2.17397809e-02
      7.76678207e-04  5.99168837e-02 -5.76385719e-05 -1.25393376e-01
      3.19101363e-02  9.34431776e-02  5.33271544e-02  4.98540141e-03
      5.42224385e-02 -2.20868620e-03 -7.07651004e-02  3.62620726e-02
      4.42130640e-02  3.13542783e-02  3.89246433e-03 -7.17964247e-02
      2.19859425e-02 -1.29230186e-01  1.63670350e-02  1.65228918e-02
     -5.81572317e-02 -8.69943586e-04  3.80404480e-02 -4.25620489e-02
     -1.10229244e-02 -1.12159904e-02 -3.41060385e-02  1.43225631e-02
      7.46656209e-02  6.39244821e-03  8.48672390e-02  9.35311243e-02
      1.72277272e-03  1.19306423e-01 -6.84257150e-02 -5.02119437e-02
      7.88654163e-02  1.13354169e-01 -4.58091535e-02  8.33208393e-03]
    
    Sentence: K
    Embedding: [-1.56091750e-02  3.61908376e-02 -4.78841923e-02  4.95182797e-02
      1.69075187e-02 -8.09057057e-02  6.39114156e-02  1.10915983e-02
     -1.29812844e-02 -6.09488646e-03 -3.02084368e-02 -1.31960347e-01
      2.49627270e-02  4.51414697e-02 -3.67907360e-02  5.91544099e-02
     -3.74916568e-02 -5.29451575e-03 -9.69610959e-02  4.21630256e-02
     -5.03912717e-02  2.11574184e-03  3.27853813e-05 -3.79552227e-03
     -2.35066041e-02  1.94466133e-02  7.47319916e-03  1.36343941e-01
      1.44451149e-02 -3.77848484e-02 -6.44354299e-02  9.38117430e-02
      8.11828449e-02  2.97311563e-02  5.80788963e-03 -5.29860146e-02
     -1.00719988e-01 -8.17183685e-03 -2.41112541e-02  1.72292050e-02
      3.40546779e-02 -1.87779218e-02 -1.09408908e-02  3.48589942e-02
      4.37886454e-02  5.63207380e-02 -1.36315450e-02 -6.26622811e-02
     -3.62134948e-02 -4.77671959e-02 -5.66992816e-03 -6.04644045e-03
     -5.49902394e-02  2.14336207e-03 -3.45184207e-02 -3.86800244e-02
     -9.97352228e-02  1.55551890e-02  4.70907278e-02  1.83456962e-03
     -2.06779931e-02 -2.21077874e-02 -1.33315757e-01  7.38615766e-02
      1.14626521e-02  7.60633796e-02  2.60435836e-03  2.96508111e-02
     -9.58476402e-03  4.86109871e-03  3.52474861e-02 -1.94019936e-02
      4.21645790e-02 -7.01911673e-02  1.39131956e-02 -3.41061354e-02
      2.81398017e-02 -1.02963326e-02  5.37508428e-02  2.48326752e-02
     -2.58332528e-02  1.30356615e-03 -9.73047465e-02  8.37633014e-03
     -4.84255739e-02  3.63186409e-04  6.21768273e-03  8.34343433e-02
     -7.66934305e-02 -9.91239212e-03  2.11177263e-02 -3.16130333e-02
      8.15186724e-02 -1.19238654e-02 -3.96694951e-02  1.40595362e-02
     -2.53939722e-02  5.71713150e-02 -1.18997373e-01  2.65184075e-01
     -1.24233076e-02  6.83198646e-02 -2.14719195e-02  2.41503194e-02
     -1.13710482e-02 -3.55171449e-02 -5.39142005e-02  4.98662405e-02
      2.41390001e-02 -6.71928283e-03 -2.77908687e-02 -5.23335673e-03
      1.08557148e-02  1.96487568e-02  9.88345221e-03  8.07530507e-02
      9.12212208e-03 -9.38923564e-03 -5.81509434e-03 -2.50742561e-03
      5.93303256e-02 -1.02526592e-02 -6.80848397e-03  4.23447713e-02
     -2.65168361e-02 -7.65914172e-02  6.19789511e-02 -2.18762603e-33
     -1.02270197e-03 -3.65430415e-02  5.02893701e-02 -2.70863380e-02
      4.81026843e-02  8.01368244e-03 -1.73530099e-03 -4.90278751e-02
     -7.30334818e-02 -2.31746957e-02  2.50991508e-02  8.83777905e-03
      4.01602089e-02 -1.88434254e-02  1.00274108e-01  3.87753546e-02
     -3.08329500e-02  2.40216684e-02  2.26371586e-02  3.71706150e-02
      6.55261194e-03 -2.95823626e-02 -1.76265147e-02  5.88528849e-02
     -8.57659951e-02 -4.03637476e-02 -3.35316509e-02 -7.27209151e-02
     -1.01703173e-02  3.01761143e-02  7.86286779e-03  2.78772675e-02
     -2.61716507e-02  6.75259829e-02 -3.34718749e-02 -8.57370347e-02
      4.98772711e-02 -8.75605047e-02 -3.28576714e-02  5.05371578e-02
     -6.69155270e-02  2.17766687e-02 -7.14557394e-02  5.77744842e-02
      1.32543948e-02  1.33866131e-01  5.03894351e-02  5.14721125e-03
     -6.27830299e-03  4.73024957e-02 -4.86480668e-02 -3.88407856e-02
     -9.59787071e-02  1.67234819e-02 -1.86060704e-02  7.81783909e-02
      6.91220686e-02 -3.20253521e-02  1.93112884e-02 -3.80508346e-03
      5.24359532e-02  7.38535672e-02 -4.05541137e-02  2.01209579e-02
     -1.44917276e-02 -9.35573205e-02 -3.39376032e-02 -1.02861524e-02
     -2.08288301e-02 -4.75849397e-02 -9.22939554e-02  1.68838166e-02
      1.04607917e-01 -1.89243797e-02  2.87650675e-02 -3.27650830e-02
     -5.66448346e-02 -2.59015821e-02 -2.88602002e-02  4.68170121e-02
     -5.04697533e-03 -3.41736712e-03 -3.47712338e-02 -4.29908410e-02
      3.88831347e-02  3.66408154e-02 -8.51693749e-03 -4.91642207e-02
      1.02719795e-02 -3.89718860e-02 -1.63764179e-01  3.74162979e-02
      1.69683006e-02  2.54842713e-02 -2.65063960e-02  2.36829441e-34
     -3.38629931e-02  5.30294478e-02 -4.00025100e-02  1.15611471e-01
     -3.96002904e-02  7.36088231e-02  5.59466742e-02  6.69199899e-02
     -1.25126475e-02  8.26816931e-02  4.78168437e-03 -9.94524136e-02
      1.12485792e-02  3.25160380e-03  3.74462605e-02  8.55816230e-02
     -6.30244054e-03  3.99726778e-02 -3.49692330e-02 -1.54282292e-02
     -3.50841694e-02 -5.72028682e-02 -7.14999959e-02  4.25401516e-02
     -1.45278089e-02  7.96179939e-03 -1.22921672e-02  8.54468048e-02
     -2.69144811e-02  6.60325736e-02  3.84757482e-02 -4.83562723e-02
     -9.54676494e-02 -6.95034340e-02  1.64850000e-02  1.91834439e-02
     -3.00786644e-02 -1.03406729e-02  2.69311797e-02 -1.46595063e-02
      2.96369512e-02 -1.37008652e-02  7.88371637e-02  1.11488618e-01
     -8.05575475e-02 -3.84991355e-02  1.52965216e-02  6.10894002e-02
      1.11355260e-01 -4.05297466e-02 -6.57615215e-02 -2.32407767e-02
     -8.36196616e-02 -5.33608943e-02 -6.88391253e-02  8.12538639e-02
      3.48141380e-02  2.61514783e-02  1.38601903e-02 -5.67123806e-03
     -4.30257581e-02 -5.78095578e-02 -2.53352504e-02  3.42320018e-02
     -5.09183928e-02 -2.51118857e-02  5.93807688e-03  1.47895049e-02
     -2.01291032e-02 -6.54527247e-02  6.66607991e-02 -4.99036983e-02
     -5.05676866e-02 -1.92201387e-05 -1.19914554e-01 -1.82366464e-02
     -5.42915016e-02  1.85775459e-02 -3.43996547e-02  1.00319823e-02
     -3.64011824e-02 -1.99423712e-02 -2.58723013e-02  3.32956091e-02
     -1.00698450e-03  2.99505088e-02  3.75319049e-02  5.59168197e-02
     -2.74317954e-02  7.53664086e-03  3.64193507e-03  1.91343967e-02
      6.25231788e-02 -2.34803930e-02  3.27329002e-02 -1.79397013e-08
      3.36263366e-02  1.70875359e-02 -4.18575481e-02 -9.86307580e-03
      1.34263873e-01  4.43110317e-02 -2.31356639e-02  9.81045794e-03
      6.04259335e-02 -1.27671491e-02  1.06000766e-01  5.86770587e-02
      3.85393463e-02  1.70838237e-02  9.95362643e-03 -7.91498367e-03
     -6.21258058e-02  4.10333788e-03 -2.14551948e-02  1.15188828e-03
      2.55873683e-03  2.93226521e-02  3.94777432e-02  2.86292806e-02
     -1.29435482e-02  4.08087410e-02  1.77873969e-02  9.72637162e-02
      1.23582985e-02 -5.05567752e-02 -2.68485467e-03 -1.59852933e-02
      4.72631790e-02 -1.83510985e-02  5.23888413e-03  2.41932813e-02
     -7.02265203e-02  4.71009947e-02  1.85900424e-02  7.51550868e-02
     -1.76284146e-02 -5.33756353e-02  2.02845223e-02  1.45869646e-02
     -1.33757383e-01  4.17622812e-02  6.17918968e-02 -2.14146096e-02
     -5.77333197e-02 -7.79499263e-02 -6.88282326e-02  1.97991263e-03
     -2.62536872e-02  6.45608641e-03  9.86917987e-02  2.73969676e-02
      3.03020875e-04 -3.18720900e-02 -1.01556377e-02  2.53404435e-02
      1.70090765e-01  3.51550244e-03 -8.41178093e-03  4.30587716e-02]
    
    Sentence: Rowling
    
    (this is BOOK 2 in the Harry Potter series)
    
    Original Scanned/OCR: Friday, April 07, 2000
    v1.0
    (edit where needed, change version number by 0.1)
    
    
    C H A P T E RR		O N E
    
    THE WORST BIRTHDAY
    
    Not for the first time, an argument had broken out over breakfast at
    number four, Privet Drive
    Embedding: [-5.60141355e-02  1.07897006e-01  1.46731017e-02  5.50195500e-02
     -5.06133996e-02 -1.68568790e-02  1.44331707e-02  6.27001747e-03
     -4.63324040e-02 -7.11256191e-02 -4.11342755e-02  5.73745817e-02
      9.64338146e-03 -2.79909726e-02 -1.25225157e-01 -7.50114322e-02
     -4.06494215e-02  6.51021861e-03  2.83026248e-02 -7.30277877e-03
     -5.33427224e-02 -2.10568518e-03  4.41821665e-02 -2.13642027e-02
      6.25949278e-02 -2.97627244e-02  2.02289093e-02  2.24787202e-02
     -4.83307242e-02 -3.54460105e-02  1.36598013e-02  7.07777217e-02
     -1.04967579e-02 -2.97533795e-02 -2.50206478e-02 -6.15592711e-02
      6.94405138e-02 -6.55575236e-03  8.78963545e-02 -1.21612055e-02
      4.47515473e-02 -7.01829642e-02 -3.95378768e-02  5.79757802e-02
     -6.29766518e-03  3.09989154e-02  3.75238969e-03 -2.40144506e-03
     -5.71010262e-02  1.76224262e-02 -6.59627318e-02  9.02682915e-03
     -4.08530422e-02 -7.26692751e-02  2.23710928e-02 -2.66255112e-03
      4.35184594e-03 -5.68261836e-03  5.71871176e-02  6.21971712e-02
     -7.93959275e-02 -2.58369297e-02  1.14511587e-02  2.05704626e-02
      8.35438073e-02 -4.22375426e-02  6.07263558e-02 -4.90500405e-02
     -1.10153956e-02  2.34074183e-02 -7.57373199e-02  7.66524300e-02
      1.19580083e-01  6.05282839e-03  1.67046692e-02  5.41034434e-03
     -9.37554613e-02 -2.99570188e-02  5.57886809e-02 -1.04389928e-01
     -6.56686798e-02 -5.04226163e-02  1.52545143e-03  5.98350093e-02
     -4.59541529e-02 -2.34475173e-02  4.50572595e-02  2.82261483e-02
     -4.64916142e-04 -2.24479828e-02  3.54037015e-03  4.95813116e-02
      3.70089523e-02  1.05762407e-01 -2.67455596e-02  3.12984101e-02
      1.80498026e-02 -3.66540346e-03 -4.54770103e-02  5.48948273e-02
     -4.48920876e-02  6.91483989e-02  7.56484121e-02  8.96125883e-02
      1.00255765e-01 -9.40406844e-02  1.83059517e-02 -1.40387446e-01
     -4.58080731e-02 -7.85100907e-02 -3.68069187e-02 -3.34152356e-02
      1.79887842e-02 -7.48067871e-02  2.67716758e-02 -7.20929205e-02
      6.93514869e-02  6.89683035e-02 -3.40233673e-03  2.04659775e-02
     -4.12034728e-02  5.17266095e-02 -5.02538756e-02 -2.77129952e-02
     -1.27369300e-01 -2.07823701e-02  4.61165197e-02  5.74456779e-33
     -1.79392155e-02  1.37250023e-02  6.38040574e-03 -2.07462497e-02
      8.09496790e-02 -1.81216970e-02 -6.08600378e-02 -4.88709398e-02
      8.19010735e-02 -3.43183167e-02 -1.29059348e-02 -1.28976777e-01
     -2.24931762e-02 -7.57955760e-02  1.16926664e-02  7.11267963e-02
     -1.09032854e-01  7.98168182e-02 -6.44821115e-03  6.58689998e-03
      1.64591521e-03  4.65362743e-02  2.38142647e-02 -4.94547635e-02
      1.65905303e-03  2.96977572e-02  3.81380431e-02 -4.20616604e-02
      3.80378142e-02 -1.08320424e-02  5.08214422e-02 -4.10974696e-02
      8.55935086e-03 -5.64426184e-02 -2.60195900e-02 -8.50091726e-02
      3.02272048e-02 -3.42922658e-02  1.46525782e-02  4.85901497e-02
     -8.15698951e-02 -1.82769429e-02  5.09454124e-02 -3.61533947e-02
     -5.18268496e-02  4.40473258e-02  1.62572172e-02  8.49284381e-02
      9.73910168e-02  3.09720566e-03 -5.09616919e-02 -4.64926884e-02
     -2.77562197e-02 -1.19587295e-02 -7.88334087e-02 -1.07281068e-02
      4.19649109e-02 -4.80005331e-02  9.56669077e-02 -2.08544880e-02
      9.71599966e-02  6.96986243e-02  3.34520116e-02 -2.71326415e-02
     -3.53698991e-02 -2.43761502e-02 -1.27033126e-02 -2.66844835e-02
      4.07197559e-03  1.82221625e-02  5.65409707e-03 -3.01145241e-02
     -2.65898090e-02 -1.48986474e-01  2.08146311e-03  8.34052358e-03
      7.79497996e-02  4.67647286e-03 -5.92500418e-02 -7.93703273e-02
      4.70698215e-02  6.29799590e-02  1.73395853e-02 -4.37350273e-02
     -6.63289055e-02 -1.89737398e-02  1.15700876e-02 -1.58921424e-02
      4.76698624e-03  1.26515403e-01  2.42904667e-02 -2.34192051e-02
     -2.98425574e-02 -3.82690765e-02  5.80633432e-02 -5.54698152e-33
      5.10262139e-02 -2.52008215e-02  7.86166452e-03  5.98930418e-02
     -2.86784023e-02 -2.60076523e-02 -4.95472774e-02  6.68868199e-02
      6.64345920e-02  1.24293528e-02  2.54388917e-02  6.84613585e-02
      1.32971853e-01 -3.11017744e-02  4.22132760e-02 -6.57600677e-03
      4.35783640e-02  7.09905326e-02 -2.15650667e-02 -3.88013199e-03
     -2.26280373e-02  4.14791368e-02 -9.84843820e-02 -9.21670906e-03
      4.63934764e-02  5.96044026e-02  1.23161651e-01 -3.94717678e-02
     -4.12786938e-02 -5.23672402e-02  4.58364753e-04  2.73891748e-03
     -9.70681012e-03  3.63397561e-02  1.78960916e-02 -7.92442006e-04
      4.00597341e-02 -7.03105032e-02 -5.06461561e-02 -6.66061863e-02
     -5.27828792e-03  4.32607234e-02  2.55857175e-03  1.00430325e-01
      1.64167266e-02 -2.08701994e-02 -2.56814179e-03  7.03200176e-02
      1.30702769e-02  2.81691942e-02 -1.57801993e-02  1.77920163e-02
     -1.10933706e-02 -1.34289842e-02  1.10872854e-02  9.18050855e-03
     -1.39781311e-02 -2.45161485e-02  5.71142286e-02  3.14374305e-02
     -5.20348772e-02  7.83722661e-03 -4.40542363e-02 -1.24058602e-02
     -6.02755621e-02 -8.36207047e-02 -5.64266630e-02  1.53264450e-02
      3.49874459e-02  1.87121332e-02  3.39085832e-02  4.15580124e-02
     -6.89255521e-02 -8.55571926e-02 -3.13360654e-02  9.13667679e-02
      3.58247720e-02  9.46816728e-02 -6.34004846e-02 -4.11167415e-03
      1.63365807e-02  1.18914001e-01 -4.16751690e-02  3.62294540e-02
     -3.86217870e-02 -5.66351451e-02  3.24211381e-02  3.95302987e-03
     -8.44042341e-04 -3.34096402e-02  6.78104535e-02 -1.83923934e-02
     -3.36355679e-02  4.92630415e-02  3.19669768e-02 -4.76750408e-08
     -9.57542006e-03  1.99615452e-02  1.52849359e-03  9.95098148e-03
      4.99972291e-02 -6.62024468e-02 -1.10674938e-02  5.92757203e-02
     -2.22986075e-03  5.75803146e-02  8.01116154e-02  1.54871754e-02
      2.49679107e-02 -5.43266982e-02 -2.36927811e-02 -2.12804021e-04
      1.32903792e-02  1.15956515e-02 -4.45142202e-02  1.45670623e-02
      3.40283252e-02 -2.52920799e-02  7.76378512e-02 -4.62482646e-02
     -6.75384095e-03  1.52055006e-02  1.50253316e-02 -5.28606884e-02
     -2.97538377e-02 -9.20997784e-02  6.61871061e-02  2.40836851e-02
      4.83874008e-02 -9.96789858e-02 -1.60668995e-02  7.10986108e-02
     -3.67830284e-02  7.80394003e-02  1.14589510e-02  7.08078314e-03
      1.81323104e-02 -1.32161900e-01  2.89816242e-02 -6.44213660e-03
     -1.07029960e-01  1.95871517e-02 -4.58830483e-02 -7.45298043e-02
     -4.82955994e-03 -5.05279675e-02 -3.07260621e-02  1.02412738e-02
      7.24617317e-02  8.07211548e-03  8.30668578e-05  1.08043235e-02
     -2.63915993e-02  7.37766996e-02 -4.40866947e-02  5.44315577e-02
      9.76732671e-02  8.45899433e-02 -1.06784049e-02  6.98377341e-02]
    
    Sentence: Mr
    Embedding: [-9.61959139e-02  5.64198494e-02  1.52234016e-02  2.44766995e-02
     -1.15675792e-01 -9.55891684e-02  7.74657205e-02 -7.29863346e-03
     -1.22630270e-02 -9.87773389e-03 -8.93971324e-02  5.22841932e-03
     -1.60977822e-02  2.01251302e-02 -8.40159506e-03  3.46920453e-02
     -3.14625427e-02  3.44147980e-02  4.75243963e-02  1.22001106e-02
     -5.52966855e-02  1.27836037e-02 -6.40452504e-02 -6.44321516e-02
      2.03145621e-03 -1.82305593e-02 -3.91571186e-02  5.19800819e-02
      1.27929561e-02 -2.92258486e-02 -5.48549276e-03  5.94939031e-02
      9.47484523e-02  2.31643356e-02 -7.86335487e-03 -3.14271376e-02
     -5.48291504e-02  4.67405878e-02  2.61337347e-02  3.88487242e-02
     -1.73393954e-02 -7.67395645e-02  3.62587310e-02 -9.95365717e-03
      4.26777005e-02  3.44600901e-02  1.84476729e-02 -9.37957019e-02
      2.53318977e-02  7.13531999e-03 -4.78629842e-02 -4.60438617e-02
     -5.16501591e-02 -4.30231215e-03 -8.11241101e-03 -6.30400926e-02
     -1.66496856e-03 -2.19875388e-03 -3.11756227e-02 -4.94114310e-02
     -7.89502710e-02 -1.02648651e-03 -2.61883195e-02  2.91155633e-02
      1.16601273e-01  1.84937194e-02 -4.23025452e-02 -2.34116744e-02
      5.74732386e-02  7.58576468e-02 -6.18018284e-02 -2.89931018e-02
     -1.93098001e-02 -8.52062330e-02 -4.78698500e-02  6.16079532e-02
      3.05048861e-02 -8.59830305e-02  4.15392444e-02  2.42630914e-02
     -3.80560383e-02  3.13483588e-02 -7.19265044e-02 -3.56100909e-02
     -1.70290656e-02  3.69237550e-02  3.55604440e-02 -4.17910330e-02
     -1.96944717e-02  4.84342985e-02 -5.41632473e-02 -3.19963656e-02
      8.96258280e-02  5.51108569e-02 -5.61132692e-02 -3.04081589e-02
     -6.95411414e-02  3.22667472e-02 -1.45059139e-01  2.17062622e-01
      4.52424660e-02  2.40176581e-02 -5.83525039e-02  3.04274429e-02
      6.98221400e-02 -4.02143151e-02 -2.47665234e-02  7.65773207e-02
      4.26138267e-02 -3.06484159e-02 -3.27011421e-02  1.97558086e-02
      9.10113472e-03  4.40235473e-02 -2.01174729e-02 -8.97845924e-02
     -2.54728254e-02 -8.24787561e-03 -1.47910276e-02 -5.75953983e-02
      7.01842131e-03  3.48899700e-02 -2.75157653e-02 -2.03246679e-02
      1.28808420e-03 -1.12697273e-01  4.90800925e-02 -3.85823327e-33
     -2.55704597e-02 -1.16489073e-02  4.85250540e-02 -9.12973005e-03
     -1.14198877e-02  6.57993555e-02 -8.61696713e-03 -2.76875291e-02
      5.15522286e-02  7.03152865e-02  5.22902496e-02  1.40766334e-02
     -2.62085609e-02 -2.83257198e-02 -4.97774743e-02  2.14675944e-02
      4.68046591e-02  1.21762129e-02 -5.24273608e-03 -5.12591423e-03
     -3.69432084e-02  4.99851666e-02  2.38634795e-02  1.56571250e-02
     -2.84325369e-02 -1.02995904e-02  7.18666762e-02  2.37006657e-02
      7.70989209e-02  4.00776789e-02  3.69517761e-03  8.03783815e-03
     -5.72463721e-02  1.86391007e-02 -4.47894521e-02 -7.32564479e-02
     -2.53537931e-02 -1.01606570e-01 -1.80033538e-02  3.74611504e-02
     -2.91749593e-02 -1.73098668e-02  2.26808386e-03  9.90763009e-02
     -4.90190042e-03  1.45883495e-02  7.87675604e-02  8.43834877e-02
     -6.20998442e-03  1.04893502e-02 -4.16264459e-02 -5.23352101e-02
     -7.42916763e-02  1.83876008e-02  1.87288076e-02 -3.60781178e-02
      6.99441954e-02  3.69635154e-03 -3.31807812e-03  4.20013145e-02
     -2.45244871e-03  2.05206126e-02 -1.42662330e-02  3.00199874e-02
     -6.64436072e-03 -1.08883202e-01  3.70337814e-02 -5.62308691e-02
      6.68163076e-02  2.64324639e-02 -3.38555202e-02 -1.18911294e-02
      9.38136056e-02 -2.33560055e-02  1.48597464e-03  3.72421332e-02
     -1.59986224e-02  6.68100417e-02 -5.31024747e-02  1.30629297e-02
     -3.22799035e-03  3.77489477e-02 -6.90573687e-03 -1.09929517e-02
      4.62723561e-02  2.85503697e-02  4.84821573e-02 -5.64677231e-02
      3.65802646e-02  1.08803790e-02 -7.39463419e-02 -1.42817688e-03
      4.26334813e-02  1.64150059e-01 -4.56156768e-02  2.23012841e-33
     -2.87146568e-02  2.13834736e-02  4.35849018e-02  4.44466807e-02
      9.41287726e-03 -3.27051827e-03  4.20160145e-02  2.85749901e-02
     -5.32313548e-02  3.92637625e-02 -8.63592699e-02 -3.34684364e-02
      5.15944660e-02  3.27836955e-04  1.75700467e-02  4.55667302e-02
      5.47723658e-02  1.01273777e-02 -1.98089108e-02  5.95811605e-02
     -8.19405392e-02  2.60192193e-02 -4.11746055e-02 -1.53568769e-02
     -4.89279218e-02  4.75144573e-02  5.70647977e-02 -5.01847304e-02
      1.11386157e-03 -1.62076950e-02  1.21893600e-01  1.72579829e-02
     -1.71176523e-01 -3.55337411e-02 -6.37550503e-02  1.54316276e-01
      5.85765243e-02  2.48803794e-02  4.47221138e-02  6.31782934e-02
      2.41794046e-02  8.00492540e-02 -3.19925696e-02  6.24973662e-02
     -1.19385961e-02 -3.76906283e-02 -5.92251420e-02 -7.39314174e-03
      2.37512160e-02  5.14716618e-02 -6.82086125e-02 -3.62516567e-02
     -1.64550189e-02 -1.04380496e-01 -4.91165407e-02  7.15348125e-03
     -3.81057127e-03  2.53531076e-02  6.79808035e-02  2.01672222e-02
     -7.35664964e-02  6.13481589e-02 -5.96813485e-03  7.61313960e-02
     -2.30083559e-02  1.04126699e-01  1.30983805e-02 -1.00170776e-01
      1.38532268e-02  2.03980263e-02  6.15105480e-02 -4.95687872e-02
     -4.61706780e-02 -1.70278456e-02 -1.54786007e-02 -5.16310818e-02
     -2.86078062e-02  4.73120771e-02  4.24120128e-02 -1.42510563e-01
     -3.81969698e-02 -5.19006737e-02  3.84550239e-03  7.39187822e-02
     -8.82766992e-02 -4.91732210e-02 -7.20757991e-03 -1.83974393e-02
     -5.04597463e-02  3.61891128e-02  6.63113687e-03  3.23360190e-02
      5.69154136e-02 -7.86797330e-02  7.17004389e-03 -1.39020901e-08
      4.39241268e-02 -4.40335497e-02  1.06175039e-02 -2.99219657e-02
      7.03510493e-02 -3.91369034e-03 -1.04645677e-01 -2.81982357e-03
      4.36992794e-02  5.43938540e-02  4.18041088e-02 -3.48760411e-02
      6.41163588e-02 -1.27387522e-02  9.26575959e-02 -3.89043391e-02
     -5.27261384e-02 -2.38650627e-02 -1.27507355e-02  2.30763163e-02
      5.16464701e-03 -6.83844322e-03 -8.38367920e-03  1.98947340e-02
      3.82166430e-02  4.95601967e-02 -2.32728012e-02  4.35663946e-02
     -2.45219097e-02  7.23707378e-02  3.88343334e-02  1.17428020e-01
     -2.96240840e-02 -3.55042182e-02 -5.96804880e-02 -1.34194898e-03
      5.12835160e-02 -4.86154296e-03  4.21844050e-02  4.58639413e-02
     -8.33412036e-02  1.42542087e-03  5.30514084e-02  2.26292517e-02
     -7.52130523e-02  1.21711148e-02 -7.00788572e-02 -2.56865639e-02
     -5.20029180e-02 -7.07962736e-02  3.42657045e-02 -2.91307792e-02
      5.79654500e-02  3.72174643e-02  4.16904055e-02 -3.24765593e-02
      3.75623330e-02  1.21377716e-02 -2.74417270e-02 -2.56373622e-02
      1.38344198e-01  9.86385159e-03 -9.50655993e-03 -5.99199953e-03]
    



```python
import numpy as np
a = bert_embeddings[0]
b = bert_embeddings[1]
cos_sim = (a @ b.T) / (np.linalg.norm(a)*np.linalg.norm(b))
cos_sim
```




    0.121609494




```python
bert_embeddings.shape
```




    (2000, 384)




```python
from tqdm import tqdm

M = np.eye(2000)

for i,a in tqdm(enumerate(bert_embeddings)):
    for j,b in enumerate(bert_embeddings):
        if i != j:
            M[i,j] = (a @ b.T) / (np.linalg.norm(a)*np.linalg.norm(b))
```

    2000it [00:53, 37.24it/s]



```python
import networkx

similarity_graph = networkx.from_numpy_array(M)
similarity_graph
```




    <networkx.classes.graph.Graph at 0x7faa32de7a58>




```python
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(14, 8))
networkx.draw_networkx(similarity_graph, node_color='lime')
```


    
![png](output_84_0.png)
    



```python
scores = networkx.pagerank(similarity_graph)
nx_ranked_sentences = sorted(((score, index) for index, score 
                                            in scores.items()), 
                          reverse=True)
nx_ranked_sentences[:10]
```




    [(0.0007379157166608729, 943),
     (0.0007355073292168067, 406),
     (0.0007349476773065155, 894),
     (0.0007327824043521785, 174),
     (0.0007211285239596218, 1467),
     (0.0007176382543918169, 243),
     (0.0007154579388173535, 408),
     (0.0007103266441297198, 206),
     (0.0007064772055979831, 1108),
     (0.0007063833665388329, 1195)]




```python
for i in range(10):
    print(hp_sentences[nx_ranked_sentences[i][1]])
    print('----------')
```

    I will attract your
    attention when it is time to pack up.
    
    "Four to a tray - there is a large supply of pots here - compost in the
    sacks over there - and be careful of the Venemous Tentacula, it's
    teething."
    
    She gave a sharp slap to a spiky, dark red plant as she spoke, making
    it draw in the long feelers that had been inching sneakily over her
    shoulder.
    
    Harry, Ron, and Hermione were joined at their tray by a curly-haired
    Hufflepuff boy Harry knew by sight but had never spoken to.
    
    "Justin Finch-Fletchley," he said brightly, shaking Harry by the hand.
    "Know who you are, of course, the famous Harry Potter..
    ----------
    "Just so you could carry on
    tinkering with all that Muggle rubbish in your shed! And for your
    information, Harry arrived this morning in the car you weren't
    intending to fly!"
    
    "Harry?" said Mr
    ----------
    A few
    people laughed and, gradually, a babble of talk broke out again.
    
    Hermione closed Voyages with Vampires and looked down at the top
    of Ron's head.
    
    * 88
    
    "Well, I don't know what you expected, Ron, but you -"
    
    "Don't tell me I deserved it," snapped Ron.
    
    Harry pushed his porridge away
    ----------
    ."
    
    And before Harry could stop him, Dobby bounded off the bed,
    seized Harry's desk lamp, and started beating himself around the
    head with earsplitting yelps.
    
    A sudden silence fell downstairs
    ----------
    "Why go up to that corridor?"
    
    Ron and Hermione looked at Harry.
    
    "Because - because -" Harry said, his heart thumping very fast;
    something told him it would sound very far-fetched if he told them he
    had been led there by a bodiless voice no one but he could hear,
    "because we were tired and wanted to go to bed," he said.
    
    "Without any supper?" said Snape, a triumphant smile flickering across
    his gaunt face
    ----------
    And someone was goggling through the bars at him: a freckle-
    faced, red-haired, long-nosed someone.
    
    Ron Weasley was outside Harry's window.
    
    H-H A P T E RR T 11-H RR E E
    
    THE BURROW
    
    Ron.l" breathed Harry, creeping to the window and pushing it up so
    they could talk through the bars
    ----------
    "Harry who?"
    
    He looked around, saw Harry, and jumped.
    
    "Good lord, is it Harry Potter? Very pleased to meet you, Ron's told us
    so much about -"
    
    "Your sons flew that car to Harry's house and back last night."
    
    shouted Mrs
    ----------
    ("Just our nephew - very disturbed
    
    	meeting strangers upsets him, so we kept him upstairs 	) He
    
    shooed the shocked Masons back into the dining room, promised
    Harry he would flay him to within an inch of his life when the Ma
    sons had left, and handed him a mop
    ----------
    The Gryffindors were
    gathered around Ron, who kept belching large, glistening slugs.
    Nobody seemed to want to touch him.
    
    "We'd better get him to Hagrid's, it's nearest," said Harry to
    Hermione, who nodded bravely, and the pair of them pulled Ron up
    by the arms.
    
    "What happened, Harry? What happened? Is he ill? But you can
    cure him, can't you?" Colin had run down from his seat and was now
    dancing alongside them as they left the field
    ----------
    ."
    
    Harry gave a huge jump and a large lilac blot appeared on Veronica
    Smethley's street.
    
    "What?" he said loudly.
    
    "I know!" said Lockhart
    ----------


# LDA (Latent Dirichlet Allocation)


```python
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
```


```python
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

hp_words = list(sent_to_words(hp_sentences))
```


```python
# Build the bigram and trigram models
bigram = gensim.models.Phrases(hp_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[hp_words], threshold=100)  

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[hp_words[0]]])
```

    ['harry', 'potter', 'and', 'the', 'chamber', 'of', 'secrets', 'by']



```python
# Define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out
```


```python
# Remove Stop Words
hp_words_nostops = remove_stopwords(hp_words)

# Form Bigrams
hp_words_bigrams = make_bigrams(hp_words_nostops)

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python3 -m spacy download en
nlp = spacy.load('en', disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
hp_lemmatized = lemmatization(hp_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

print(hp_lemmatized[:2])
```

    [['chamber', 'secret'], []]



```python
# Create Dictionary
id2word = corpora.Dictionary(hp_lemmatized)

# Create Corpus
texts = hp_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

# View
print(corpus[:2])
```

    [[(0, 1), (1, 1)], []]



```python
id2word[0]
```




    'chamber'




```python
[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:2]]
```




    [[('chamber', 1), ('secret', 1)], []]



### Building the Topic Model


```python
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=20, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)
```

### View the topics in LDA model


```python
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
```


```python
from pprint import pprint

# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]
```

    [(0,
      '0.194*"snape" + 0.073*"master" + 0.052*"arrive" + 0.052*"sleep" + '
      '0.049*"far" + 0.041*"care" + 0.040*"drop" + 0.036*"magical" + '
      '0.032*"great_hall" + 0.030*"muggle"'),
     (1,
      '0.100*"time" + 0.056*"die" + 0.043*"course" + 0.043*"thing" + '
      '0.040*"mother" + 0.039*"believe" + 0.039*"remember" + 0.038*"dead" + '
      '0.037*"day" + 0.037*"reach"'),
     (2,
      '0.364*"look" + 0.124*"let" + 0.082*"happen" + 0.058*"place" + 0.051*"walk" '
      '+ 0.034*"move" + 0.032*"spell" + 0.025*"sure" + 0.024*"burn" + '
      '0.020*"notice"'),
     (3,
      '0.202*"eye" + 0.079*"sit" + 0.067*"end" + 0.066*"ear" + 0.062*"later" + '
      '0.047*"little" + 0.044*"second" + 0.041*"touch" + 0.040*"power" + '
      '0.036*"bed"'),
     (4,
      '0.162*"face" + 0.088*"right" + 0.084*"point" + 0.082*"stare" + 0.063*"foot" '
      '+ 0.049*"arm" + 0.047*"night" + 0.043*"black" + 0.027*"pass" + '
      '0.026*"call"'),
     (5,
      '0.379*"try" + 0.062*"explain" + 0.007*"chamber" + 0.000*"dumbledore" + '
      '0.000*"voldemort" + 0.000*"fudge" + 0.000*"see" + 0.000*"death_eater" + '
      '0.000*"away" + 0.000*"step"'),
     (6,
      '0.130*"shake" + 0.126*"talk" + 0.098*"fall" + 0.062*"bring" + 0.053*"set" + '
      '0.048*"friend" + 0.038*"rest" + 0.037*"fire" + 0.032*"family" + '
      '0.025*"effect"'),
     (7,
      '0.096*"large" + 0.074*"sound" + 0.067*"chair" + 0.052*"glass" + '
      '0.050*"school" + 0.048*"wear" + 0.046*"roll" + 0.040*"cry" + 0.034*"thin" + '
      '0.031*"love"'),
     (8,
      '0.206*"hear" + 0.153*"word" + 0.127*"watch" + 0.072*"leg" + 0.055*"carry" + '
      '0.039*"mutter" + 0.037*"morning" + 0.035*"fly" + 0.022*"week" + '
      '0.011*"fast"'),
     (9,
      '0.290*"come" + 0.237*"tell" + 0.155*"turn" + 0.053*"stop" + 0.042*"dursley" '
      '+ 0.038*"run" + 0.017*"order" + 0.014*"miss" + 0.012*"hope" + 0.012*"pay"'),
     (10,
      '0.236*"hand" + 0.113*"give" + 0.112*"ground" + 0.067*"potion" + '
      '0.051*"scream" + 0.036*"mouth" + 0.028*"play" + 0.027*"blood" + '
      '0.025*"have" + 0.025*"deep"'),
     (11,
      '0.135*"leave" + 0.104*"take" + 0.079*"voice" + 0.074*"open" + 0.070*"wait" '
      '+ 0.056*"hogwart" + 0.055*"door" + 0.054*"smile" + 0.041*"keep" + '
      '0.036*"hair"'),
     (12,
      '0.000*"pondering" + 0.000*"hypnotized" + 0.000*"lunacy" + '
      '0.000*"courteously" + 0.000*"maim" + 0.000*"playground" + 0.000*"protector" '
      '+ 0.000*"ability" + 0.000*"talentless" + 0.000*"mainland"'),
     (13,
      '0.116*"moment" + 0.094*"work" + 0.073*"suddenly" + 0.066*"car" + '
      '0.042*"use" + 0.037*"water" + 0.035*"finish" + 0.034*"allow" + '
      '0.034*"evening" + 0.030*"magic"'),
     (14,
      '0.229*"go" + 0.077*"way" + 0.073*"year" + 0.067*"old" + 0.063*"speak" + '
      '0.062*"buy" + 0.054*"dark" + 0.053*"great" + 0.053*"room" + 0.038*"wizard"'),
     (15,
      '0.240*"hermione" + 0.171*"feel" + 0.050*"table" + 0.035*"mean" + '
      '0.035*"quickly" + 0.034*"silence" + 0.033*"tear" + 0.032*"fight" + '
      '0.027*"emerge" + 0.026*"laugh"'),
     (16,
      '0.379*"think" + 0.238*"want" + 0.072*"meet" + 0.024*"better" + 0.024*"pick" '
      '+ 0.007*"happy" + 0.004*"sir" + 0.000*"dumbledore" + 0.000*"see" + '
      '0.000*"whisper"'),
     (17,
      '0.452*"get" + 0.216*"good" + 0.036*"chance" + 0.019*"gaze" + '
      '0.001*"miserably" + 0.000*"dumbledore" + 0.000*"see" + 0.000*"hagrid" + '
      '0.000*"voldemort" + 0.000*"one"'),
     (18,
      '0.132*"stand" + 0.097*"need" + 0.095*"long" + 0.077*"summer" + '
      '0.065*"tonight" + 0.057*"boy" + 0.047*"break" + 0.033*"small" + 0.032*"bad" '
      '+ 0.026*"fact"'),
     (19,
      '0.484*"know" + 0.226*"head" + 0.032*"heavy" + 0.006*"blow" + 0.003*"duck" + '
      '0.000*"dumbledore" + 0.000*"see" + 0.000*"voldemort" + 0.000*"one" + '
      '0.000*"ask"')]



```python
# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=hp_lemmatized, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
```

    
    Perplexity:  -18.824462990598203
    
    Coherence Score:  0.31568042423488213


### Visualize the topics-keywords


```python
import pyLDAvis
import pyLDAvis.gensim_models # don't skip this

# Visualize the topics
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)
vis
#pyLDAvis.save_html(vis, './output.html')
```

    /Users/animeshgiri/opt/anaconda3/envs/specified/lib/python3.7/site-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only
      by='saliency', ascending=False).head(R).drop('saliency', 1)






<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css">


<div id="ldavis_el672031403697653589528615024962"></div>
<script type="text/javascript">

var ldavis_el672031403697653589528615024962_data = {"mdsDat": {"x": [-0.14823145808507748, -0.1977185679878552, -0.14151973342909802, -0.157550848722997, -0.04526958319983847, -0.03738938530152859, -0.17568413571885425, -0.04985349583257079, -0.007159854536501963, 0.02749387870517455, 0.05352654247694557, 0.08582354398177143, 0.07864353131658526, 0.10768805865498879, 0.08054567641442632, 0.08353613045427334, 0.07851183533609014, 0.09335296207836324, 0.13323638960771433, 0.13801851378798846], "y": [0.0731255264680495, -0.3691556797478611, 0.061561976289962766, 0.09445880401626153, 0.004165229401276756, 0.0030397884614379073, 0.1863879978888253, 0.00489662194658296, -0.0005058940612590138, -0.003090800295636446, -0.004410104133732735, -0.005657213772425447, -0.005422046177501356, -0.00614041823731505, -0.00540495598417887, -0.005578216571535103, -0.005323128202285674, -0.005760052477690437, -0.005908407228361219, -0.005279027582616194], "topics": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], "cluster": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "Freq": [9.314266684836262, 8.588546034862993, 8.297468442643671, 7.767468811477367, 7.2288987940543326, 6.946863724113647, 6.264875547407982, 6.116746698605943, 5.492217578541755, 5.045486266558486, 4.024527724243279, 3.9326856385001814, 3.6726840142757124, 3.608750256843813, 3.2832225492233427, 3.1629744461805482, 3.065539422347942, 2.9577348955725364, 1.1821901586408752, 0.046852311069330854]}, "tinfo": {"Term": ["look", "know", "come", "get", "hermione", "think", "go", "tell", "face", "hand", "eye", "feel", "turn", "want", "leave", "hear", "let", "head", "good", "snape", "stand", "take", "time", "try", "word", "right", "point", "stare", "happen", "give", "face", "right", "point", "stare", "foot", "arm", "night", "black", "pass", "call", "hurry", "say", "wish", "soon", "noise", "bit", "woman", "offer", "spread", "warn", "egg", "invisible", "mention", "pound", "plate", "dare", "snarl", "fist", "roar", "flower", "look", "let", "happen", "place", "walk", "move", "spell", "sure", "burn", "notice", "half", "past", "neck", "blaze", "kitchen", "card", "owl", "maybe", "rip", "stair", "overhead", "nervously", "stumble", "dumbledore", "see", "fudge", "return", "cedric", "whisper", "sirius", "voldemort", "ask", "time", "die", "course", "thing", "mother", "believe", "remember", "dead", "day", "reach", "close", "send", "shout", "kill", "clear", "write", "manage", "attack", "trouble", "real", "high", "big", "risk", "deal", "letter", "important", "throat", "completely", "cross", "ignore", "leave", "take", "voice", "open", "wait", "hogwart", "door", "smile", "keep", "hair", "appear", "make", "parent", "story", "ago", "exactly", "repeat", "vanish", "pretend", "jet", "welcome", "odd", "enormous", "spot", "float", "brilliant", "dumbledore", "lie", "step", "goyle", "see", "voldemort", "hermione", "feel", "table", "mean", "quickly", "silence", "tear", "fight", "emerge", "laugh", "hour", "finally", "straight", "lose", "jump", "wake", "learn", "dream", "cut", "early", "today", "fun", "draco_malfoy", "sneer", "birthday", "dumbledore", "voldemort", "wand", "see", "return", "fudge", "go", "way", "year", "old", "speak", "buy", "dark", "great", "room", "wizard", "stay", "fear", "witch", "reason", "famous", "survive", "age", "dress", "dumbledore", "lie", "step", "goyle", "voldemort", "malfoy", "floor", "crabbe", "compartment", "different", "mark", "cover", "ask", "see", "return", "fudge", "wand", "death_eater", "cedric", "come", "tell", "turn", "stop", "dursley", "run", "order", "miss", "hope", "pay", "lock", "dinner", "huge", "terror", "schedule", "dumbledore", "hagrid", "nod", "see", "kiss", "follow", "silently", "station", "fervently", "wink", "voldemort", "whisper", "find", "tightly", "return", "death_eater", "cedric", "wand", "ask", "wormtail", "fudge", "eye", "sit", "end", "ear", "later", "little", "second", "touch", "power", "bed", "escape", "cold", "low", "nose", "wonder", "slip", "wide", "clutch", "green", "narrow", "regain", "ball", "size", "bow", "bulge", "garden", "bat", "self", "dumbledore", "see", "stand", "need", "long", "summer", "tonight", "boy", "break", "small", "bad", "fact", "sign", "possible", "loud", "change", "drown", "closely", "breakfast", "number", "sharp", "normal", "breathe", "treat", "privet_drive", "message", "argue", "whip", "position", "excited", "luck", "news", "hand", "give", "ground", "potion", "scream", "mouth", "play", "blood", "have", "deep", "class", "visit", "practice", "secret", "cabin", "house", "clap", "dash", "howl", "trip", "broomstick", "poster", "homework", "player", "quidditch", "shame", "team", "hold", "wormtail", "raise", "see", "dumbledore", "voldemort", "hear", "word", "watch", "leg", "carry", "mutter", "morning", "fly", "week", "fast", "outside", "enjoy", "couple", "fat", "coat", "crabbe", "goyle", "mark", "favor", "dress_robe", "different", "hex", "cover", "floor", "compartment", "step", "malfoy", "lie", "cedric", "spider", "voldemort", "dumbledore", "see", "fudge", "return", "wand", "death_eater", "moment", "work", "suddenly", "car", "use", "water", "finish", "allow", "evening", "magic", "window", "mad", "corner", "wouldn", "wind", "spend", "eat", "promise", "clean", "holiday", "wash", "hagrid", "enter", "rise", "ensure", "bug", "intend", "color", "destroy", "uncontrollably", "professor_mcgonagall", "father", "fudge", "wand", "follow", "man", "see", "dumbledore", "voldemort", "cedric", "return", "ask", "shake", "talk", "fall", "bring", "set", "friend", "rest", "fire", "family", "effect", "decide", "backward", "forget", "panic", "grin", "subject", "crash", "dad", "simple", "injure", "apart", "weight", "wound", "pearly", "mask", "fre", "fake", "cavort", "faithless", "fair", "office", "spider", "beautiful", "find", "moody", "barty_crouch", "wormtail", "forward", "death_eater", "sirius", "wand", "dumbledore", "raise", "fudge", "see", "ask", "voldemort", "return", "cedric", "large", "sound", "chair", "glass", "school", "wear", "roll", "cry", "thin", "love", "shape", "edge", "term", "particularly", "food", "sink", "excellent", "round", "cage", "forehead", "hungry", "unusual", "jar", "beetle", "dog", "golden", "kid", "decor", "jumble", "jinx", "unconscious", "bag", "pull", "raise", "death_eater", "ask", "voldemort", "wand", "dumbledore", "goyle", "see", "lie", "fudge", "think", "want", "meet", "better", "pick", "happy", "sir", "hug", "tightly", "whisper", "idea", "relive", "anymore", "wasn", "answer", "rush", "lack", "question", "interfere", "undisturbed", "winning", "company", "ache", "bone", "carefully", "stepping", "factly", "protect", "train", "wing", "gold", "professor_mcgonagall", "instruction", "see", "madame_maxime", "potter", "dumbledore", "help", "kiss", "find", "light", "lord_voldemort", "ask", "wand", "pull", "return", "voldemort", "follow", "body", "office", "death_eater", "cedric", "fudge", "lie", "hagrid", "goyle", "wormtail", "sirius", "step", "snape", "master", "arrive", "sleep", "far", "care", "drop", "magical", "great_hall", "muggle", "lead", "home", "drink", "tall", "ghost", "line", "matter", "passageway", "wonderful", "sweep", "lucius_malfoy", "disturbance", "affront", "visibly", "sudden_movement", "warm", "glitter", "wordlessly", "paler", "apprehension", "severus", "name", "office", "moody", "remain", "man", "cornelius", "fudge", "dumbledore", "death_eater", "return", "continue", "find", "wormtail", "see", "wand", "voldemort", "ask", "know", "head", "heavy", "blow", "duck", "one", "soundly", "affect", "suggest", "murder", "spare", "urgently", "swift", "cornelius", "slam", "swing", "safe", "dementor", "goggle", "rid", "join", "preferably", "sputter", "furry", "doze", "supporter", "hoot", "crookshank", "cushion", "shocked", "lord_voldemort", "chapter_thirty", "champion", "instruction", "creature", "lunatic", "help", "ask", "sweep", "office", "raise", "pain", "fre", "dumbledore", "begin", "win", "voldemort", "strong", "see", "return", "cedric", "start", "whisper", "robe", "wand", "sirius", "death_eater", "find", "fudge", "follow", "crouch", "wormtail", "step", "body", "pull", "lie", "moody", "get", "good", "chance", "gaze", "miserably", "one", "hagrid", "money", "able", "sittin", "bushy", "disbelieve", "eyebrow", "worried", "weigh", "ter", "weakly", "thousand_galleon", "job", "expression", "rid", "preferably", "decent", "bag", "mate", "backed_away", "heaven", "master_barty", "galleon", "ruffle", "person", "couldn", "man", "maze", "hold", "pull", "usual", "raise", "dumbledore", "fudge", "death_eater", "voldemort", "ask", "return", "see", "step", "crouch", "find", "away", "body", "nod", "wand", "floor", "whisper", "cedric", "sirius", "lie", "follow", "wormtail", "try", "explain", "chamber", "crane", "flooding", "permit", "nasty", "rebuild", "pretty", "excuse", "harry", "convince", "curious", "leer", "court", "upright", "screeched", "history", "draining", "lidde", "riddle", "alternative", "fred", "afraid", "defend", "speech", "companion", "ravenclaw", "mop", "endless", "strange", "world", "goblin", "relive", "carriage", "destroy", "back", "idea", "linger", "softly", "minute", "away", "win", "fre", "crowd", "death_eater", "help", "fudge", "step", "voldemort", "find", "mind", "malfoy", "see", "dumbledore", "crabbe", "goyle", "man", "whisper", "hold", "wand", "office", "cedric", "dementor", "sirius", "body", "edit", "rr", "fried", "fry", "sweetum", "smelting", "drooped", "temple", "throb", "abnormality", "rhinocero", "stomachache", "banquet", "padlock", "blond", "neckless", "skinny", "untidy", "doorstep", "builder", "drill", "graciously", "rapturously", "tonelessly", "compliment", "golfer", "vacation", "majorca", "cunne", "remembering", "chamber", "secret", "argument", "bad", "birthday", "break", "breakfast", "change", "need", "number", "privet_drive", "scan", "time", "version", "early", "hour", "loud", "morning", "roar", "table", "wake", "week", "explain", "try", "fly", "outside", "bit", "dangle"], "Freq": [4673.0, 2219.0, 2711.0, 1997.0, 2597.0, 1859.0, 2382.0, 2215.0, 2251.0, 1777.0, 1843.0, 1851.0, 1455.0, 1171.0, 1570.0, 1239.0, 1597.0, 1035.0, 955.0, 918.0, 1088.0, 1206.0, 1236.0, 672.0, 922.0, 1228.0, 1169.0, 1143.0, 1059.0, 856.0, 2249.3865780983597, 1225.900720412441, 1166.6061273285165, 1141.4035570825233, 875.1275973937326, 676.3232067935346, 648.7892435981448, 598.2058414536597, 381.12919100022077, 361.92994464386186, 341.0366943862855, 320.3429779929378, 318.89581984220825, 292.1574529115903, 198.68684932308153, 183.40790835475886, 158.4719273736728, 126.37548932420633, 114.10183771804815, 111.1269020660439, 107.93156842253258, 79.88743488736044, 79.4902132552315, 72.89447381499443, 65.61797177915338, 61.26041361754768, 57.40321881382453, 55.34070817530106, 48.36679428277419, 43.32915366631714, 4670.49928303111, 1595.1735947354823, 1056.6028250998422, 749.3822962553555, 659.1755808955303, 439.79877075186363, 416.8383827957203, 321.12473795837076, 306.745091221752, 259.66756840595025, 187.74412936144998, 113.86207844198518, 111.74764726172037, 100.82228182197878, 97.47543389847004, 91.63474243151839, 76.10757682061987, 68.42577107813132, 45.491869656385134, 37.654543250319115, 18.693632440613346, 14.924284126157541, 9.565717616718944, 0.149637666617394, 0.1489974650897838, 0.1487972001501077, 0.14878219806981274, 0.1487638945985975, 0.14875340714122182, 0.14871576195217684, 0.14885556360867352, 0.14875612524752718, 1233.9182399189656, 695.5528468953914, 536.9189135817771, 536.6802495518388, 498.0344495551099, 488.2043536434766, 486.680089116487, 471.55026684256177, 462.7170664292882, 458.66808730629054, 449.85764537788344, 417.0772562739814, 407.31474336811783, 406.1706794118386, 299.17228601609014, 294.3136264766312, 234.53126450702743, 223.03458275229227, 220.89722403156725, 219.38957827648824, 218.4576806416011, 157.57056272761216, 155.2169868598419, 140.7848529450026, 130.45685170243922, 128.73745509526594, 128.38454854925595, 122.88581239333458, 120.1311470818274, 110.75211233780105, 1567.7308946256455, 1203.4928852542782, 916.8667127693463, 858.8899406344287, 806.6507687985217, 646.457806813077, 638.5967282923841, 621.3661229868028, 470.90041811143465, 423.3881006726508, 253.2579667360327, 228.94137968586037, 206.82314012390685, 204.36935123049523, 192.87662711871937, 191.66782877900465, 166.01077935574406, 160.89272493823367, 113.33985061249783, 87.06243423064382, 83.254635192104, 81.14975004512709, 62.49195069665164, 48.11633307920942, 35.245849707563146, 5.003809237571496, 0.14397422877743873, 0.1432529588040591, 0.14323562443851198, 0.1432164437467502, 0.14337673651349228, 0.14331758022157345, 2595.3649592970132, 1848.9700973932147, 544.9418646393714, 379.6855565941563, 375.4172761991632, 363.99570084323375, 360.87034213021826, 341.9140018945137, 294.0573839747231, 284.64511142798494, 283.6521032011968, 234.14623618316932, 222.6134976373959, 217.7825038666062, 180.58934765275995, 149.5926745807535, 131.29370132695914, 120.69200476891741, 107.10615570608816, 81.29520622752793, 34.783852935989735, 22.54468870768691, 12.909952787912244, 12.830238997709483, 1.2661413862116044, 0.17304333675020625, 0.17223641921979843, 0.1721302572297915, 0.17230642800529136, 0.17211446843216563, 0.17212420878492984, 2380.027423124996, 798.1253285940967, 757.587350047229, 695.4319917698049, 648.5085604092485, 638.628840763742, 562.6619106858012, 549.1455022056539, 547.4535983805278, 397.9575313937302, 307.413603041755, 200.51960944111343, 121.18978088619778, 69.2943916097146, 51.84540147243937, 26.917973519313964, 26.02316912057219, 14.916335121636221, 0.17167800813439885, 0.17072593389412577, 0.17068047211325724, 0.17066690340406485, 0.17083238881561993, 0.17064425743601078, 0.17065724112436179, 0.17063582181291065, 0.17063574632635045, 0.17062036593971375, 0.17060779742744372, 0.1706105904301704, 0.1706943616403304, 0.17087498210720162, 0.17070128753222694, 0.17069385210604918, 0.17067101742159466, 0.17066380845509746, 0.17065790163176336, 2708.9309474858856, 2213.275826187793, 1453.2267019458322, 497.3836780619062, 390.01357981268234, 355.6212499363325, 156.5989891721378, 131.11159518389474, 116.71618330798773, 107.68380064701212, 105.37238359076059, 34.613133196656364, 21.270481681361186, 3.6574821006874982, 0.46774754268701946, 0.11505020503035272, 0.1144952418957415, 0.11439895706405459, 0.1145643984867375, 0.11436750600458355, 0.1143961234052873, 0.1143628683348293, 0.11435556719602351, 0.1143530313692588, 0.11435780519078566, 0.1145088570730017, 0.11441669934192178, 0.11440828346047782, 0.11435835830736185, 0.1144261703996034, 0.11441450389458857, 0.1144092620513434, 0.11441595050717246, 0.11440487115667698, 0.11439273662994384, 0.11440398617015507, 1840.8289736222584, 719.9845589528254, 610.1223924622226, 599.3348540430308, 568.8352513054357, 431.13478754988415, 397.81413907534846, 372.18644424505874, 366.0110571423418, 332.23766378579285, 218.86534621486933, 204.92787203226234, 145.2686266990802, 139.1583488977397, 128.63472732658107, 125.30649604128618, 121.06674304440475, 99.90663697355497, 83.82247858198369, 57.270436707319604, 54.4571074392603, 52.39732070896563, 46.680629612829236, 41.77617130867744, 40.20310914806072, 8.399557165543907, 2.071346943968223, 1.832233848210528, 0.1439460823168068, 0.14338293016932674, 1085.4286769770595, 796.583034713873, 778.1652576812033, 634.1815894541438, 533.6776032886536, 470.14444323890416, 387.1675530586125, 269.39622868982195, 259.34102664740374, 213.3713091163863, 160.56147648019922, 147.3980270587499, 126.81360550884017, 112.22555989316605, 103.04501983961869, 102.96636832443153, 87.02306604477289, 86.22832365107371, 69.26792191632624, 69.13475823987496, 68.1175518382814, 57.601417999546314, 36.13689546709696, 35.277393145270445, 34.726981500163326, 34.36755721490001, 33.934309285234136, 30.174839812082137, 16.923337131825154, 11.170271179989207, 1774.5489153047477, 854.064247097613, 847.1306622918348, 505.3068738621052, 383.8461337828692, 269.4723008856042, 209.45313108824624, 205.8114860523828, 186.34755847185235, 185.57106569077416, 123.45171338136652, 92.70977693862203, 74.19263327706331, 69.50723395449499, 66.34735297601188, 65.29434236313534, 53.24627944559178, 51.40698287258281, 25.357789759486735, 22.912338912665092, 16.033000809924758, 13.7074435980385, 13.427764299264917, 5.885089195821072, 4.518243335332244, 2.2998446343026493, 2.1041714079162213, 0.1524167384947141, 0.1524343786514719, 0.15238418575788445, 0.15261956603139, 0.1529809767954845, 0.15253266735707421, 1236.8554266174629, 920.3396592902876, 762.2906799291004, 431.46122865721793, 330.65919314589615, 236.59527298750788, 219.90191602467442, 209.6031130411376, 131.08138190353426, 63.8860871510641, 52.95029957235967, 49.33269503535745, 42.862444033516304, 38.78614005760192, 0.5643913384185725, 0.13844277731727095, 0.13845699010077345, 0.1384158386260785, 0.1384032548462236, 0.13840247860958613, 0.13841914036501524, 0.13840677524336806, 0.13841264621624563, 0.13843906012773952, 0.13842336046842443, 0.13845043035454152, 0.13842515346572784, 0.13846129766746573, 0.13844871388761085, 0.1383185794552798, 0.13852693886142656, 0.13894056366008067, 0.13853238345079905, 0.1384480032484357, 0.1384289143869008, 0.13842398364431646, 0.13841952301687876, 683.7076191297709, 552.8040892149841, 426.2723726934034, 386.1882271428074, 245.14283056259052, 217.34145196725623, 203.52487892181708, 196.85692923681333, 196.7738741468257, 178.98275621948315, 173.1098139692123, 169.4923009520134, 168.12323756469732, 164.06876638044278, 122.64350867473817, 89.35509534057388, 71.56075563715108, 71.37552812712397, 29.019163459452038, 20.137915143777352, 11.246315710089268, 0.1614227461977496, 0.16132131785619472, 0.16132694801587444, 0.16129087012358134, 0.1612881351693536, 0.1612779325080744, 0.16129891473504024, 0.1612863510390566, 0.1612780393422239, 0.16133613575273323, 0.16131419201842168, 0.16145785189928208, 0.16144143149050075, 0.16137962793500307, 0.16136537625945702, 0.1615662244605558, 0.16197567702200855, 0.16149429302768356, 0.16141332342576187, 0.16141854761567342, 0.16140067426245863, 712.0479638362273, 692.3638540491464, 537.8294606846523, 342.15317243794664, 288.65170057387877, 262.76301693864343, 207.40219375428393, 202.58948220088234, 175.61124406987344, 136.83244807957473, 131.86005180504958, 117.85677450056475, 105.092734407732, 85.23069313612854, 77.74153590647512, 38.365418936956395, 20.97701455088028, 9.460651180892405, 3.2323864237363558, 0.14414313929146752, 0.14414799814033508, 0.1441531263709674, 0.1441361852510965, 0.14413632493053008, 0.14413798112952805, 0.1441671142685286, 0.14413568639597663, 0.14413279303628138, 0.14413279303628138, 0.1441393579696589, 0.1442235846680982, 0.14415136042384302, 0.1441443964063696, 0.14426150763431092, 0.14422098064437247, 0.14416381184763505, 0.14424449667472328, 0.1441640313438878, 0.14426016072548728, 0.14422219785086496, 0.1442640517954223, 0.14470899065393913, 0.14420418918103756, 0.1442702076676015, 0.14436179746761013, 0.14424528486581267, 0.14430791113756147, 0.14425375542574811, 0.14423285339622544, 519.7028633175562, 400.8318386142902, 362.5279186669759, 278.9767689894022, 270.93903969534813, 257.28409334058796, 250.08319041276073, 215.17197094988467, 181.02403430766233, 169.08343759141022, 142.17985138796797, 137.19978083203708, 96.09034427520463, 81.25891200405819, 69.73768214780044, 65.7674500023471, 65.44048941275966, 60.74209485262961, 43.60797461709389, 38.00938437167736, 26.955275490386853, 2.136532825541296, 0.17850864643957579, 0.17850841115745056, 0.17850713671260562, 0.17849798031656575, 0.1784766480705457, 0.17846604076806696, 0.17846606037491075, 0.17846653093916118, 0.178488137680994, 0.17848262815789506, 0.17860234754594514, 0.17856189862725086, 0.17863201270056678, 0.17862813054550064, 0.17872430211418483, 0.17863458119710046, 0.17919827795544302, 0.17858191721473843, 0.17875004590005278, 0.17859942612622365, 0.1786241699630594, 1856.7990852753826, 1168.9925927566603, 352.6832833256248, 118.32455673302196, 117.84535800875416, 34.47106360135489, 18.388895992118943, 0.13317167322457354, 0.1331804139456623, 0.13322911224887105, 0.13308563955557146, 0.13307749641439387, 0.13308398952148837, 0.13310245206501256, 0.133093194035941, 0.13308127811413023, 0.13307226090084376, 0.13309826008653122, 0.13306776567285528, 0.1330668559243338, 0.13307646179842825, 0.1330699776104369, 0.13306797973133092, 0.13307531123412167, 0.13307337578873774, 0.13306633861635098, 0.13306640105007306, 0.1330650007508782, 0.13307597124775492, 0.13307377714837956, 0.13311435906772018, 0.13311462664081475, 0.13307758560542537, 0.13333001406282716, 0.13308908232938804, 0.13310028472294666, 0.13363519901537174, 0.13312231490773158, 0.13311727561445083, 0.1331684980238515, 0.13312450900710693, 0.13311771265050526, 0.13316869424412087, 0.1331755441153414, 0.1331435869687486, 0.1331761238570463, 0.133220353689576, 0.1331329108022759, 0.13312526713087483, 0.1331243306250439, 0.13315239904266254, 0.13315005331853363, 0.13315935594312095, 0.13314194585376865, 0.1331304134533934, 0.13313019939491774, 0.13313424866774867, 0.13312673878289485, 0.1331251779398433, 916.2864738981824, 343.68028061138654, 247.312471156995, 244.21643409317414, 229.8325441340204, 195.97452860342364, 187.45337895176243, 171.14677075284698, 149.1737027396233, 141.33451957826728, 123.43216012036928, 120.88982269058498, 110.38070394128799, 105.28526832131216, 76.14695925969389, 73.31279669464675, 68.31630639859526, 28.840474442006997, 2.7076185962660304, 0.12809842724428014, 0.1280888380803594, 0.12808465356169507, 0.1280853667342806, 0.12808537532672137, 0.1280854268813661, 0.12809389902798424, 0.1280876609159713, 0.1280839919437543, 0.1280839919437543, 0.12808406068328063, 0.128097903105392, 0.12810313590183262, 0.12815683006432427, 0.1281568042870019, 0.1281219275698386, 0.1281449724960352, 0.1281006269091222, 0.12819698254013218, 0.1285493671293377, 0.12817618024098154, 0.12818353537029709, 0.1281240670875951, 0.12816816349372523, 0.12815822203973212, 0.12825377857374873, 0.12817602557704733, 0.12821693418764468, 0.12816575761030424, 2217.1377826692146, 1032.4551270921743, 145.92520961234248, 27.50612538471918, 11.67951912909027, 0.12321519968330316, 0.12310224205859159, 0.12310432399652863, 0.12310867108294114, 0.12310624770718244, 0.12310255018540626, 0.12310033500344127, 0.12310120108962307, 0.12312069635646543, 0.12310740526467542, 0.12310327469980835, 0.12310494025015797, 0.12314909398992654, 0.12309919410145177, 0.12309893594114758, 0.12310649753973488, 0.12309823641000074, 0.12309914413494129, 0.12309958550578394, 0.12309959383353568, 0.12311285994207045, 0.12309988530484686, 0.12310050155847622, 0.12310249189114403, 0.1230988360081266, 0.12315789642352432, 0.12310685563306005, 0.12311430064312287, 0.12310931231982575, 0.1231088709489831, 0.12310847121689919, 0.12314961031053492, 0.12319810280896426, 0.12310820472884325, 0.12315866257668513, 0.12315443207879709, 0.1231288242421716, 0.12311978030377314, 0.12355165750943094, 0.12314326456370285, 0.12313524493876941, 0.12324025788831328, 0.12312087123925214, 0.12326563254788982, 0.12318914214808328, 0.12317889901343308, 0.12313356273291629, 0.12317383574037022, 0.12313036487624501, 0.12318096429586661, 0.12315326619355235, 0.1231746435322898, 0.12316954694821994, 0.12318471178415327, 0.12315245007388104, 0.12314403071686368, 0.1231601782275033, 0.1231570303373425, 0.12314551305667484, 0.12315127586088455, 0.1231574800359369, 0.12314517994660493, 1995.1484648668725, 952.7440925505856, 157.12245484764586, 81.88446543403288, 4.805811311354144, 0.1320803469323938, 0.13208576245042997, 0.13195260016494986, 0.1319675932757147, 0.1319387801485964, 0.13193884442774223, 0.13193888460220837, 0.13193914975368493, 0.131940065731513, 0.13193948721920054, 0.1319516359777624, 0.13193650627381268, 0.13193977647535676, 0.13194900053278338, 0.13194697573968972, 0.13193241651315923, 0.13193170944255508, 0.13193372620075552, 0.13194466169043984, 0.13193068097622182, 0.13193096219748482, 0.1319344172015732, 0.13194972367317395, 0.13193076936004733, 0.13193039975495882, 0.1319530019096113, 0.13196017706926458, 0.13200416810969198, 0.13195628818094185, 0.1319986401031506, 0.13200959969751463, 0.1319542071435956, 0.13198923124317974, 0.13241103903300955, 0.13204163481681766, 0.13202558110014662, 0.13208088527024012, 0.1320230340389931, 0.13203044221055, 0.13210768967405123, 0.13200360566716599, 0.13198331756176337, 0.13201170483954056, 0.13198078657039633, 0.13198550305272158, 0.13198458707489352, 0.13201701590396475, 0.13199102302436974, 0.13200583133259033, 0.13200631342618407, 0.13198749570624232, 0.13199789285808033, 0.1319853664595367, 0.13198883753341153, 669.8430028062652, 108.98276153006634, 13.034909908850375, 0.10479691939774934, 0.10479503745754402, 0.10479522372466332, 0.1047967588226465, 0.10479529437770857, 0.10479678451466296, 0.10479679093766707, 0.10479539072277028, 0.10479315551733871, 0.10479532006972503, 0.10479851872577366, 0.1047932904004251, 0.10479056062367677, 0.1047906698147467, 0.10479548064482787, 0.10478921179281289, 0.10478921179281289, 0.10478932098388283, 0.10478932740688694, 0.10479704785783162, 0.10479557698988957, 0.10479063127672202, 0.10479472273034246, 0.10478926959984991, 0.10478878145153728, 0.1047889805646648, 0.10478876218252493, 0.10479510811058927, 0.10479757454416894, 0.10479930233227552, 0.10479472915334657, 0.10479750389112369, 0.10479559625890192, 0.10479582748705, 0.10479626425132975, 0.10479597521614463, 0.104799385831329, 0.10479796634741988, 0.10481575164581074, 0.10480874414832268, 0.10480071539318055, 0.10479852514877777, 0.10481926502906093, 0.10480630982976359, 0.10482277841231114, 0.10481007371017421, 0.10482450620041772, 0.10481002874914543, 0.10479817830655563, 0.10480334882486717, 0.10482076158901943, 0.10485342898794175, 0.10480262302540232, 0.10480392047223329, 0.10480202568601975, 0.10480384981918803, 0.1048010172743739, 0.10480394616424973, 0.1048005548180777, 0.10480149899968241, 0.10479829392062967, 0.10479854441779012, 0.10479844807272841, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708, 0.007527872741444708], "Total": [4673.0, 2219.0, 2711.0, 1997.0, 2597.0, 1859.0, 2382.0, 2215.0, 2251.0, 1777.0, 1843.0, 1851.0, 1455.0, 1171.0, 1570.0, 1239.0, 1597.0, 1035.0, 955.0, 918.0, 1088.0, 1206.0, 1236.0, 672.0, 922.0, 1228.0, 1169.0, 1143.0, 1059.0, 856.0, 2251.978461450166, 1228.492603764247, 1169.198010688832, 1143.9954404343293, 877.7194807455385, 678.9150901453405, 651.3811269499507, 600.7977248054656, 383.72107435202673, 364.5218279956678, 343.62857773809145, 322.93486134474375, 321.4877031940142, 294.7493362723154, 201.27873267488746, 185.9997917065648, 161.06381072547873, 128.96737269545466, 116.69372121755127, 113.71878542676895, 110.52345177433853, 82.47931823916639, 82.08209660703744, 75.48635716680037, 68.20985513095933, 63.8522969693536, 59.995102165630456, 57.932591527106986, 50.95867764526353, 45.92103939007145, 4673.114256732916, 1597.788568437288, 1059.2177988016479, 751.9972699571609, 661.7905545973357, 442.41374445366915, 419.4533564975258, 323.7397116601763, 309.3600649235575, 262.2825421077558, 190.35910306325547, 116.4770521437907, 114.3626209635259, 103.43725587357333, 100.09040762143648, 94.24971642705809, 78.7225505224254, 71.04074477993684, 48.10684335819064, 40.26951695212462, 21.308606284865032, 17.539257827963066, 12.180691876199653, 2.77851751402332, 2.769673929566647, 2.7670604339640805, 2.766834863253999, 2.7665076244871245, 2.7663537360908252, 2.7656587590507544, 2.768454949084858, 2.7665374051467997, 1236.5284655846488, 698.1630725610745, 539.5291392474602, 539.2904752175218, 500.64467522079303, 490.81457930915974, 489.2903147821701, 474.1604925082449, 465.3272920949713, 461.27831297197366, 452.46787104356656, 419.6874819396645, 409.92496903380095, 408.7809050775217, 301.78251168177326, 296.92385215286475, 237.14149017271052, 225.64480841797536, 223.50744969725034, 221.99980394217133, 221.06790630728418, 160.1807884563101, 157.82721260345986, 143.3950786467666, 133.0670773681223, 131.34768077261472, 130.99477426530248, 125.49603805901768, 122.7413727475105, 113.36233800348415, 1570.3514221041064, 1206.113412732739, 919.487240247807, 861.5104681128894, 809.2712962769823, 649.0783342915377, 641.2172557708448, 623.9866504652634, 473.5209455898954, 426.00862815111157, 255.8784942144934, 231.56190716432107, 209.44366760236755, 206.98987870895593, 195.49715460845042, 194.28835628178248, 168.63130683420476, 163.51325241669437, 115.96037810222889, 89.68296170910455, 85.87516276849531, 83.77027752358782, 65.11247817511239, 50.73686055767013, 37.866377186023854, 7.624336716032222, 2.77851751402332, 2.7662881478936425, 2.765967494905895, 2.765760675762569, 2.769673929566647, 2.768454949084858, 2597.9566402517185, 1851.56177834792, 547.5335455940764, 382.2772375488613, 378.0089571538682, 366.58738179793875, 363.46202308492326, 344.5056828492187, 296.6490649294281, 287.23679238268994, 286.2437841559018, 236.73791713787432, 225.2051785921009, 220.3741848213112, 183.18102860746495, 152.1843555354585, 133.88538228166414, 123.28368572362245, 109.6978366607932, 83.88688718223297, 37.37553389069473, 25.13636966239193, 15.501633742617264, 15.421919952414502, 3.8578223409166226, 2.77851751402332, 2.768454949084858, 2.7667687184622918, 2.769673929566647, 2.766834863253999, 2.7670604339640805, 2382.6205157316776, 800.718421200778, 760.1804426539104, 698.0250843764862, 651.1016530159299, 641.2219335879278, 565.2550032924826, 551.7385948123352, 550.0466909872091, 400.55062400041163, 310.0066956484364, 203.11270204779478, 123.78287349287918, 71.88748423246578, 54.43849407912073, 29.511066125995352, 28.61626172725358, 17.509427728317608, 2.77851751402332, 2.7662881478936425, 2.765967494905895, 2.765760675762569, 2.768454949084858, 2.7654793009753704, 2.765704868625524, 2.7654027297849018, 2.7654082522841104, 2.765188947220935, 2.765062498021084, 2.7651231089293806, 2.7665374051467997, 2.769673929566647, 2.766834863253999, 2.7670604339640805, 2.7667687184622918, 2.7665721972468544, 2.7665076244871245, 2711.580215523413, 2215.92509422532, 1455.8759699833595, 500.03294609943316, 392.66284807712543, 358.27051797385946, 159.24825720966476, 133.7608632214217, 119.36545134551473, 110.33306868453911, 108.02165190545998, 37.26240123418332, 23.919749718888166, 6.306750138214473, 3.1170155802139936, 2.77851751402332, 2.7657594519978352, 2.765388551621136, 2.769673929566647, 2.764955613863765, 2.7656528335872106, 2.7648705727229563, 2.7647225559331785, 2.7646638940849453, 2.764784689407921, 2.768454949084858, 2.7663537360908252, 2.766369838291837, 2.765185574124514, 2.766834863253999, 2.7665721972468544, 2.7665076244871245, 2.7667687184622918, 2.7665374051467997, 2.7660867567869043, 2.7670604339640805, 1843.4495737564703, 722.6051590870371, 612.7429925964343, 601.9554542753667, 571.455851601029, 433.7553876840959, 400.43473920956023, 374.8070443792705, 368.6316572765536, 334.85826406718525, 221.48594637032852, 207.5484721664741, 147.88922684821196, 141.77894903195147, 131.25532746079284, 127.92709617549797, 123.68734317861653, 102.52723710776675, 86.44307871619547, 59.89103684153136, 57.07770777110745, 55.017920843177386, 49.30122979540002, 44.3967714428892, 42.823709282272475, 11.020157299755681, 4.691947078179996, 4.452833982422301, 2.77851751402332, 2.769673929566647, 1088.0474567388865, 799.2018144756998, 780.7840374430302, 636.800369396768, 536.2963830504805, 472.7632230007311, 389.78633282043944, 272.0150084516489, 261.9598064092307, 215.99008887821321, 163.18025624202613, 150.01680682057682, 129.43238529400196, 114.84433965499301, 105.66379962576276, 105.58514813018243, 89.64184580659985, 88.84710341290067, 71.88670168908611, 71.75353800170191, 70.73633160010836, 60.220197761373235, 38.75567842538323, 37.89617316049561, 37.345761489489355, 36.98633715355911, 36.55308904706106, 32.79361957390906, 19.542116893652096, 13.789050941816146, 1777.160174931749, 856.6755067246144, 849.7419219188362, 507.91813348910654, 386.4573934098705, 272.0835605126056, 212.06439071524758, 208.42274582517163, 188.9588180988537, 188.1823253177755, 126.06297300836788, 95.32103656562339, 76.80389290406467, 72.11849358149635, 68.95861260301324, 67.9056019901367, 55.85753908151221, 54.01824249958413, 27.969049386488084, 25.52359853966644, 18.644260436926107, 16.318703225039844, 16.03902392626626, 8.496348822822418, 7.129502994131451, 4.9111042613039935, 4.715431034917565, 2.765451864054874, 2.7660867567869043, 2.7652921368252006, 2.769673929566647, 2.77851751402332, 2.768454949084858, 1239.480672230008, 922.9649049133832, 764.9159255416455, 434.0864742697631, 333.2844387584413, 239.22051860005303, 222.52716163721956, 212.22835865368273, 133.7066275160794, 66.51133276360926, 55.57554518490479, 51.95794071118159, 45.487689646061426, 41.411386746313084, 3.1896369509637106, 2.7654027297849018, 2.765760675762569, 2.765062498021084, 2.7648384314586507, 2.764839632881024, 2.765188947220935, 2.7649530875423785, 2.7651231089293806, 2.765704868625524, 2.7654082522841104, 2.765967494905895, 2.7654793009753704, 2.7662881478936425, 2.7665076244871245, 2.7639505071563577, 2.768454949084858, 2.77851751402332, 2.769673929566647, 2.7670604339640805, 2.766834863253999, 2.7667687184622918, 2.7665721972468544, 686.309885768133, 555.4063558533461, 428.8746393317654, 388.7907752335403, 247.74509720095259, 219.9437186056183, 206.12714556017914, 199.4591958751754, 199.37614078518777, 181.58502285784522, 175.71208060757436, 172.09456759037548, 170.7255042030594, 166.67103301880485, 125.24577531310024, 91.95736197893595, 74.16302227551314, 73.97779476548604, 31.621430110465525, 22.740181782139413, 13.848582348451325, 2.7657594519978352, 2.764245552620813, 2.764370067286199, 2.7637974162223493, 2.763772537230516, 2.763619136678008, 2.7640002875047625, 2.763794878754576, 2.7636525280872366, 2.7646619906156396, 2.764316811959405, 2.7670604339640805, 2.7667687184622918, 2.7656528335872106, 2.765366138705835, 2.769673929566647, 2.77851751402332, 2.768454949084858, 2.7665076244871245, 2.766834863253999, 2.7665374051467997, 714.6673663542163, 694.9832565671354, 540.4488632026413, 344.77257495593545, 291.2711030918676, 265.38241945663225, 210.0215962722728, 205.20888471887122, 178.2306465878623, 139.4518507694338, 134.47945432303845, 120.47617701855366, 107.7121369257209, 87.85009569705035, 80.36093842446402, 40.98482166611517, 23.596418372492877, 12.080053698881294, 5.851788941725242, 2.763702507244353, 2.7638245390748084, 2.7639238522555982, 2.763610757327096, 2.7636162856390802, 2.7636691963701625, 2.764228125835199, 2.7636281098656483, 2.7635786814100376, 2.7635786814100376, 2.7637065550144113, 2.7653912567496994, 2.7639505071563577, 2.7638128986851065, 2.766369838291837, 2.765485418485885, 2.7642436001577826, 2.7660867567869043, 2.7642602759880384, 2.7665721972468544, 2.7656587590507544, 2.7667687184622918, 2.77851751402332, 2.7652921368252006, 2.7670604339640805, 2.769673929566647, 2.7665374051467997, 2.768454949084858, 2.766834863253999, 2.7665076244871245, 522.2879451700442, 403.4169204667781, 365.1130005194638, 281.56185084189013, 273.52412154783605, 259.8691751930759, 252.66827226524865, 217.75705280237258, 183.60911616015025, 171.66851944389813, 144.7649332404559, 139.784862684525, 98.67542612769259, 83.84399385654615, 72.3227640002884, 68.35253185483506, 68.02557126524762, 63.32717670511755, 46.1930565351621, 40.594466243803105, 29.540357342874806, 4.721614678029247, 2.764069958297526, 2.764103875986333, 2.764214777943388, 2.7640922753813504, 2.763771915339404, 2.76364109544132, 2.763641530453572, 2.7636505956299224, 2.7639892888619837, 2.7639046559626297, 2.765826511214359, 2.7652921368252006, 2.7665721972468544, 2.7665374051467997, 2.768454949084858, 2.7667687184622918, 2.77851751402332, 2.765760675762569, 2.769673929566647, 2.7662881478936425, 2.7670604339640805, 1859.4295569699668, 1171.6230644512445, 355.31375502020927, 120.9550284276064, 120.47582970333859, 37.101535295939314, 21.019367686703376, 2.7648457202072936, 2.765185574124514, 2.7663537360908252, 2.7638494633204185, 2.7637235880426023, 2.7638818349517895, 2.7642681918018632, 2.7640989944625276, 2.7638599965427635, 2.7637072809011696, 2.7642520690006265, 2.7636209351655925, 2.763612965161674, 2.763812706488835, 2.763684750346165, 2.7636570969222043, 2.763811147624167, 2.763772292011951, 2.7636271792977642, 2.763629730618348, 2.763604860279104, 2.7638360175768093, 2.7637930281285414, 2.764651874341331, 2.7646619906156396, 2.763875904812047, 2.769673929566647, 2.764167651972925, 2.7644641567386636, 2.77851751402332, 2.7650447209971944, 2.764955613863765, 2.766369838291837, 2.7651643749964627, 2.7649814128759185, 2.7665374051467997, 2.7667687184622918, 2.765826511214359, 2.766834863253999, 2.768454949084858, 2.7656528335872106, 2.765424004702526, 2.7653912567496994, 2.7665721972468544, 2.7665076244871245, 2.7670604339640805, 2.7662881478936425, 2.7657594519978352, 2.765760675762569, 2.7660867567869043, 2.7656587590507544, 2.765967494905895, 918.921926579054, 346.3157332922582, 249.94792383786665, 246.85188681800074, 232.46799681489205, 198.6099812842953, 190.08883163263408, 173.78222343371863, 151.80915544147416, 143.96997225913893, 126.06761280124095, 123.52527537145666, 113.01615664179748, 107.92072100218384, 78.78241194056557, 75.94824937551843, 70.95175907946694, 31.47592724505092, 5.343071277137706, 2.76387762908771, 2.7636734483044982, 2.763600168486153, 2.7636158366311614, 2.7636161207735923, 2.7636184059962225, 2.763817678217302, 2.763694424341909, 2.7636234311075203, 2.7636234311075203, 2.763627036363853, 2.7639479346953855, 2.7640737556410766, 2.7653912567496994, 2.765485418485885, 2.7646300570502103, 2.765366138705835, 2.7640802165665392, 2.7670604339640805, 2.77851751402332, 2.7665721972468544, 2.766834863253999, 2.7648440581829754, 2.766369838291837, 2.7660867567869043, 2.769673929566647, 2.7667687184622918, 2.768454949084858, 2.7665374051467997, 2219.778217845243, 1035.0955622682027, 148.5656447883708, 30.146560560747517, 14.319954305118607, 2.7651039882064823, 2.7636188704775684, 2.763669580456923, 2.7637885270973483, 2.7637355555209018, 2.7636637369118064, 2.7636148978943096, 2.763637219889925, 2.7640802165665392, 2.76378200180361, 2.7636899401596153, 2.7637299894690965, 2.7647275701633545, 2.763610791730069, 2.7636051644657953, 2.763775888824077, 2.7635932108663335, 2.763617562424925, 2.7636277166528083, 2.7636279453764767, 2.763926300519762, 2.7636354412464965, 2.7636497106101365, 2.7636979149431222, 2.7636171131255582, 2.7649814128759185, 2.7638032045403165, 2.763998729434507, 2.763875904812047, 2.7638743542752753, 2.7638662096733, 2.7650447209971944, 2.7665374051467997, 2.76387762908771, 2.7653912567496994, 2.7652921368252006, 2.7645124970080537, 2.764228125835199, 2.77851751402332, 2.7650261105132343, 2.7647744854935863, 2.768454949084858, 2.764312448651756, 2.769673929566647, 2.766834863253999, 2.7665076244871245, 2.7647916708972704, 2.7663537360908252, 2.7646993937970894, 2.7667687184622918, 2.7656587590507544, 2.7665721972468544, 2.766369838291837, 2.7670604339640805, 2.7656528335872106, 2.765291454502424, 2.7660867567869043, 2.765967494905895, 2.765424004702526, 2.765826511214359, 2.7662881478936425, 2.765485418485885, 1997.780073934045, 955.3757016177581, 159.75406391481835, 84.5160745012054, 7.437420378526658, 2.7651039882064823, 2.7657594519978352, 2.763865233549158, 2.7641970921028034, 2.7636265271797815, 2.7636294425309096, 2.7636307830796065, 2.763640512928982, 2.7636759940546938, 2.7636785249048526, 2.763959631288032, 2.7636456719572418, 2.763725715378515, 2.7639365487806384, 2.7638945895124225, 2.7636051644657953, 2.7635932108663335, 2.7636490192654652, 2.7639046559626297, 2.763624376052884, 2.7636304592269365, 2.7637081848996443, 2.764034815586501, 2.7636383928458406, 2.7636309661315352, 2.7641413738050242, 2.7643104170542427, 2.765366138705835, 2.7642749414763927, 2.765451864054874, 2.765826511214359, 2.7642445502974424, 2.7652921368252006, 2.77851751402332, 2.7670604339640805, 2.7665721972468544, 2.768454949084858, 2.7665374051467997, 2.766834863253999, 2.769673929566647, 2.765967494905895, 2.765291454502424, 2.766369838291837, 2.7652031100683487, 2.765424004702526, 2.765388551621136, 2.7667687184622918, 2.765704868625524, 2.7663537360908252, 2.7665076244871245, 2.7656587590507544, 2.7662881478936425, 2.7656528335872106, 2.7660867567869043, 672.5017419826644, 111.64150070646559, 15.693649085249625, 2.763628774737018, 2.7636127598554925, 2.7636198092301556, 2.763664693938666, 2.7636349842072123, 2.763677727327195, 2.7636836258431123, 2.763653908686333, 2.7635950412967514, 2.76366116053468, 2.7637467100911692, 2.7636379740975974, 2.7635688193596795, 2.763576168219858, 2.763705582759196, 2.763542275728178, 2.763542275728178, 2.763548542915167, 2.763550239696068, 2.763759806325335, 2.763721514259208, 2.763593240623255, 2.763701187251536, 2.7635579755005346, 2.763547546266131, 2.763553827513231, 2.763548079288991, 2.7637196136658635, 2.7637934634760177, 2.763846434930627, 2.7637235880426023, 2.763844808794009, 2.763794878754576, 2.763813200419488, 2.7638494633204185, 2.76387744295022, 2.76406191951779, 2.764041915139614, 2.7652031100683487, 2.7647744854935863, 2.764228125835199, 2.764134629944022, 2.7665721972468544, 2.7650447209971944, 2.7670604339640805, 2.765967494905895, 2.768454949084858, 2.766369838291837, 2.7642017906607337, 2.7654793009753704, 2.769673929566647, 2.77851751402332, 2.7654027297849018, 2.765760675762569, 2.765366138705835, 2.7663537360908252, 2.765451864054874, 2.7667687184622918, 2.7653912567496994, 2.7665076244871245, 2.7647275701633545, 2.7656587590507544, 2.765424004702526, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 2.7635259924115276, 15.693649085249625, 72.11849358149635, 2.7635287531287855, 261.9598064092307, 3.8578223409166226, 389.78633282043944, 89.64184580659985, 114.84433965499301, 799.2018144756998, 88.84710341290067, 38.75567842538323, 2.763543922774949, 1236.5284655846488, 2.763537012618434, 83.88688718223297, 286.2437841559018, 129.43238529400196, 222.52716163721956, 50.95867764526353, 547.5335455940764, 152.1843555354585, 133.7066275160794, 111.64150070646559, 672.5017419826644, 212.22835865368273, 55.57554518490479, 185.9997917065648, 19.996931357788505], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic12", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic13", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic14", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic15", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic16", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic17", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic18", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic19", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20", "Topic20"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.822, -2.429, -2.4785, -2.5004, -2.766, -3.0237, -3.0653, -3.1465, -3.5973, -3.6489, -3.7084, -3.771, -3.7755, -3.8631, -4.2487, -4.3287, -4.4748, -4.7011, -4.8033, -4.8297, -4.8589, -5.1598, -5.1648, -5.2514, -5.3565, -5.4253, -5.4903, -5.5269, -5.6616, -5.7716, -1.0103, -2.0845, -2.4965, -2.84, -2.9683, -3.373, -3.4266, -3.6874, -3.7333, -3.8999, -4.2242, -4.7243, -4.743, -4.8459, -4.8797, -4.9415, -5.1271, -5.2335, -5.6417, -5.8308, -6.5311, -6.7563, -7.2011, -11.3588, -11.3631, -11.3644, -11.3645, -11.3647, -11.3647, -11.365, -11.3641, -11.3647, -2.3068, -2.8801, -3.1389, -3.1394, -3.2141, -3.2341, -3.2372, -3.2688, -3.2877, -3.2965, -3.3159, -3.3915, -3.4152, -3.418, -3.7238, -3.7401, -3.9672, -4.0175, -4.0271, -4.0339, -4.0382, -4.3649, -4.38, -4.4776, -4.5538, -4.567, -4.5698, -4.6135, -4.6362, -4.7175, -2.0014, -2.2658, -2.5378, -2.6031, -2.6659, -2.8873, -2.8995, -2.9269, -3.2041, -3.3105, -3.8244, -3.9253, -4.0269, -4.0389, -4.0967, -4.103, -4.2467, -4.2781, -4.6284, -4.8922, -4.9369, -4.9625, -5.2238, -5.4852, -5.7964, -7.7486, -11.2969, -11.3019, -11.3021, -11.3022, -11.3011, -11.3015, -1.4254, -1.7645, -2.9863, -3.3476, -3.3589, -3.3898, -3.3984, -3.4524, -3.6032, -3.6357, -3.6392, -3.831, -3.8815, -3.9034, -4.0907, -4.279, -4.4095, -4.4937, -4.6131, -4.8888, -5.7378, -6.1714, -6.7289, -6.7351, -9.051, -11.0411, -11.0458, -11.0464, -11.0454, -11.0465, -11.0465, -1.4723, -2.5649, -2.617, -2.7026, -2.7725, -2.7878, -2.9145, -2.9388, -2.9419, -3.2608, -3.5189, -3.9462, -4.4498, -5.0088, -5.2989, -5.9543, -5.9881, -6.5447, -11.0093, -11.0148, -11.0151, -11.0152, -11.0142, -11.0153, -11.0152, -11.0154, -11.0154, -11.0154, -11.0155, -11.0155, -11.015, -11.014, -11.015, -11.015, -11.0152, -11.0152, -11.0152, -1.2395, -1.4416, -1.8623, -2.9344, -3.1776, -3.2699, -4.0901, -4.2678, -4.3841, -4.4646, -4.4863, -5.5996, -6.0865, -7.847, -9.9036, -11.3062, -11.311, -11.3119, -11.3104, -11.3121, -11.3119, -11.3122, -11.3122, -11.3123, -11.3122, -11.3109, -11.3117, -11.3118, -11.3122, -11.3116, -11.3117, -11.3118, -11.3117, -11.3118, -11.3119, -11.3118, -1.6019, -2.5406, -2.7062, -2.7241, -2.7763, -3.0535, -3.1339, -3.2005, -3.2172, -3.314, -3.7314, -3.7972, -4.1413, -4.1843, -4.2629, -4.2891, -4.3235, -4.5156, -4.6912, -5.0721, -5.1225, -5.161, -5.2765, -5.3875, -5.4259, -6.9917, -8.3917, -8.5143, -11.0582, -11.0621, -2.0224, -2.3318, -2.3552, -2.5598, -2.7324, -2.8591, -3.0533, -3.416, -3.454, -3.6491, -3.9335, -4.019, -4.1695, -4.2917, -4.377, -4.3778, -4.546, -4.5552, -4.7742, -4.7761, -4.7909, -4.9586, -5.4249, -5.4489, -5.4647, -5.4751, -5.4877, -5.6052, -6.1835, -6.5989, -1.446, -2.1773, -2.1855, -2.7022, -2.9771, -3.3309, -3.5828, -3.6004, -3.6997, -3.7039, -4.1115, -4.3979, -4.6207, -4.6859, -4.7324, -4.7484, -4.9524, -4.9876, -5.6943, -5.7957, -6.1527, -6.3094, -6.33, -7.1549, -7.4192, -8.0945, -8.1834, -10.8085, -10.8084, -10.8087, -10.8071, -10.8048, -10.8077, -1.5809, -1.8765, -2.0649, -2.6341, -2.9002, -3.2349, -3.3081, -3.356, -3.8254, -4.5442, -4.7319, -4.8027, -4.9433, -5.0432, -9.2733, -10.6785, -10.6784, -10.6787, -10.6788, -10.6788, -10.6787, -10.6788, -10.6788, -10.6786, -10.6787, -10.6785, -10.6787, -10.6784, -10.6785, -10.6794, -10.6779, -10.675, -10.6779, -10.6785, -10.6787, -10.6787, -10.6787, -2.1506, -2.3632, -2.6231, -2.7218, -3.1763, -3.2967, -3.3624, -3.3957, -3.3961, -3.4909, -3.5242, -3.5454, -3.5535, -3.5779, -3.8689, -4.1855, -4.4076, -4.4102, -5.3102, -5.6756, -6.2581, -10.5019, -10.5025, -10.5025, -10.5027, -10.5027, -10.5028, -10.5027, -10.5027, -10.5028, -10.5024, -10.5026, -10.5017, -10.5018, -10.5022, -10.5023, -10.501, -10.4985, -10.5015, -10.502, -10.5019, -10.502, -2.0416, -2.0697, -2.3222, -2.7745, -2.9445, -3.0385, -3.2751, -3.2986, -3.4415, -3.691, -3.728, -3.8403, -3.9549, -4.1644, -4.2564, -4.9626, -5.5663, -6.3626, -7.4365, -10.5467, -10.5467, -10.5466, -10.5468, -10.5468, -10.5468, -10.5465, -10.5468, -10.5468, -10.5468, -10.5467, -10.5462, -10.5467, -10.5467, -10.5459, -10.5462, -10.5466, -10.546, -10.5466, -10.5459, -10.5462, -10.5459, -10.5428, -10.5463, -10.5458, -10.5452, -10.546, -10.5456, -10.5459, -10.5461, -2.3389, -2.5987, -2.6991, -2.9611, -2.9903, -3.042, -3.0704, -3.2208, -3.3936, -3.4618, -3.6351, -3.6708, -4.0269, -4.1946, -4.3475, -4.4061, -4.4111, -4.4856, -4.817, -4.9544, -5.298, -7.833, -10.3153, -10.3153, -10.3153, -10.3154, -10.3155, -10.3156, -10.3156, -10.3156, -10.3154, -10.3155, -10.3148, -10.315, -10.3146, -10.3147, -10.3141, -10.3146, -10.3115, -10.3149, -10.314, -10.3148, -10.3147, -0.9711, -1.4338, -2.6321, -3.7242, -3.7283, -4.9575, -5.5859, -10.5138, -10.5137, -10.5134, -10.5144, -10.5145, -10.5144, -10.5143, -10.5144, -10.5145, -10.5145, -10.5143, -10.5146, -10.5146, -10.5145, -10.5145, -10.5146, -10.5145, -10.5145, -10.5146, -10.5146, -10.5146, -10.5145, -10.5145, -10.5142, -10.5142, -10.5145, -10.5126, -10.5144, -10.5143, -10.5103, -10.5142, -10.5142, -10.5138, -10.5141, -10.5142, -10.5138, -10.5138, -10.514, -10.5138, -10.5134, -10.5141, -10.5141, -10.5141, -10.5139, -10.5139, -10.5139, -10.514, -10.5141, -10.5141, -10.5141, -10.5141, -10.5141, -1.64, -2.6206, -2.9497, -2.9623, -3.023, -3.1824, -3.2268, -3.3178, -3.4552, -3.5092, -3.6447, -3.6655, -3.7564, -3.8037, -4.1277, -4.1656, -4.2362, -5.0986, -7.4643, -10.5153, -10.5154, -10.5154, -10.5154, -10.5154, -10.5154, -10.5153, -10.5154, -10.5154, -10.5154, -10.5154, -10.5153, -10.5153, -10.5149, -10.5149, -10.5151, -10.515, -10.5153, -10.5145, -10.5118, -10.5147, -10.5146, -10.5151, -10.5148, -10.5148, -10.5141, -10.5147, -10.5144, -10.5148, -0.7251, -1.4894, -3.446, -5.1147, -5.9712, -10.5229, -10.5238, -10.5238, -10.5238, -10.5238, -10.5238, -10.5238, -10.5238, -10.5237, -10.5238, -10.5238, -10.5238, -10.5234, -10.5238, -10.5238, -10.5238, -10.5238, -10.5238, -10.5238, -10.5238, -10.5237, -10.5238, -10.5238, -10.5238, -10.5238, -10.5234, -10.5238, -10.5237, -10.5238, -10.5238, -10.5238, -10.5234, -10.523, -10.5238, -10.5233, -10.5234, -10.5236, -10.5237, -10.5202, -10.5235, -10.5235, -10.5227, -10.5237, -10.5225, -10.5231, -10.5232, -10.5236, -10.5232, -10.5236, -10.5232, -10.5234, -10.5232, -10.5233, -10.5231, -10.5234, -10.5235, -10.5233, -10.5234, -10.5235, -10.5234, -10.5234, -10.5235, -0.7948, -1.5339, -3.3362, -3.988, -6.8234, -10.4176, -10.4176, -10.4186, -10.4185, -10.4187, -10.4187, -10.4187, -10.4187, -10.4187, -10.4187, -10.4186, -10.4187, -10.4187, -10.4186, -10.4186, -10.4187, -10.4187, -10.4187, -10.4186, -10.4187, -10.4187, -10.4187, -10.4186, -10.4187, -10.4187, -10.4186, -10.4185, -10.4182, -10.4186, -10.4182, -10.4181, -10.4186, -10.4183, -10.4151, -10.4179, -10.418, -10.4176, -10.418, -10.418, -10.4174, -10.4182, -10.4183, -10.4181, -10.4184, -10.4183, -10.4183, -10.4181, -10.4183, -10.4182, -10.4182, -10.4183, -10.4182, -10.4183, -10.4183, -0.9692, -2.785, -4.9086, -9.7319, -9.732, -9.732, -9.7319, -9.732, -9.7319, -9.7319, -9.732, -9.732, -9.732, -9.7319, -9.732, -9.732, -9.732, -9.732, -9.732, -9.732, -9.732, -9.732, -9.7319, -9.732, -9.732, -9.732, -9.732, -9.732, -9.732, -9.732, -9.732, -9.7319, -9.7319, -9.732, -9.7319, -9.732, -9.732, -9.732, -9.732, -9.7319, -9.7319, -9.7318, -9.7318, -9.7319, -9.7319, -9.7317, -9.7319, -9.7317, -9.7318, -9.7317, -9.7318, -9.7319, -9.7319, -9.7317, -9.7314, -9.7319, -9.7319, -9.7319, -9.7319, -9.7319, -9.7319, -9.7319, -9.7319, -9.7319, -9.7319, -9.7319, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372, -9.1372], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.3725, 2.3715, 2.3714, 2.3714, 2.3707, 2.3698, 2.3696, 2.3693, 2.3668, 2.3665, 2.3661, 2.3656, 2.3655, 2.3648, 2.3607, 2.3596, 2.3574, 2.3533, 2.3512, 2.3506, 2.3499, 2.3417, 2.3415, 2.3387, 2.3349, 2.3322, 2.3295, 2.3279, 2.3214, 2.3155, 2.4542, 2.4531, 2.4523, 2.4513, 2.4508, 2.4488, 2.4485, 2.4466, 2.4463, 2.4447, 2.4409, 2.432, 2.4316, 2.4291, 2.4283, 2.4266, 2.421, 2.4172, 2.3988, 2.3876, 2.3238, 2.2933, 2.2131, -0.4667, -0.4678, -0.4682, -0.4682, -0.4682, -0.4683, -0.4683, -0.4683, -0.4683, 2.4871, 2.4855, 2.4844, 2.4844, 2.484, 2.4839, 2.4839, 2.4837, 2.4836, 2.4835, 2.4834, 2.483, 2.4828, 2.4828, 2.4805, 2.4804, 2.4782, 2.4776, 2.4775, 2.4774, 2.4773, 2.4728, 2.4725, 2.4708, 2.4694, 2.4691, 2.4691, 2.4682, 2.4677, 2.4659, 2.5536, 2.5531, 2.5524, 2.5522, 2.552, 2.5512, 2.5511, 2.551, 2.5497, 2.5491, 2.5449, 2.5438, 2.5426, 2.5425, 2.5417, 2.5416, 2.5396, 2.5391, 2.5324, 2.5256, 2.5242, 2.5234, 2.5141, 2.5022, 2.4835, 2.1341, -0.4048, -0.4054, -0.4054, -0.4055, -0.4058, -0.4058, 2.6261, 2.6257, 2.6223, 2.6203, 2.6202, 2.62, 2.6199, 2.6195, 2.6183, 2.618, 2.618, 2.6161, 2.6155, 2.6153, 2.6128, 2.6099, 2.6075, 2.6058, 2.6032, 2.5957, 2.5552, 2.5183, 2.4441, 2.4431, 1.513, -0.149, -0.1501, -0.1501, -0.1501, -0.1502, -0.1502, 2.6658, 2.6636, 2.6635, 2.6632, 2.6629, 2.6628, 2.6623, 2.6622, 2.6622, 2.6604, 2.6585, 2.654, 2.6457, 2.6301, 2.6181, 2.5749, 2.5719, 2.5066, -0.1172, -0.1183, -0.1185, -0.1185, -0.1185, -0.1185, -0.1185, -0.1185, -0.1185, -0.1185, -0.1186, -0.1186, -0.1186, -0.1187, -0.1187, -0.1188, -0.1188, -0.1188, -0.1188, 2.7692, 2.769, 2.7684, 2.7649, 2.7634, 2.7628, 2.7534, 2.7502, 2.7478, 2.7459, 2.7454, 2.6965, 2.6528, 2.2254, 0.8735, -0.4141, -0.4143, -0.415, -0.4151, -0.4152, -0.4152, -0.4152, -0.4152, -0.4152, -0.4152, -0.4152, -0.4152, -0.4153, -0.4153, -0.4153, -0.4153, -0.4153, -0.4154, -0.4154, -0.4153, -0.4156, 2.7927, 2.7905, 2.7899, 2.7898, 2.7895, 2.7881, 2.7876, 2.7871, 2.787, 2.7863, 2.7822, 2.7814, 2.7763, 2.7755, 2.774, 2.7734, 2.7727, 2.7682, 2.7634, 2.7494, 2.7471, 2.7453, 2.7395, 2.7333, 2.731, 2.5226, 1.9765, 1.9061, -0.1661, -0.1668, 2.8994, 2.8986, 2.8985, 2.8977, 2.8969, 2.8963, 2.8951, 2.8922, 2.8918, 2.8896, 2.8857, 2.8842, 2.8814, 2.8788, 2.8767, 2.8767, 2.8722, 2.8719, 2.8647, 2.8647, 2.8641, 2.8574, 2.8319, 2.8302, 2.8291, 2.8284, 2.8275, 2.8186, 2.758, 2.6912, 2.9852, 2.9836, 2.9836, 2.9815, 2.9799, 2.977, 2.9743, 2.9741, 2.9728, 2.9727, 2.9657, 2.9589, 2.9521, 2.9498, 2.9481, 2.9475, 2.9388, 2.9371, 2.8887, 2.8787, 2.8358, 2.8123, 2.809, 2.6195, 2.5306, 2.228, 2.1798, 0.0883, 0.0882, 0.0882, 0.0881, 0.0873, 0.088, 3.2106, 3.2099, 3.2093, 3.2067, 3.2049, 3.2017, 3.2009, 3.2003, 3.1929, 3.1725, 3.1644, 3.1609, 3.1533, 3.1473, 1.4808, 0.2183, 0.2183, 0.2182, 0.2182, 0.2182, 0.2182, 0.2182, 0.2182, 0.2181, 0.2181, 0.2181, 0.2181, 0.2181, 0.2179, 0.2179, 0.2178, 0.2171, 0.2174, 0.2177, 0.2177, 0.2176, 0.2177, 3.232, 3.2312, 3.2298, 3.2291, 3.2253, 3.2239, 3.2231, 3.2227, 3.2227, 3.2214, 3.2209, 3.2206, 3.2205, 3.2201, 3.2149, 3.2071, 3.2001, 3.2, 3.15, 3.1143, 3.0277, 0.3948, 0.3947, 0.3947, 0.3947, 0.3947, 0.3947, 0.3947, 0.3947, 0.3947, 0.3947, 0.3947, 0.3946, 0.3946, 0.3946, 0.3946, 0.3943, 0.3936, 0.3943, 0.3945, 0.3944, 0.3944, 3.3006, 3.3005, 3.2994, 3.2966, 3.2952, 3.2943, 3.2917, 3.2914, 3.2894, 3.2853, 3.2846, 3.2823, 3.2796, 3.274, 3.2711, 3.2382, 3.1866, 3.0598, 2.7107, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3507, 0.3506, 0.3506, 0.3507, 0.3506, 0.3507, 0.3505, 0.3506, 0.3505, 0.3493, 0.3506, 0.3504, 0.3501, 0.3504, 0.3502, 0.3504, 0.3503, 3.3168, 3.3154, 3.3147, 3.3126, 3.3123, 3.3118, 3.3115, 3.3099, 3.3076, 3.3066, 3.3038, 3.3031, 3.2953, 3.2905, 3.2854, 3.2833, 3.2831, 3.2801, 3.2642, 3.256, 3.2302, 2.5288, 0.582, 0.582, 0.5819, 0.5819, 0.5819, 0.5819, 0.5819, 0.5819, 0.5819, 0.5819, 0.5819, 0.5818, 0.5818, 0.5818, 0.5816, 0.5817, 0.5806, 0.5818, 0.5813, 0.5817, 0.5816, 3.4149, 3.4141, 3.4089, 3.3944, 3.3943, 3.3428, 3.2826, 0.3832, 0.3832, 0.3831, 0.383, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3829, 0.3827, 0.3829, 0.3828, 0.3818, 0.3828, 0.3828, 0.3827, 0.3828, 0.3828, 0.3826, 0.3826, 0.3827, 0.3826, 0.3823, 0.3827, 0.3827, 0.3827, 0.3825, 0.3825, 0.3824, 0.3825, 0.3826, 0.3826, 0.3825, 0.3826, 0.3825, 3.4508, 3.446, 3.4431, 3.4429, 3.4423, 3.4403, 3.4397, 3.4384, 3.4361, 3.4352, 3.4325, 3.4321, 3.4301, 3.4289, 3.4196, 3.4183, 3.4158, 3.3662, 2.7739, 0.3821, 0.3821, 0.3821, 0.3821, 0.3821, 0.3821, 0.3821, 0.382, 0.382, 0.382, 0.382, 0.382, 0.382, 0.382, 0.3819, 0.382, 0.3819, 0.382, 0.3817, 0.3803, 0.3817, 0.3817, 0.3819, 0.3817, 0.3817, 0.3812, 0.3816, 0.3813, 0.3816, 3.4838, 3.4824, 3.467, 3.3933, 3.2811, 0.374, 0.3737, 0.3737, 0.3737, 0.3737, 0.3737, 0.3737, 0.3737, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3736, 0.3735, 0.3734, 0.3736, 0.3735, 0.3735, 0.3736, 0.3736, 0.3719, 0.3735, 0.3735, 0.373, 0.3736, 0.3728, 0.3732, 0.3732, 0.3735, 0.3733, 0.3735, 0.3732, 0.3733, 0.3732, 0.3732, 0.3731, 0.3733, 0.3734, 0.3732, 0.3733, 0.3734, 0.3733, 0.3731, 0.3733, 3.5194, 3.518, 3.5041, 3.4891, 3.084, 0.4793, 0.4791, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4788, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4787, 0.4786, 0.4785, 0.4787, 0.4786, 0.477, 0.4783, 0.4784, 0.4781, 0.4784, 0.4783, 0.4779, 0.4784, 0.4785, 0.4783, 0.4785, 0.4785, 0.4785, 0.4782, 0.4784, 0.4783, 0.4783, 0.4784, 0.4783, 0.4784, 0.4783, 4.4338, 4.4137, 4.2522, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1655, 1.1654, 1.1654, 1.1654, 1.1654, 1.1654, 1.1651, 1.1652, 1.1653, 1.1654, 1.1647, 1.1651, 1.1645, 1.1648, 1.164, 1.1647, 1.1653, 1.1649, 1.1636, 1.1607, 1.1649, 1.1648, 1.1649, 1.1646, 1.1649, 1.1645, 1.1649, 1.1645, 1.1651, 1.1648, 1.1649, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 1.7603, 0.0235, -1.5015, 1.7603, -2.7914, 1.4267, -3.1888, -1.719, -1.9668, -3.9068, -1.7101, -0.8805, 1.7603, -4.3433, 1.7603, -1.6527, -2.8801, -2.0864, -2.6283, -1.1542, -3.5286, -2.2483, -2.1189, -1.9385, -3.7342, -2.5809, -1.241, -2.449, -0.2188]}, "token.table": {"Topic": [6, 4, 12, 4, 9, 1, 16, 3, 13, 9, 8, 8, 8, 3, 15, 3, 5, 1, 1, 2, 10, 17, 8, 9, 9, 9, 9, 4, 13, 10, 8, 2, 6, 10, 14, 1, 12, 2, 16, 11, 14, 19, 18, 9, 10, 10, 12, 3, 3, 9, 8, 11, 8, 7, 3, 12, 11, 3, 13, 3, 14, 5, 13, 1, 1, 6, 10, 3, 3, 3, 13, 10, 3, 7, 4, 5, 5, 6, 16, 16, 9, 17, 7, 8, 5, 12, 14, 13, 1, 5, 8, 11, 4, 8, 12, 4, 14, 9, 19, 8, 1, 9, 13, 13, 6, 16, 11, 11, 6, 5, 5, 5, 12, 13, 1, 4, 1, 11, 14, 1, 14, 13, 13, 5, 8, 18, 18, 16, 10, 14, 6, 18, 6, 16, 8, 13, 10, 4, 2, 10, 2, 15, 10, 17, 11, 17, 5, 3, 4, 12, 16, 10, 7, 5, 10, 10, 7, 14, 1, 3, 3, 1, 4, 5, 4, 3, 2, 17, 14, 8, 5, 16, 5, 4, 11, 2, 3, 16, 8, 7, 9, 2, 5, 9, 14, 8, 9, 12, 12, 16, 4, 3, 16, 16, 2, 5, 15, 1, 9, 18, 7, 12, 11, 3, 10, 2, 16, 11, 8, 2, 9, 2, 9, 1, 1, 9, 8, 2, 9, 4, 1, 6, 4, 7, 11, 2, 2, 13, 4, 14, 1, 16, 2, 7, 15, 2, 1, 10, 10, 1, 9, 9, 10, 10, 1, 8, 10, 4, 9, 12, 5, 10, 3, 3, 6, 8, 3, 4, 13, 1, 2, 3, 1, 14, 6, 14, 7, 1, 14, 10, 8, 10, 8, 3, 13, 13, 10, 14, 9, 3, 9, 5, 13, 14, 15, 8, 8, 16, 8, 9, 4, 16, 1, 5, 1, 14, 6, 2, 12, 4, 1, 2, 9, 1, 6, 7, 4, 5, 2, 13, 12, 9, 2, 6, 5, 4, 13, 16, 10, 5, 7, 14, 7, 14, 3, 15, 3, 3, 5, 9, 8, 9, 10, 3, 19, 7, 14, 12, 4, 10, 4, 4, 5, 2, 15, 1, 12, 11, 12, 6, 14, 11, 4, 9, 8, 12, 12, 1, 6, 6, 1, 8, 16, 11, 12, 12, 3, 6], "Freq": [0.908574301137248, 0.9872266447383757, 0.9876706818936821, 0.9887505426224665, 0.9371880128846871, 0.9957062522432423, 0.9882058478718195, 0.9882788864653327, 0.9794467497240361, 0.9887012956308003, 0.9451465850230938, 0.42626226738597384, 0.9914642570487322, 0.9942654936755926, 0.9755691973618523, 0.9863854556009698, 0.25921359555463586, 0.9838720695381353, 0.9953433165773531, 0.9764373498408322, 0.9883758089090532, 0.9287958387019959, 0.9460147356441821, 0.9941551650672141, 0.9928516405378354, 0.9705288776371298, 0.9613164615946218, 0.6557947512320845, 0.9919582497062308, 0.8581729510874572, 0.9340620107506336, 0.9923711390345723, 0.9965348446902073, 0.9570958218077034, 0.952524108607259, 0.9930818189694314, 0.9928219098514776, 0.9761302578687417, 0.9868587607359003, 0.9931456783072401, 0.994212749158596, 0.8283605635236632, 0.9827606018443023, 0.9752330879907729, 0.9488423742166257, 0.9757028337879627, 0.9170995713568967, 0.9907797451009772, 0.9945457540711682, 0.975515987087549, 0.9753505782555092, 0.3135153045232505, 0.9877210747934102, 0.9990484458071196, 0.9801106226330121, 0.9840357525035176, 0.9453107057004196, 0.995312321312269, 0.8899655731007208, 0.9776654547187628, 0.9873388587561627, 0.9754066557471373, 0.7450298007229446, 0.8501304373072599, 0.9553297672169478, 0.9960106442590553, 0.9441254961301755, 0.9949985910250536, 0.9954435417071208, 0.9832973441671138, 0.9815625789417434, 0.9884031334287622, 0.9969017660112849, 0.9392846097071204, 0.9965421146251292, 0.8386212844301859, 0.9814761725347665, 0.8566813394900871, 0.9733121641062602, 0.9837505885742748, 0.974789855795482, 0.8379915008325586, 0.99321848733547, 0.9950902442126311, 0.9655859541436841, 0.970834221568296, 0.9800775088872817, 0.9824179402718173, 0.9771681780307514, 0.9910700378237891, 0.9955234207007229, 0.9430704783389341, 0.9521984378057038, 0.9887760536906844, 0.9880821206798868, 0.9882218557736748, 0.9555230303991096, 0.9148121003351611, 0.9763394374874019, 0.9986712011050681, 0.9986774023370331, 0.9861563607212589, 0.9954688345755238, 0.9874844947792821, 0.9552064376436161, 0.9893834985946163, 0.9622420321581151, 0.9417699590432631, 0.9895983755496609, 0.9986164229690431, 0.9927267299961626, 0.9884348178315695, 0.9896804200417255, 0.9892359206478935, 0.9493792449154496, 0.9243028406984282, 0.9363899548253037, 0.9895001842929059, 0.9678833624185168, 0.996901651603735, 0.9360881794030447, 0.9748205076685907, 0.9910226929820362, 0.9150088222330576, 0.7259424509464454, 0.9702296336400538, 0.9986084184288762, 0.9646823209390358, 0.996876872627252, 0.9909012856882776, 0.9989001539630943, 0.9975133325939364, 0.9950364269636299, 0.9814954807349735, 0.9717377174381255, 0.9706208206281313, 0.9967732297911764, 0.9929376356432754, 0.9876070908861576, 0.9987844793270635, 0.9979061919048594, 0.9164041252956243, 0.9843414658885843, 0.9970093947061087, 0.997998619675493, 0.9827305647141671, 0.9988619362594782, 0.986122335175058, 0.995257376299738, 0.8795004451419292, 0.9795566100633022, 0.8105231378020821, 0.9801831156431713, 0.9921612825147682, 0.957211159241932, 0.8938451805972912, 0.877935607470734, 0.9140038384306294, 0.9923505263869674, 0.9791611742921926, 0.9821262106890265, 0.9699401220560884, 0.9700839305708147, 0.988093589035693, 0.9946761687875182, 0.9931970768620068, 0.9691238381891193, 0.9987484254855244, 0.9956193797095981, 0.9957024648638235, 0.9922127232930876, 0.9756669240173733, 0.9784488625084257, 0.9985026140830594, 0.9928897248527375, 0.9982547325144432, 0.9769508925213903, 0.9611808119375983, 0.9936476000936669, 0.97202734959002, 0.9964343053782867, 0.9993335800149913, 0.9892265746859766, 0.9812072899029337, 0.9844553943114176, 0.9804635745971046, 0.8699159918300429, 0.9820182145566544, 0.985764118553605, 0.9839901724195641, 0.9889364049739706, 0.990969567699221, 0.993313231050049, 0.9583976617667657, 0.9571971719981798, 0.9940429684920221, 0.9934881355210196, 0.9624510492002565, 0.9235761049478556, 0.6722761045531344, 0.979359708401018, 0.9966343399446335, 0.988643356529485, 0.9947174606029182, 0.9886668620963496, 0.9945441467767918, 0.9793708909397223, 0.9907176917220656, 0.9517283888542305, 0.9793409687219444, 0.9972449831371513, 0.8552243285964647, 0.7977343797202041, 0.9963444950253622, 0.9886787210719964, 0.9616250560127557, 0.9803994242380425, 0.9912973921580415, 0.9679550226902809, 0.9669300663017647, 0.9769912914139776, 0.9956662239736149, 0.9970859691137723, 0.9858820608208935, 0.9536568615506025, 0.8916585038926371, 0.9654158750655591, 0.9675572840935899, 0.9883325782519866, 0.966079933389002, 0.9929087179883938, 0.9213390212216792, 0.9787335608328, 0.9788543116551054, 0.9794495733340446, 0.9960142542042317, 0.9676021137016562, 0.9855497157966405, 0.7061856951874592, 0.9981200697668506, 0.9301539455728618, 0.9798902077406235, 0.8579113062438708, 0.9942547168594659, 0.967062165136591, 0.9928610111893367, 0.9634928283183901, 0.9744707791516594, 0.9288961376153234, 0.9597474515842784, 0.9920399845111516, 0.7013111578907644, 0.9950608712616583, 0.9864873576962584, 0.9598332830353558, 0.9460786375050371, 0.9953191086907376, 0.9843960953419414, 0.9856129258804623, 0.9979710062912798, 0.9354178503241646, 0.9820866594751109, 0.9419396699054939, 0.9894396227855331, 0.994461032059404, 0.9632515323404669, 0.9936625598257428, 0.9909119091927003, 0.9907718502720989, 0.9936412306976768, 0.993919760272632, 0.9706248220631178, 0.44915215970212713, 0.9935964686693922, 0.9922027861062782, 0.9962676813300942, 0.4072403870059483, 0.9809005317892606, 0.9598437315767908, 0.9928646233951175, 0.9866389703494985, 0.9929419780210413, 0.5126637392215193, 0.9655823743320687, 0.8563530677179506, 0.9963947682157036, 0.9533230752062348, 0.9884469717660964, 0.9771190290172583, 0.9889160216974394, 0.995213598779659, 0.9968202667772531, 0.9500775553750741, 0.8429560028915, 0.9906722902005949, 0.9940088768116578, 0.9967721583777358, 0.9941510624256018, 0.9678398562628039, 0.946057747215966, 0.9769163140103366, 0.9436418133641189, 0.9971991509009906, 0.9973815975759553, 0.990301191262507, 0.9939345074697737, 0.9855554352338167, 0.9902081355060889, 0.8209714277018537, 0.9271725105837678, 0.9932972503661106, 0.9956024375434631, 0.991537301228426, 0.9149110331943097, 0.9953728029735102, 0.9974186401545068, 0.9957074410945219, 0.9729364205959601, 0.4241393809367766, 0.993226188904066, 0.9986799670110948, 0.9728866017337446, 0.6342410769950774, 0.9857898332353253, 0.9957527986812711, 0.9986933858500529, 0.9771382157639572, 0.9979551901512811, 0.9364414727120149, 0.9957180709714681, 0.9925106947124771, 0.9631320081317082, 0.9011268518526299, 0.9887813596341116, 0.9962799472083317, 0.9980245776133029, 0.42358390855282146, 0.9889196709361076, 0.9846296714208238, 0.9756503218046157, 0.9972949703498477, 0.9971934056138759, 0.985646648581105, 0.9957833266462475, 0.9977611703534762, 0.9760920289768681, 0.7943051298120866, 0.996187913672237, 0.9866160369376281, 0.9966050222789912, 0.9889591553482857, 0.9797569681745663, 0.9665192743069814, 0.9192583698904679, 0.9782730948086116, 0.9820690533673806, 0.9845652012189681, 0.9922619024948742, 0.977518105579935, 0.9936322056499679, 0.9809776590304276, 0.9828172501305402, 0.5614748230735013, 0.9967876298463793, 0.9956673959021429, 0.9839742217323182, 0.9901528552466728, 0.9971316775181717], "Term": ["age", "ago", "allow", "appear", "argue", "arm", "arrive", "attack", "backward", "bad", "ball", "bat", "bed", "believe", "better", "big", "birthday", "bit", "black", "blaze", "blood", "blow", "bow", "boy", "break", "breakfast", "breathe", "brilliant", "bring", "broomstick", "bulge", "burn", "buy", "cabin", "cage", "call", "car", "card", "care", "carry", "chair", "chamber", "chance", "change", "clap", "class", "clean", "clear", "close", "closely", "clutch", "coat", "cold", "come", "completely", "corner", "couple", "course", "crash", "cross", "cry", "cut", "dad", "dangle", "dare", "dark", "dash", "day", "dead", "deal", "decide", "deep", "die", "dinner", "door", "draco_malfoy", "dream", "dress", "drink", "drop", "drown", "duck", "dursley", "ear", "early", "eat", "edge", "effect", "egg", "emerge", "end", "enjoy", "enormous", "escape", "evening", "exactly", "excellent", "excited", "explain", "eye", "face", "fact", "fall", "family", "famous", "far", "fast", "fat", "fear", "feel", "fight", "finally", "finish", "fire", "fist", "float", "flower", "fly", "food", "foot", "forehead", "forget", "friend", "fun", "garden", "gaze", "get", "ghost", "give", "glass", "go", "good", "great", "great_hall", "green", "grin", "ground", "hair", "half", "hand", "happen", "happy", "have", "head", "hear", "heavy", "hermione", "high", "hogwart", "holiday", "home", "homework", "hope", "hour", "house", "howl", "huge", "hungry", "hurry", "ignore", "important", "invisible", "jet", "jump", "keep", "kill", "kitchen", "know", "large", "later", "laugh", "lead", "learn", "leave", "leg", "let", "letter", "line", "little", "lock", "long", "look", "lose", "loud", "love", "low", "luck", "mad", "magic", "magical", "make", "manage", "master", "matter", "maybe", "mean", "meet", "mention", "message", "miserably", "miss", "moment", "morning", "mother", "mouth", "move", "muggle", "mutter", "narrow", "neck", "need", "nervously", "news", "night", "noise", "normal", "nose", "notice", "number", "odd", "offer", "old", "open", "order", "outside", "overhead", "owl", "panic", "parent", "particularly", "pass", "passageway", "past", "pay", "pick", "place", "plate", "play", "player", "point", "position", "possible", "poster", "potion", "pound", "power", "practice", "pretend", "privet_drive", "promise", "quickly", "quidditch", "reach", "real", "reason", "regain", "remember", "repeat", "rest", "right", "rip", "risk", "roar", "roll", "room", "round", "run", "say", "school", "scream", "second", "secret", "self", "send", "set", "shake", "shame", "shape", "sharp", "shout", "sign", "silence", "simple", "sink", "sir", "sit", "size", "sleep", "slip", "small", "smile", "snape", "snarl", "sneer", "soon", "sound", "speak", "spell", "spend", "spot", "spread", "stair", "stand", "stare", "stay", "stop", "story", "straight", "stumble", "subject", "suddenly", "summer", "sure", "survive", "table", "take", "talk", "tall", "team", "tear", "tell", "term", "terror", "thin", "thing", "think", "throat", "time", "today", "tonight", "touch", "treat", "trip", "trouble", "try", "turn", "unusual", "use", "vanish", "visit", "voice", "wait", "wake", "walk", "want", "warn", "wash", "watch", "water", "way", "wear", "week", "welcome", "whip", "wide", "wind", "window", "wish", "witch", "wizard", "woman", "wonder", "wonderful", "word", "work", "wouldn", "write", "year"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [5, 3, 2, 12, 16, 15, 10, 4, 19, 11, 9, 14, 7, 8, 17, 1, 20, 18, 6, 13]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el672031403697653589528615024962", ldavis_el672031403697653589528615024962_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://d3js.org/d3.v5"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
        new LDAvis("#" + "ldavis_el672031403697653589528615024962", ldavis_el672031403697653589528615024962_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://d3js.org/d3.v5.js", function(){
         LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el672031403697653589528615024962", ldavis_el672031403697653589528615024962_data);
            })
         });
}
</script>



### Optimizing coherence scores


```python
import mallet
```


```python
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    """
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    """
    mallet_path = '/Users/animeshgiri/mallet-2.0.8/bin/mallet'
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values
```


```python
model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=hp_lemmatized, 
                                                        start=2, limit=40, step=6)
python
limit=40; start=2; step=6;
x = range(start, limit, step)
```


```python
# Print the coherence scores
for m, cv in zip(x, coherence_values):
    print("Num Topics =", m, " has Coherence Value of", round(cv, 4))
```

    Num Topics = 2  has Coherence Value of 0.19
    Num Topics = 8  has Coherence Value of 0.3493
    Num Topics = 14  has Coherence Value of 0.3502
    Num Topics = 20  has Coherence Value of 0.3298
    Num Topics = 26  has Coherence Value of 0.3362
    Num Topics = 32  has Coherence Value of 0.3377
    Num Topics = 38  has Coherence Value of 0.3281



```python
# Show graph

plt.plot(x, coherence_values)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()
```


    
![png](output_110_0.png)
    



```python
optimal_model = model_list[1]
model_topics = optimal_model.show_topics(formatted=False)
pprint(optimal_model.print_topics(num_words=10))
```

    [(0,
      '0.050*"eye" + 0.042*"hand" + 0.033*"black" + 0.029*"long" + 0.024*"foot" + '
      '0.018*"ground" + 0.015*"light" + 0.015*"arm" + 0.013*"shake" + '
      '0.013*"hair"'),
     (1,
      '0.036*"time" + 0.032*"good" + 0.029*"thing" + 0.027*"feel" + 0.023*"malfoy" '
      '+ 0.022*"start" + 0.022*"give" + 0.020*"turn" + 0.019*"happen" + '
      '0.018*"work"'),
     (2,
      '0.030*"head" + 0.025*"large" + 0.024*"point" + 0.022*"pull" + 0.021*"wand" '
      '+ 0.020*"turn" + 0.020*"hold" + 0.014*"small" + 0.014*"robe" + '
      '0.013*"catch"'),
     (3,
      '0.035*"leave" + 0.026*"stare" + 0.023*"walk" + 0.023*"stop" + 0.018*"find" '
      '+ 0.017*"moment" + 0.016*"night" + 0.016*"sirius" + 0.015*"lupin" + '
      '0.015*"win"'),
     (4,
      '0.043*"hear" + 0.030*"open" + 0.029*"door" + 0.025*"voice" + 0.025*"stand" '
      '+ 0.019*"room" + 0.018*"fall" + 0.015*"floor" + 0.015*"reach" + '
      '0.015*"sound"'),
     (5,
      '0.026*"year" + 0.023*"hogwart" + 0.019*"people" + 0.019*"end" + '
      '0.019*"school" + 0.019*"boy" + 0.018*"find" + 0.016*"book" + 0.016*"wizard" '
      '+ 0.015*"place"'),
     (6,
      '0.037*"face" + 0.034*"snape" + 0.025*"watch" + 0.022*"table" + 0.019*"talk" '
      '+ 0.016*"speak" + 0.015*"smile" + 0.015*"wait" + 0.014*"bad" + '
      '0.013*"suppose"'),
     (7,
      '0.103*"hermione" + 0.047*"dumbledore" + 0.030*"sit" + 0.022*"hagrid" + '
      '0.018*"great" + 0.015*"whisper" + 0.013*"word" + 0.011*"read" + '
      '0.009*"stay" + 0.009*"meet"')]


### Finding the dominant topic in each sentence


```python
def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=hp_sentences):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=hp_sentences)

# Format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Show
df_dominant_topic.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Document_No</th>
      <th>Dominant_Topic</th>
      <th>Topic_Perc_Contrib</th>
      <th>Keywords</th>
      <th>Text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>4.0</td>
      <td>0.1587</td>
      <td>hear, open, door, voice, stand, room, fall, fl...</td>
      <td>\nHARRY POTTER AND THE CHAMBER OF SECRETS\nby J</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.0</td>
      <td>0.1250</td>
      <td>eye, hand, black, long, foot, ground, light, a...</td>
      <td>K</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>1.0</td>
      <td>0.1636</td>
      <td>time, good, thing, feel, malfoy, start, give, ...</td>
      <td>Rowling\n\n(this is BOOK 2 in the Harry Potter...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.0</td>
      <td>0.1250</td>
      <td>eye, hand, black, long, foot, ground, light, a...</td>
      <td>Mr</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>3.0</td>
      <td>0.1595</td>
      <td>leave, stare, walk, stop, find, moment, night,...</td>
      <td>Vernon Dursley had been woken in\nthe early ho...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>7.0</td>
      <td>0.1422</td>
      <td>hermione, dumbledore, sit, hagrid, great, whis...</td>
      <td>"If you can't\ncontrol that owl, it'll have to...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>2.0</td>
      <td>0.1422</td>
      <td>head, large, point, pull, wand, turn, hold, sm...</td>
      <td>"She's used to flying around outside</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7</td>
      <td>1.0</td>
      <td>0.1447</td>
      <td>time, good, thing, feel, malfoy, start, give, ...</td>
      <td>If I could\njust let her out at night -"\n\n"D...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>8</td>
      <td>1.0</td>
      <td>0.1469</td>
      <td>time, good, thing, feel, malfoy, start, give, ...</td>
      <td>"I know what'll happen if that owl's let\nout....</td>
    </tr>
    <tr>
      <th>9</th>
      <td>9</td>
      <td>7.0</td>
      <td>0.1394</td>
      <td>hermione, dumbledore, sit, hagrid, great, whis...</td>
      <td>"We must build you up while\nwe've got the cha...</td>
    </tr>
  </tbody>
</table>
</div>



### Find the most representative document for each topic


```python
# Group top 5 sentences under each topic
sent_topics_sorteddf = pd.DataFrame()

sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')

for i, grp in sent_topics_outdf_grpd:
    sent_topics_sorteddf = pd.concat([sent_topics_sorteddf, 
                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], 
                                            axis=0)

# Reset Index    
sent_topics_sorteddf.reset_index(drop=True, inplace=True)

# Format
sent_topics_sorteddf.columns = ['Topic_Num', "Topic_Perc_Contrib", "Keywords", "Text"]

# Show
sent_topics_sorteddf.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Topic_Num</th>
      <th>Topic_Perc_Contrib</th>
      <th>Keywords</th>
      <th>Text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.3670</td>
      <td>eye, hand, black, long, foot, ground, light, a...</td>
      <td>He heard Hermione shriek with pain and fall\nt...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>0.2586</td>
      <td>time, good, thing, feel, malfoy, start, give, ...</td>
      <td>Blocks of ice cream in every flavor you\ncould...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.0</td>
      <td>0.4189</td>
      <td>head, large, point, pull, wand, turn, hold, sm...</td>
      <td>The Quidditch season was approaching, and O1iv...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.0</td>
      <td>0.2417</td>
      <td>leave, stare, walk, stop, find, moment, night,...</td>
      <td>She took the newspaper, scanned the first few...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4.0</td>
      <td>0.2989</td>
      <td>hear, open, door, voice, stand, room, fall, fl...</td>
      <td>With a tiny hint of sarcasm\nin his voice, he ...</td>
    </tr>
  </tbody>
</table>
</div>



### Topic distribution across documents


```python
# Number of Documents for Each Topic
topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()

# Percentage of Documents for Each Topic
topic_contribution = round(topic_counts/topic_counts.sum(), 4)

# Topic Number and Keywords
topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]

# Concatenate Column wise
df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)

# Change Column names
df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']

# Show
df_dominant_topics
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Dominant_Topic</th>
      <th>Topic_Keywords</th>
      <th>Num_Documents</th>
      <th>Perc_Documents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0.0</th>
      <td>4.0</td>
      <td>hear, open, door, voice, stand, room, fall, fl...</td>
      <td>6630.0</td>
      <td>0.3082</td>
    </tr>
    <tr>
      <th>1.0</th>
      <td>0.0</td>
      <td>eye, hand, black, long, foot, ground, light, a...</td>
      <td>2654.0</td>
      <td>0.1234</td>
    </tr>
    <tr>
      <th>2.0</th>
      <td>1.0</td>
      <td>time, good, thing, feel, malfoy, start, give, ...</td>
      <td>2068.0</td>
      <td>0.0961</td>
    </tr>
    <tr>
      <th>3.0</th>
      <td>0.0</td>
      <td>eye, hand, black, long, foot, ground, light, a...</td>
      <td>2222.0</td>
      <td>0.1033</td>
    </tr>
    <tr>
      <th>4.0</th>
      <td>3.0</td>
      <td>leave, stare, walk, stop, find, moment, night,...</td>
      <td>2142.0</td>
      <td>0.0996</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>21504.0</th>
      <td>7.0</td>
      <td>hermione, dumbledore, sit, hagrid, great, whis...</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>21505.0</th>
      <td>0.0</td>
      <td>eye, hand, black, long, foot, ground, light, a...</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>21506.0</th>
      <td>1.0</td>
      <td>time, good, thing, feel, malfoy, start, give, ...</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>21507.0</th>
      <td>1.0</td>
      <td>time, good, thing, feel, malfoy, start, give, ...</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>21508.0</th>
      <td>0.0</td>
      <td>eye, hand, black, long, foot, ground, light, a...</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>21509 rows × 4 columns</p>
</div>



## --End-- ##
